{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call ***loss.backward()*** we use PyTorch autograd to calculate all the gradients along the way. In this notebook, we are going to remove this line and instead we are going to **write all the backward pass** manually on the level of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "with open('names.txt', 'r') as file:\n",
    "    words = file.read().splitlines()\n",
    "\n",
    "print(len(words))\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # context length\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix]\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])   # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])   # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializing MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are initializing `b1`, `b2`, `bngain`, `bnbias` in **non-standard ways** because sometimes initializating with all zeros could mask an incorrect implementation of the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10   # dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "print('Number of parameters:', sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass, \"chunkated\" into **smaller steps** that are possible to backpropagate one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "\n",
    "# mini-batch construct\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.3621020317077637\n"
     ]
    }
   ],
   "source": [
    "emb = C[Xb]                         # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1           # hidden layer pre-activation\n",
    "\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*bndiff2.sum(0, keepdim=True) # Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact)             # hidden layer\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2                # output layer\n",
    "\n",
    "# Cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1     # same as 1.0 / counts_sum\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "print('loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The [Bessel's correction](https://math.oxford.emory.edu/site/math117/besselCorrection/) paper explains why dividing by n-1 gives a better estimate of the variance when the population size or samples for the population are very small. Using 1/n almost always underestimates the variance and it is a biased estimator. Hence, it is advised using the **unbiased version of the variance** and divide by 1-n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "  p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropopagate manually through all of the variables as they are defined in the forward pass above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to backpropagate the loss by calculating the derivatives of each variable so we get matrices or vectors containing the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dlogprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss` is the negative mean of all the **log probabilities of the correct next character** in a sequence:\n",
    "\n",
    "*loss = -logprobs[range(n), Yb].mean()*\n",
    "\n",
    "<br>\n",
    "\n",
    "`dlogprobs` will hold the derivative of the loss with respect to all the elements of `logprobs`.\n",
    "\n",
    "- Let us simplify the problem to *loss = -(a + b + c) / 3*. \n",
    "\n",
    "- Algebraically, *loss = -1/3a -1/3b -1/3c*. \n",
    "\n",
    "- Then, the derivative of the loss with respect to a (or b or c) is *dloss/da = -1/3*. \n",
    "\n",
    "- More generally *dloss/da = -1/n*.\n",
    "\n",
    "Thus, the derivative of the loss with respect to the **log probabilities of the correct next characters** is -1/n.\n",
    "\n",
    "The **log probabilties of the incorrect next characters** do not participate in the calculation of the loss. Thus, the derivative of the loss with respect to them is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs) # matrix of zeros with the shape of logprobs\n",
    "dlogprobs[range(n), Yb] = -1.0/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logprobs` is calculated taking the logarithm element wise of all the elements of `probs`:\n",
    "\n",
    "*logprobs = log(probs)*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we multiply the local derivative of the above operation (*d/dx log(x) = 1/x*) times `dlogprobs`.\n",
    "\n",
    "**Note:** This line is taking the examples that have a very low probability currently assigned and it's boosting their gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprobs = (1.0 / probs) * dlogprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dcounts (1) and dcounts_sum_inv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`probs` is calculated muliplying the matrix `counts` and the column vector `counts_sum_inv`:\n",
    "\n",
    "*probs = counts * counts_sum_inv*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{11}x_1 & a_{12}x_1 & a_{13}x_1 \\\\\n",
    "a_{21}x_2 & a_{22}x_2 & a_{23}x_2 \\\\\n",
    "a_{31}x_3 & a_{32}x_3 & a_{33}x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "·\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `probs` with respect to `counts` is `counts_sum_inv` and with respect to `counts_sum_inv` is `counts`. Thus, we multiply these matrices times `dprobs`. In addition, the column vector `counts_sum_inv` is broadcasted so we have to sum horizontally across the rows.\n",
    "\n",
    "**Note:** *If a node is used multiple times, the gradients of all of it uses will be summed during backpropagation.* We are multiplying some elements multiple times so we have to **sum horizontally across the rows**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dcounts_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`counts_sum_inv` is calculated rising element wise all the elements of `counts_sum` to the power of -1:\n",
    "\n",
    "*counts_sum_inv = counts_sum<sup>-1</sup>*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we multiply the local derivative of the above operation (d/dx x<sup>-1</sup> = -x<sup>-2</sup>) times `dcounts_sum_inv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dcounts (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`counts_sum` is calculated summing up the rows of the matrix `counts`:\n",
    "\n",
    "*counts_sum = counts.sum(1, keepdims=True)*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} + a_{12} + a_{13} \\\\\n",
    "a_{21} + a_{22} + a_{23} \\\\\n",
    "a_{31} + a_{32} + a_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "The local derivative of `counts_sum` with respect to `counts` is 1. Thus, we multiply a matrix of ones with the shape of `counts` times `dcounts_sum`.\n",
    "\n",
    "**Note:** We already calculated `dcounts` before so we have to add these gradients with the ones calculated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts += torch.ones_like(counts) * dcounts_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dnorm_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`counts` is calculated exponentiating element wise all the elements of `norm_logits`:\n",
    "\n",
    "*counts = e<sup>norm_logits</sup>*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we multiply the local derivative of the above operation (d/dx e<sup>x</sup> = e<sup>x</sup>) times `dcounts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnorm_logits = counts * dcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dlogits (1) and dlogit_maxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`norm_logits` is calculated subtracting the matrix `logits` with the column vector `logit_maxes`.\n",
    "\n",
    "*norm_logits = logits - logit_maxes*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{11}-x_1 & a_{12}-x_1 & a_{13}-x_1 \\\\\n",
    "a_{21}-x_2 & a_{22}-x_2 & a_{23}-x_2 \\\\\n",
    "a_{31}-x_3 & a_{32}-x_3 & a_{33}-x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `norm_logits` with respect to `logits` is 1 and with respect to `logit_maxes` is -1. Thus, we multiply 1 and -1 times `dnorm_logits`. In addition, the column vector `logit_maxes` is broadcasted so we have to sum horizontally across the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits = dnorm_logits.clone() # copy of dnorm_logits\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dlogits (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logit_maxes` is calculated pluging out the max number of each row of the matrix `logits`:\n",
    "\n",
    "*logit_maxes = logits.max(1, keepdim=True).values*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "max(a_{11} & a_{12} & a_{13}) \\\\\n",
    "max(a_{21} & a_{22} & a_{23}) \\\\\n",
    "max(a_{31} & a_{32} & a_{33})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `logit_maxes` with respect to the maximum `logits` is 1. Thus, we only multiply those maximum `logits` times `dlogit_maxes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dh, dW2, and db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logits` is calculated matrix multypling the matrix `h` and `W2` and adding the broadcasted vector `b2`:\n",
    "\n",
    "*logits = h @ W2 + b2*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "d_{11} & d_{12} \\\\\n",
    "d_{21} & d_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "c_{1} & c_{2} \\\\\n",
    "c_{1} & c_{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Multiplying and adding:\n",
    "\n",
    "$$\n",
    "d_{11} = a_{11} b_{11} + a_{12} b_{21} + c_{1} \\\\\n",
    "d_{12} = a_{11} b_{12} + a_{12} b_{22} + c_{2} \\\\\n",
    "d_{21} = a_{21} b_{11} + a_{22} b_{21} + c_{1} \\\\\n",
    "d_{22} = a_{21} b_{12} + a_{22} b_{22} + c_{2}\n",
    "$$\n",
    "\n",
    "Derivating with respect to a:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial a_{11}} = \\frac{\\partial L}{\\partial d_{11}} · b_{11} + \\frac{\\partial L}{\\partial d_{12}} · b_{12} \\\\\n",
    "\\frac{\\partial L}{\\partial a_{12}} = \\frac{\\partial L}{\\partial d_{11}} · b_{21} + \\frac{\\partial L}{\\partial d_{12}} · b_{22} \\\\\n",
    "\\frac{\\partial L}{\\partial a_{21}} = \\frac{\\partial L}{\\partial d_{21}} · b_{11} + \\frac{\\partial L}{\\partial d_{22}} · b_{12} \\\\\n",
    "\\frac{\\partial L}{\\partial a_{22}} = \\frac{\\partial L}{\\partial d_{21}} · b_{21} + \\frac{\\partial L}{\\partial d_{22}} · b_{22}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial a_{11}} & \\frac{\\partial L}{\\partial a_{12}} \\\\\n",
    "\\frac{\\partial L}{\\partial a_{21}} & \\frac{\\partial L}{\\partial a_{22}}\n",
    "\\end{align*}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial d_{11}} & \\frac{\\partial L}{\\partial d_{12}} \\\\\n",
    "\\frac{\\partial L}{\\partial d_{21}} & \\frac{\\partial L}{\\partial d_{22}}\n",
    "\\end{align*}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{21} \\\\\n",
    "b_{12} & b_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{\\partial L}{\\partial d} \\times b^{T}\n",
    "$$\n",
    "\n",
    "Similarly:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = a^{T} \\times \\frac{\\partial L}{\\partial d}\n",
    "$$\n",
    "\n",
    "Finally:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial c_{1}} &= \\frac{\\partial L}{\\partial d_{11}} · 1 + \\frac{\\partial L}{\\partial d_{21}} · 1 \\\\\n",
    "\\frac{\\partial L}{\\partial c_{2}} &= \\frac{\\partial L}{\\partial d_{12}} · 1 + \\frac{\\partial L}{\\partial d_{22}} · 1 \\\\\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\frac{\\partial L}{\\partial c} &= \\frac{\\partial L}{\\partial d} · sum(0)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dhpreact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`h` is calculated applying the tanh function element wise to all the elements of `hpreact`:\n",
    "\n",
    "*h = torch.tanh(hpreact)*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we multiply the local derivative of the above operation (d/dx tanh(x) = 1-tanh<sup>2</sup>(x)) times `dh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhpreact = (1.0 - h**2) * dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbngain, dbnraw, and dbnbias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hpreact` is calculated muliplying the row vector `bngain` and the matrix `bnraw` and adding the row vector `bnbias`:\n",
    "\n",
    "*hpreact = bngain * bnraw + bnbias*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1a_{11}+b_1 & x_2a_{12}+b_2 & x_3a_{13}+b_3 \\\\\n",
    "x_1a_{21}+b_1 & x_2a_{22}+b_2 & x_3a_{23}+b_3 \\\\\n",
    "x_1a_{31}+b_1 & x_2a_{32}+b_2 & x_3a_{33}+b_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "·\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & b_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `hpreact` with respect to `bngain` is `bnraw` and with respect to `bnraw` is `bngain`. Thus, we multiply these matrices times `dhpreact`. In addition, the row vector `bngain` is broadcasted so we have to sum vertically across the columns.\n",
    "\n",
    "The local derivative of `hpreact` with respect to `bnbias` is 1. Thus, we multiply 1 times `dhpreact`. In addition, the row vector `bnbias` is broadcasted so we have to sum vertically across the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbndiff (1) and dbnvar_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnraw` is calculated muliplying the matrix `bndiff` and the row vector `bnvar_inv`:\n",
    "\n",
    "*bnraw = bndiff * bnvar_inv*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{11}x_1 & a_{12}x_2 & a_{13}x_3 \\\\\n",
    "a_{21}x_1 & a_{22}x_2 & a_{23}x_3 \\\\\n",
    "a_{31}x_1 & a_{32}x_2 & a_{33}x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "·\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `bnraw` with respect to `bndiff` is `bnvar_inv` and with respect to `bnvar_inv` is `bndiff`. Thus, we multiply these matrices times `dbnraw`. In addition, the row vector `bnvar_inv` is broadcasted so we have to sum vertically across the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbnvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnvar_inv` is calculated dividing 1 by the square root of `bnvar` plus epsilon:\n",
    "\n",
    "*bnvar_inv = (bnvar + 1e-5)<sup>-0.5</sup>*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we multiply the local derivative of the above operation (d/dx x<sup>-0.5</sup> = -0.5x<sup>-1.5</sup>) times `dbnvar_inv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbndiff2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnvar` is calculated dividing the sum of the columns of the matrix `bndiff2` by n-1:\n",
    "\n",
    "*bnvar = 1/(n-1)bndiff2.sum(0, keepdim=True)*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{n-1}\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "+ & + & + \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "+ & + & + \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `bnvar` with respect to `bndiff2` is 1/n-1. Thus, we multiply a matrix of ones with the shape of `bndiff2` times 1/n-1. Then, we multiply that matrix times `dbnvar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbndiff (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnvar` is calculated squaring element wise all the elements of `bndiff`:\n",
    "\n",
    "*bndiff2 = bndiff <sup>2</sup>*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we multiply the local derivative of the above operation (d/dx x<sup>2</sup> = 2x) times `dbndiff2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbndiff += (2*bndiff) * dbndiff2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dhprebn (1) and dbnmeani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bndiff` is calculated subtracting the matrix `hprebn` with the row vector `bnmeani`.\n",
    "\n",
    "*bndiff = hprebn - bnmeani*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{11}-x_1 & a_{12}-x_2 & a_{13}-x_3 \\\\\n",
    "a_{21}-x_1 & a_{22}-x_2 & a_{23}-x_3 \\\\\n",
    "a_{31}-x_1 & a_{32}-x_2 & a_{33}-x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `bndiff` with respect to `hprebn` is 1 and with respect to `bnmeani` is -1. Thus, we multiply 1 and -1 times `dbndiff`. In addition, the row vector `bnmeani` is broadcasted so we have to sum vertically across the colunns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dhprebn (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnmeani` is calculated dividing the sum of the columns of the matrix `hprebn` by n:\n",
    "\n",
    "*bnmeani = (1/n)hprebn.sum(0, keepdim=True)*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "+ & + & + \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "+ & + & + \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `bnmeani` with respect to `hprebn` is 1/n. Thus, we multiply a matrix of ones with the shape of `hprebn` times 1/n. Then, we multiply that matrix times `dbnmeani`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhprebn += 1.0/n * torch.ones_like(hprebn) * dbnmeani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dembcat, dW1, and db1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hprebn` is calculated matrix multypling the matrix `embcat` and `W1` and adding the broadcasted vector `b1`:\n",
    "\n",
    "*hprebn = embcat @ W1 + b1*\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "d_{11} & d_{12} \\\\\n",
    "d_{21} & d_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "c_{1} & c_{2} \\\\\n",
    "c_{1} & c_{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "From before:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial a} &= \\frac{\\partial L}{\\partial d} \\times b^{T} \\\\\n",
    "\\frac{\\partial L}{\\partial b} &= a^{T} \\times \\frac{\\partial L}{\\partial d} \\\\\n",
    "\\frac{\\partial L}{\\partial c} &= \\frac{\\partial L}{\\partial d} · sum(0)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`embcat` concatenates the last two vectors of `emb` in the forward pass (from [32, 3, 10] to [32,50]):\n",
    "\n",
    "*embcat = emb.view(emb.shape[0], -1)*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, in the backward pass we just undo that concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "demb = dembcat.view(emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`emb` is calculated indexing the integers of the rows of the matrix `Xb` using the look-up table `C`:\n",
    "\n",
    "emb = C[Xb]\n",
    "\n",
    "Thus, in the backward pass we undo the indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dC = torch.zeros_like(C)\n",
    "\n",
    "for k in range(Xb.shape[0]):\n",
    "  for j in range(Xb.shape[1]): # iterate over all the elements of Xb\n",
    "    ix = Xb[k,j]\n",
    "    dC[ix] += demb[k,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cmp** is a utility function for comparing the gradients we just calculated manually with the gradients calculated by PyTorch autograd.\n",
    "\n",
    "- `dt` is the value calcualated by us manually.\n",
    "\n",
    "- `t` is the value calculated by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagate through cross_entropy but all in one go. To complete this challenge look at the mathematical expression of the loss, take the derivative, and simplify the expression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_fast: 3.3621022701263428\n"
     ]
    }
   ],
   "source": [
    "# BEFORE:\n",
    "\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1     # same as 1.0 / counts_sum\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "\n",
    "# NOW:\n",
    "\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print('loss_fast:', loss_fast.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.3621020317077637\n",
      "loss_fast: 3.3621022701263428\n",
      "diff (loss_fast - loss): 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "print('loss:', loss.item())\n",
    "print('loss_fast:', loss_fast.item())\n",
    "print('diff (loss_fast - loss):', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax:**\n",
    "\n",
    "$$\n",
    "prob_i = \\frac{e^{logits_{i}}}{\\sum_{j}{e^{logits_{j}}}} \\\\\n",
    "$$\n",
    "\n",
    "For simplicity:\n",
    "\n",
    "$$\n",
    "prob_i = \\frac{e^{l_{i}}}{\\sum_{j}{e^{l_{j}}}} \\\\\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**Forward pass:**\n",
    "\n",
    "$$\n",
    "\\text{logits} \\xrightarrow{\\text{softmax}} \\text{probs} \\xrightarrow{\\text{- log}} \\text{- logprobs} \\xrightarrow{\\text{identity corrects}} \\text{- logprobs [ j ]}  \\xrightarrow{\\text{mean}} \\text{loss}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**For a single example**, loss is the negative log probability for y label:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{loss} &= -\\log prob_{y} \\\\\n",
    "&= -\\log \\frac{e^{l_{y}}}{\\sum_{j}{e^{l_{j}}}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**Backward pass**:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial loss}{\\partial l_{i}} &= \\frac{\\partial}{\\partial l_{i}}\\begin{bmatrix}-\\log \\frac{e^{l_{y}}}{\\sum_{j}{e^{l_{j}}}}\\end{bmatrix} \\\\\n",
    "&= - \\frac{\\sum_{j}{e^{l_{j}}}}{e^{l_{y}}} · \\frac{\\partial}{\\partial l_{i}}\\begin{bmatrix}\\frac{e^{l_{y}}}{\\sum_{j}{e^{l_{j}}}}\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If i ≠ y:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&= - \\frac{\\sum_{j}{e^{l_{j}}}}{e^{l_{y}}} · \\begin{bmatrix}0·\\frac{1}{\\sum_{j}{e^{l_{j}}}}-\\frac{e^{l_{y}}e^{l_{i}}}{(\\sum_{j}{e^{l_{j}}})^2}\\end{bmatrix} \\\\\n",
    "&= \\frac{(\\sum_{j}{e^{l_{j}}})e^{l_{y}}e^{l_{i}}}{e^{l_{y}}(\\sum_{j}{e^{l_{j}}})^2} \\\\\n",
    "&= \\frac{e^{l_{i}}}{\\sum_{j}{e^{l_{j}}}} \\\\\n",
    "&= prob_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If i = y:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&= - \\frac{\\sum_{j}{e^{l_{j}}}}{e^{l_{y}}} · \\begin{bmatrix}\\frac{e^{l_{y}}}{\\sum_{j}{e^{l_{j}}}}-\\frac{e^{l_{y}}e^{l_{i}}}{(\\sum_{j}{e^{l_{j}}})^2}\\end{bmatrix} \\\\\n",
    "&= - \\frac{\\sum_{j}{e^{l_{j}}}}{e^{l_{y}}} · \\begin{bmatrix}\\frac{\\sum_{j}{e^{l_{j}}}e^{l_{y}}-e^{l_{y}}e^{l_{i}}}{(\\sum_{j}{e^{l_{j}}})^2}\\end{bmatrix} \\\\\n",
    "&= -\\frac{\\sum_{j}{e^{l_{j}}}-e^{l_{i}}}{\\sum_{j}{e^{l_{j}}}} \\\\\n",
    "&= \\frac{e^{l_{i}}}{\\sum_{j}{e^{l_{j}}}} - 1 \\\\\n",
    "&= prob_i - 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**For a batch**, loss is the average of all the examples' losses. Hence, `dlogits` has to be scaled down by the number of examples `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE:\n",
    "\n",
    "# dlogprobs = torch.zeros_like(logprobs) # matrix of zeros with the shape of logprobs\n",
    "# dlogprobs[range(n), Yb] = -1.0/n\n",
    "# dprobs = (1.0 / probs) * dlogprobs\n",
    "# dcounts = counts_sum_inv * dprobs\n",
    "# dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "# dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "# dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "# dnorm_logits = counts * dcounts\n",
    "# dlogits = dnorm_logits.clone() # copy of dnorm_logits\n",
    "# dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "# dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "\n",
    "# NOW:\n",
    "\n",
    "dlogits = F.softmax(logits, 1) # calculate prob_i\n",
    "dlogits[range(n), Yb] -= 1     # subtract a 1 at the correct positions\n",
    "dlogits /= n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 4.190951585769653e-09\n"
     ]
    }
   ],
   "source": [
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in the forward pass the probabilities came out exactly correct (zeros everywhere except for a one at the correct position), then `dlogits` would have a row of all zeros for that example.\n",
    "\n",
    "If we very confidently mispredicted an element, that element is going to be pulled down very heavily and the correct answer is going to be pulled up to the same amount and the other characters are not going to be influenced too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagate through batchnorm but all in one go. To complete this challenge look at the mathematical expression of the loss, take the derivative, and simplify the expression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE:\n",
    "\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "\n",
    "# NOW:\n",
    "\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff (hpreact_fast - hpreact): tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print('max diff (hpreact_fast - hpreact):', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE:\n",
    "\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "\n",
    "# NOW:\n",
    "\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "cmp('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the MLP neural net with your own backward pass.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:      0 / 200000   Loss: 3.4946\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m hprebn \u001b[38;5;241m=\u001b[39m embcat \u001b[38;5;241m@\u001b[39m W1 \u001b[38;5;241m+\u001b[39m b1           \u001b[38;5;66;03m# hidden layer pre-activation\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# BatchNorm layer\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m bnmean \u001b[38;5;241m=\u001b[39m \u001b[43mhprebn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m bnvar \u001b[38;5;241m=\u001b[39m hprebn\u001b[38;5;241m.\u001b[39mvar(\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unbiased\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     28\u001b[0m bnvar_inv \u001b[38;5;241m=\u001b[39m (bnvar \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-5\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # mini-batch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "\n",
    "    # FORWARD PASS:\n",
    "\n",
    "    # Embedding\n",
    "    emb = C[Xb]                         # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "\n",
    "    # Linear layer 1\n",
    "    hprebn = embcat @ W1 + b1           # hidden layer pre-activation\n",
    "\n",
    "    # BatchNorm layer\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact)             # hidden layer\n",
    "\n",
    "    # Linear layer 2\n",
    "    logits = h @ W2 + b2                # output layer\n",
    "\n",
    "    # Cross entropy\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "\n",
    "    # BACKWARD PASS:\n",
    "\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    \n",
    "    # Cross entropy backprop\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "\n",
    "    # Linear layer 2 backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "\n",
    "    # Non-linearity backprop\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "\n",
    "    # BatchNorm layer backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    \n",
    "    # Linear layer 2 backprop\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "\n",
    "    # Embedding backprop\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "      for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "    \n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01    # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      p.data += -lr * grad\n",
    "    \n",
    "    # track stats\n",
    "    if i % 10000 == 0:\n",
    "      print(f'Step:{i:7d} /{max_steps:7d}   Loss: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibrate the batch norm at the end of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  \n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.070523500442505\n",
      "val 2.109893560409546\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "\n",
    "  # get x,y based on the split\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[x]                          # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact)             # (N, n_hidden)\n",
    "  logits = h @ W2 + b2                # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "\n",
    "  print(split, 'loss:', loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
