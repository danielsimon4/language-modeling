{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "# Generatively Pretrained Transformer (GPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to create a Generatively Pretrained Transformer (GPT) and train it on the [Shakespeare text](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) dataset which contains around 1M characters. Once the model is trained, it will genearte Shakespeare like text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LueVRTGOdGD"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjrYsZyeqNkC",
        "outputId": "aada9961-aa03-4f8a-d6b0-5fff2b1c864a"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6medjfRsLD9",
        "outputId": "cd79a565-7ad1-4fe6-c031-0650e8e13bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of the dataset: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# load dataset\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f'Length of the dataset: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "c0b60970-085d-4f97-bb94-ba80aff17115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ],
      "source": [
        "# print first 200 characters\n",
        "print(text[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msGLLvVyOdGM"
      },
      "source": [
        "## Vocabulary size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_xVppKzOdGN"
      },
      "source": [
        "The vocabulary size is the **number of unique characters**.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:**\n",
        "- chars[0] is the new line character, '\\n'.\n",
        "- chars[1] is the space character, ' '."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "724bc3d1-5322-435b-c165-74b52740a164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocabulary size: 65\n"
          ]
        }
      ],
      "source": [
        "# unique characters and vocabulary size\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print('Unique characters:', ''.join(chars))\n",
        "print('Vocabulary size:', vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovlcDjjLOdGP"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuFBmxWMOdGQ"
      },
      "source": [
        "Tokenize means **convert the raw text** as a string **to some sequence of integers** according to some vocabulary of possible elements. \n",
        "\n",
        "We are building a character level language model so our tokenizer is going to simply translate individual characters into integers using a **lookup table**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "6fa8f6e4-1d1a-4f65-e36d-cdce1d16337b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder output: [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "Decoder output: hii there\n"
          ]
        }
      ],
      "source": [
        "# mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# encoder: take a string, output a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "# decoder: take a list of integers, output a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print('Encoder output:', encode('hii there'))\n",
        "print('Decoder output:', decode([46, 47, 47, 1, 58, 46, 43, 56, 43]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1rOlGPyOdGT"
      },
      "source": [
        "Large language models (LLMs) also encode text into integers but in a different schema and using a different vocabulary. They use a tokenization process that splits the text into subword units, known as **tokens**, rather than individual characters or entire words. LLMs use a bigger vocabulary size so the encoder tensors are smaller. For example, Google uses the [SentencePiece](https://github.com/google/sentencepiece?tab=readme-ov-file) tokenizer and OpenAI uses the [Tiktoken](https://github.com/openai/tiktoken) tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVGbu3PBqNkI",
        "outputId": "2f3cc769-e506-433e-d40e-68bee358bfe7"
      },
      "outputs": [],
      "source": [
        "# pip install tiktoken;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKmR8r96OdGT",
        "outputId": "e09b2ed6-360a-4189-c095-4d22ad5efd64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 50257\n",
            "Encoder output: [71, 4178, 612]\n",
            "Decoder output: hii there\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "# load gpt2 encoder\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "print('Vocabulary size:', enc.n_vocab)\n",
        "\n",
        "print('Encoder output:', enc.encode('hii there'))\n",
        "print('Decoder output:', enc.decode([71, 4178, 612]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9jYxS-hOdGU"
      },
      "source": [
        "## Build dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "4f773a79-3d13-41b5-839a-885901f5b130"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# encode the entire text dataset and store it into a PyTorch tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# split the data into train and validation sets\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n] # 90%\n",
        "val_data = data[n:]   # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
              "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
              "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
              "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
              "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
              "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
              "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
              "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
              "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
              "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
              "        53, 59])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the earlier 200 characters would look like this to the GPT\n",
        "data[:200]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMIU-QLXOdGX"
      },
      "source": [
        "## Chunk, block size, and context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASlKAAWBOdGY"
      },
      "source": [
        "We cannot feed the entire training set into a transformer all at once because that would be computationally very expensive. Therefore, when training a transformer, we sample random smaller **chunks** from the training set and train on just one chunk at a time. The length of these chunks is referred to as the **block size**, which represents the number of tokens (in this case, characters) that the transformer can process at once.\n",
        "\n",
        "In a chunk of nine tokens, there are actually eight training examples packed into it (see below). This is because, for each token, the transformer learns how to predict the next token based on its **context**, which consists of the preceding tokens in the sequence. Please note that, as we will see later, thanks to the self-attention mechanism, these examples are processed simultaneously, allowing the transformer to efficiently learn from the relationships between all the tokens in the sequence.\n",
        "\n",
        "Since the transformer is trained with contexts of varying lengths (from 1 up to the block size), during inference we can start generating text with just one token. The transformer will know how to predict the next tokens as the sequence grows, up to the block size. Once the sequence reaches the block size, we start truncating older tokens from the context to mantain the sequence length to block size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "c03a5f3b-8a30-48e4-aa35-95097d647ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk: [18, 47, 56, 57, 58, 1, 15, 47, 58]\n",
            "\n",
            "Examples:\n",
            "1) When context is [18] the target is 47\n",
            "2) When context is [18, 47] the target is 56\n",
            "3) When context is [18, 47, 56] the target is 57\n",
            "4) When context is [18, 47, 56, 57] the target is 58\n",
            "5) When context is [18, 47, 56, 57, 58] the target is 1\n",
            "6) When context is [18, 47, 56, 57, 58, 1] the target is 15\n",
            "7) When context is [18, 47, 56, 57, 58, 1, 15] the target is 47\n",
            "8) When context is [18, 47, 56, 57, 58, 1, 15, 47] the target is 58\n"
          ]
        }
      ],
      "source": [
        "block_size = 8\n",
        "\n",
        "print('Chunk:', train_data[:block_size+1].tolist())\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "print('\\nExamples:')\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f'{t+1}) When context is {context.tolist()} the target is {target}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwD3jGngSIpz"
      },
      "source": [
        "## CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgNo5yyhSLwl"
      },
      "source": [
        "CUDA exploits the advantages of GPUs over CPUs by utilizing the **parallelism** offered by GPUs' multiple cores. Unlike CPUs, which are optimized for sequential processing, GPUs have thousands of cores that can launch a large number of simultaneous threads, allowing for highly parallel execution of tasks.\n",
        "\n",
        "We are going to add the capability to run computations on a GPU, if available, to significantly speed up operations. This is particularly beneficial for tasks that can be parallelized, such as data processing and matrix operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM4M9ykhSIXH",
        "outputId": "849b11e9-53a9-477f-f560-bcffe811463c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print('Device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7UvOK7DOdGa"
      },
      "source": [
        "## Batch dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibh1l9SqOdGa"
      },
      "source": [
        "Because GPUs excel at parallel processing of data, we can stack chunks in a single tensor, known as a **batch**, that feeds into the transformer. Thus, multiple chunks can be processed simultaneously and completely independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "70d9901f-a04b-46a0-a1d4-7de6c9e98eb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs shape: (4, 8)\n",
            "tensor([[ 1, 39, 52, 42,  1, 45, 43, 50],\n",
            "        [ 1, 58, 46, 39, 58,  1, 42, 53],\n",
            "        [ 1, 61, 53, 59, 50, 42,  1, 21],\n",
            "        [59, 57, 40, 39, 52, 42,  1, 40]])\n",
            "\n",
            "Targets shape: (4, 8)\n",
            "tensor([[39, 52, 42,  1, 45, 43, 50, 42],\n",
            "        [58, 46, 39, 58,  1, 42, 53,  1],\n",
            "        [61, 53, 59, 50, 42,  1, 21,  1],\n",
            "        [57, 40, 39, 52, 42,  1, 40, 47]])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 4 # number of chunks per bacth\n",
        "block_size = 8 # chunks maximum context length\n",
        "\n",
        "# generate a small batch of chunks of inputs x and targets y\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "print('Inputs shape:', tuple(xb.shape))\n",
        "print(xb)\n",
        "print('\\nTargets shape:', tuple(yb.shape))\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpxxU1kcOdGc"
      },
      "source": [
        "# Bigram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv_h-eRYOdGd"
      },
      "source": [
        "Although a bigram model is a simple model, it is a good starting point to begin building the GPT architecture.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:**\n",
        "\n",
        "[**nn.Module**](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) is a base class in PyTorch that provides a way to define complex models by encapsulating parameters and methods that are used during training and evaluation. Some of its key features and functionalities are:\n",
        "- Parameter Management: It allows you to define parameters (weights and biases) that can be automatically registered and tracked.\n",
        "- Modularity: By inheriting from nn.Module, you can create custom layers or models, making your code modular and reusable.\n",
        "- Forward Method: This method defines how the input data flows through the model.\n",
        "- Backward Pass: The nn.Module automatically supports backpropagation through the layers defined in it, allowing you to compute gradients easily.\n",
        "- Model Evaluation: It provides methods like train() and eval() to set the mode of the model.\n",
        "- Built-in Layers: PyTorch provides a wide range of pre-defined layers (like nn.Linear) that inherit from nn.Module.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "*super().\\_\\_init\\_\\_()* calls the constructor of the parent class (in this case, nn.Module).\n",
        "\n",
        "**Note:** \n",
        "\n",
        "[nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) is a class in PyTorch that creates a simple lookup table that stores embeddings of a fixed dictionary and size. The primary purpose of this embedding layer is to convert token indices into dense vector representations (embeddings) that can be used as input to a neural network.\n",
        "\n",
        "**Note:**\n",
        "- B = Batch dimension (batch_size)\n",
        "- T = Time dimension (block_size)\n",
        "- C = Channels (vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nql_1ER53oCf"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B, T) tensor of integers\n",
        "\n",
        "        # each token reads off the logits for the next token from a lookup table\n",
        "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fB8BJyYCOdGf"
      },
      "outputs": [],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOt7NSCEZxze"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4qb4eC8Z0SF"
      },
      "source": [
        "Instead of printing the bacth loss in every iteration, the *estimate_loss()* function averages up the **loss over multiple batches**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tIpe9xadZr2k"
      },
      "outputs": [],
      "source": [
        "eval_iters = 200      # how many iterations are used to calculate the loss\n",
        "eval_interval = 10000 # every how many iterations calculate the loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() # put the model in evaluation mode\n",
        "\n",
        "    # calculate train loss and evaulation loss\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "    model.train() # put the model back in train mode\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN4Zsn3sOdGf"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "4341ef57-07cf-4efb-81e8-8c7155ac87ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:     0 / 40000   Train loss: 2.4513   Val loss: 2.4989\n",
            "Step: 10000 / 40000   Train loss: 2.4387   Val loss: 2.4847\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m /\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_iters\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m     19\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n",
            "Cell \u001b[1;32mIn[16], line 9\u001b[0m, in \u001b[0;36mget_batch\u001b[1;34m(split)\u001b[0m\n\u001b[0;32m      7\u001b[0m ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m block_size, (batch_size,))\n\u001b[0;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i:i\u001b[38;5;241m+\u001b[39mblock_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m----> 9\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mix\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     10\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
            "Cell \u001b[1;32mIn[16], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m ix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m-\u001b[39m block_size, (batch_size,))\n\u001b[0;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i:i\u001b[38;5;241m+\u001b[39mblock_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m----> 9\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39mblock_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[0;32m     10\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "batch_size = 32 # number of chunks per bacth\n",
        "learning_rate = 1e-3\n",
        "max_iters = 40000\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step:{iter:6d} /{max_iters:6d}   Train loss: {losses['train']:.4f}   Val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW-2Yfl5OdGj"
      },
      "source": [
        "## Generate from the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbXzA3uYOdGk"
      },
      "source": [
        "We are going to start the inference with the tensor [[0]]. The reason for thit is that index 0 corresponds to the new line character, '\\n'. The generate method will produce additional characters up to `max_new_tokens`.\n",
        "\n",
        "Please note that we are currently feeding the entire growing context (whatever is generated) into the model. However, because it is a bigram model, we are only using the last character to predict the next character, which explains the poor results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "41f459b7-4774-4093-8adf-aafc3ad0b49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "And iveangouecast myow ge my, upo he:\n",
            "SCEcoth an here hatotoulyowhieren?\n",
            "lleasthenou:\n",
            "I esspofouten bs'eernds Le thero t\n",
            "IOfay ags GABEdret w u whap the ate moury ed foes com wsesounighallkimear, ft tw he n COLURonthin rs o avouser bushayour! siqur.\n",
            "PO:\n",
            "INI f\n",
            "Why, uequt to alfin mband be-d ure anof \n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device) # tensor [[0]]\n",
        "print(decode(m.generate(context, max_new_tokens=300)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzrmNDrTOdG4"
      },
      "source": [
        "## Introduction to attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdzUhFQJOdG5"
      },
      "source": [
        "Attention is a **communication mechanism** that can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "\n",
        "In our bigram model, the tokens in each chunk are currently not interacting with each other, so we would like to implement a mechanism to allow them to **communicate**. In particular, we want to ensure that each token only interacts with the tokens before it in the sequence.\n",
        "\n",
        "<br>\n",
        "\n",
        "<div style=\"width: 620px; margin: 0 auto;\"\">\n",
        "    <img src=\"https://raw.githubusercontent.com/danielsimon4/language-modeling/refs/heads/main/Images/attention-graph.png\">\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "The simplest way to achieve this is by computing the **average** of the preceding tokens. For instance, the fourth token should aggregate its channels with those of the third, second, and first tokens, averaging their values.\n",
        "\n",
        "\n",
        "Consider the following chunk where every row represents a token and the columns represent the channels:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "a_{21} & a_{22} & a_{23} & ... & a_{2C} \\\\\n",
        "a_{31} & a_{32} & a_{33} & ... & a_{3C} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & ... & a_{TC}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Performing this algorithm, the tokens would interact as follows:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "\\frac{a_{11} + a_{21}}{2} & \\frac{a_{12} + a_{22}}{2} & \\frac{a_{13} + a_{23}}{2} & ... & \\frac{a_{1C} + a_{2C}}{2} \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31}}{3} & \\frac{a_{12} + a_{22} + a_{32}}{3} & \\frac{a_{13} + a_{23} + a_{33}}{3} & ... & \\frac{a_{1C} + a_{2C} + a_{3C}}{3} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31} + ... + a_{T1}}{T} & \\frac{a_{12} + a_{22} + a_{32} + ... + a_{T2}}{T} & \\frac{a_{13} + a_{23} + a_{33} + ... + a_{T3}}{T} & ... & \\frac{a_{1C} + a_{2C} + a_{3C} + ... + a_{TC}}{T}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "We can implement this averaging process with **two nested for loops**, where the inner loop iterates over each token in the chunk and computes the average of the channels for all the preceding tokens and the outer loop iterates over each chunk in the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Hs_E24uRE8kr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.2351,  0.1965],\n",
            "        [-0.2441, -1.9718],\n",
            "        [-0.1156,  1.4286],\n",
            "        [ 1.3774, -2.1948],\n",
            "        [-0.6180,  1.1502],\n",
            "        [-0.9516,  0.6960],\n",
            "        [-0.4450, -0.5644],\n",
            "        [ 0.5460, -0.1539]])\n",
            "tensor([[-0.2351,  0.1965],\n",
            "        [-0.2396, -0.8876],\n",
            "        [-0.1982, -0.1155],\n",
            "        [ 0.1957, -0.6353],\n",
            "        [ 0.0329, -0.2782],\n",
            "        [-0.1312, -0.1159],\n",
            "        [-0.1760, -0.1799],\n",
            "        [-0.0857, -0.1767]])\n"
          ]
        }
      ],
      "source": [
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "xbow = torch.zeros((B,T,C))\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0) # (C)\n",
        "\n",
        "# print only the first chunk of the batch\n",
        "print(x[0])\n",
        "print(xbow[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix multiplication efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ITaaVkuOdG8"
      },
      "source": [
        "We can improve efficiency by replacing the inner for loop with **matrix multiplication** and a **lower triangular matrix** like the one below. In addtion, PyTorch can perform multiple matrix multiplications simultaneously and independently, even when using only a CPU. This allows us to process several chunks more efficiently than with the outer loop.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "a_{21} & a_{22} & a_{23} & ... & a_{2C} \\\\\n",
        "a_{31} & a_{32} & a_{33} & ... & a_{3C} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & ... & a_{TC}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Performing this matrix multiplication, the tokens would interact as follows:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "\\frac{a_{11} + a_{21}}{2} & \\frac{a_{12} + a_{22}}{2} & \\frac{a_{13} + a_{23}}{2} & ... & \\frac{a_{1C} + a_{2C}}{2} \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31}}{3} & \\frac{a_{12} + a_{22} + a_{32}}{3} & \\frac{a_{13} + a_{23} + a_{33}}{3} & ... & \\frac{a_{1C} + a_{2C} + a_{3C}}{3} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31} + ... + a_{T1}}{T} & \\frac{a_{12} + a_{22} + a_{32} + ... + a_{T2}}{T} & \\frac{a_{13} + a_{23} + a_{33} + ... + a_{T3}}{T} & ... & \\frac{a_{1C} + a_{2C} + a_{3C} + ... + a_{TC}}{T}\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "fbe4968f-1875-4752-830a-ddeca93668d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# lower triangular matrix of all ones\n",
        "wei = torch.tril(torch.ones(T, T)) # (T, T)\n",
        "\n",
        "# lower triangular matrix\n",
        "wei /= wei.sum(1, keepdim=True) # (T, T)\n",
        "\n",
        "# matrix multiplication\n",
        "xbow2 = wei @ x # (B, T, C) = (B, T, T) x (B, T, C)\n",
        "\n",
        "# compare results\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJbu25s0swJg"
      },
      "source": [
        "## Decoder attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leA0XMKrOdG_"
      },
      "source": [
        "In fact, the lower triangular matrix contains the weights for the weighted sum of the past elements. Those **attention weights** control how much influence each past token should have on the current token. We are going to modify the way we construct this lower triangular matrix.\n",
        "\n",
        "Initially, the matrix is going to be completely zeroed out, indicating no attention is being paid yet.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Next, we are going to apply a **masking** to prevent future tokens from interacting with the past. To accomplish this, we can set those positions to −∞, so after applying Softmax their attention weights are zero.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & -∞ & -∞ & -∞ & -∞ \\\\\n",
        "0 & 0 & -∞ & -∞ & -∞ \\\\\n",
        "0 & 0 & 0 & -∞ & -∞ \\\\\n",
        "0 & 0 & 0 & 0 & -∞ \\\\\n",
        "0 & 0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, we are going to normalize the attention weights and convert them into probabilities using **Softmax**, which ensures that the attention weights in each row sum to 1, distributing influence only among the past tokens.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "We just created a **decoder attention block** because we used a triangular masking to ensure that tokens from the future cannot influence tokens from the past. Such a setup is commonly used in autoregressive models, like language modeling, where predictions are made token by token, without access to future context.\n",
        "\n",
        "In contrast, in an **encoder attention block**, there is no masking. All tokens are free to interact with each another, allowing information to flow bidirectionally across the entire sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "c84f38ab-3b4f-46f0-e501-208855752ce6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# triangular masking\n",
        "tril = torch.tril(torch.ones(T, T)) # (T, T)\n",
        "wei = torch.zeros((T,T))            # (T, T)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # (T, T)\n",
        "\n",
        "# apply softmax across each row\n",
        "wei = F.softmax(wei, dim=-1)        # (T, T)\n",
        "\n",
        "# matrix multiplication\n",
        "xbow3 = wei @ x # (B, T, C) = (B, T, T) x (B, T, C)\n",
        "\n",
        "# compare results\n",
        "torch.allclose(xbow, xbow3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M78tmbwFpPoF"
      },
      "source": [
        "## Scaled self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niSrDDZ0OdHB"
      },
      "source": [
        "However, we want the attention weights in each row to vary based on the relationships between the tokens. Different tokens might find certain other tokens more relevant or important, and we need to capture these dynamic relationships. In other words, we want to gather information from previous tokens in a **data-dependent way**.\n",
        "\n",
        "We can achieve this using a **query matrix** (representing \"what am I looking for\") and a **key matrix** (representing \"what information do I have\") for each token. The **affinity** between tokens is computed by performing matrix multiplication between the queries of one token and the transposed keys of all the other tokens. This results in an **affinity matrix**, where the query of each token measures how closely it aligns with the keys of the other tokens.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1T} \\\\\n",
        "a_{21} & a_{22} & a_{23} & ... & a_{2T} \\\\\n",
        "a_{31} & a_{32} & a_{33} & ... & a_{3T} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & ... & a_{TT}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "q_{11} & q_{12} & q_{13} & ... & q_{1H} \\\\\n",
        "q_{21} & q_{22} & q_{23} & ... & q_{2H} \\\\\n",
        "q_{31} & q_{32} & q_{33} & ... & q_{3H} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "q_{T1} & q_{T2} & q_{T3} & ... & q_{TH}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "k_{11} & k_{12} & k_{13} & ... & k_{1T} \\\\\n",
        "k_{21} & k_{22} & k_{23} & ... & k_{2T} \\\\\n",
        "k_{31} & k_{32} & k_{33} & ... & k_{3T} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "k_{H1} & k_{H2} & k_{H3} & ... & k_{HT}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Next, we are goint to apply a masking, similar to the one from before, to prevent future tokens from influencing the current token by setting those affinities to −∞.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "a_{11} & -∞ & -∞ & -∞ & -∞ \\\\\n",
        "a_{21} & a_{22} & -∞ & -∞ & -∞ \\\\\n",
        "a_{31} & a_{32} & a_{33} & -∞ & -∞ \\\\\n",
        "a_{41} & a_{42} & a_{43} & a_{44} & -∞ \\\\\n",
        "a_{51} & a_{52} & a_{53} & a_{54} & a_{55}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, we are going to apply softmax to convert these affinities into attention weights, which determine how much attention each token pays to others.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "w_{11} & 0 & 0 & 0 & 0 \\\\\n",
        "w_{21} & w_{22} & 0 & 0 & 0 \\\\\n",
        "w_{31} & w_{32} & w_{33} & 0 & 0 \\\\\n",
        "w_{41} & w_{42} & w_{43} & w_{44} & 0 \\\\\n",
        "w_{51} & w_{52} & w_{53} & w_{54} & w_{55}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "We just created a **self-attention block** because the queries, keys, and values all originate from the same input. In contrast, in a **cross-attention block**, the keys and values come from a source different from the one of the queries (such as an encoder module in a transformer model).\n",
        "\n",
        "<br>\n",
        "\n",
        "In addition, we are going to implement **scaled attention** because we aregoing to multiply the computed affinities by $\\frac{1}{\\sqrt{\\text{head\\_size}}}$ before appying the masking. This scaling is crucial because it ensures that Softmax does not produce extremely sharp distributions that concentrate too much attention in one element.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Note:** To get `q` and `k` we are using the Linear modules `key` and `query`. They are just going to apply matrix multiplication of the input with some weights.\n",
        "\n",
        "**Note:** When we do the agregation in a singe Head, we do not agregate the inputs directly. We instead agregate the values. To get `v` we use the Linear module `value` that just applies matrix multiplication of the input with some weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EDarxEWIRMKq"
      },
      "outputs": [],
      "source": [
        "B, T, C = 4, 8, 32\n",
        "head_size = 16\n",
        "\n",
        "# random imput tensor\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# single Head performing self-attention\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "# compute keys and queries\n",
        "k = key(x)   # (B, T, hs)\n",
        "q = query(x) # (B, T, hs)\n",
        "\n",
        "# compute attention scores (\"affinities\")\n",
        "# for every bacth element, a square matrix contains the affinities\n",
        "wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T) = (B, T, hs) @ (B, hs, T)\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))  # (B, T, T)\n",
        "wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "# perform the weighted aggregation of the values\n",
        "v = value(x)     # (B, T, hs)\n",
        "xbow4 = wei @ v  # (B, T, hs) = (B, T, T) @ (B, T, hs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5PVmHOiOdHD",
        "outputId": "2c04c938-b7a9-43e2-e0d6-67d7ec2f379b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3450, 0.6550, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3097, 0.3256, 0.3647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3294, 0.1735, 0.1863, 0.3109, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2001, 0.2052, 0.2034, 0.1750, 0.2164, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0998, 0.3419, 0.2265, 0.0703, 0.1754, 0.0861, 0.0000, 0.0000],\n",
              "        [0.1515, 0.0746, 0.1231, 0.1933, 0.1203, 0.2302, 0.1069, 0.0000],\n",
              "        [0.1019, 0.1119, 0.1405, 0.0578, 0.1043, 0.3486, 0.0534, 0.0816]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here’s an improved version of your explanation:\n",
        "\n",
        "---\n",
        "\n",
        "As we saw before, when we initialize the attention scores (affinities) between all tokens to 0, block communication with future tokens using a mask, and apply softmax, we get the same weights in each row:\n",
        "\n",
        "\\[\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "However, we want the weights in each row to vary based on the relationships between tokens. Different tokens find certain other tokens more relevant or important, and we need to capture these dynamic relationships. In other words, we want to **gather information from previous tokens in a data-dependent way**.\n",
        "\n",
        "**Self-attention** achieves this by using a **query matrix** (representing \"what am I looking for\") and a **key matrix** (representing \"what information do I have\") for each token. The affinity between tokens is computed by performing matrix multiplication between the queries of one token and the transposed keys of other tokens. This results in an **affinity matrix**, where the query of each token measures how closely it aligns with the keys of all other tokens.\n",
        "\n",
        "\\[\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & \\dots & a_{1T} \\\\\n",
        "a_{21} & a_{22} & a_{23} & \\dots & a_{2T} \\\\\n",
        "a_{31} & a_{32} & a_{33} & \\dots & a_{3T} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & \\dots & a_{TT}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "q_{11} & q_{12} & q_{13} & \\dots & q_{1H} \\\\\n",
        "q_{21} & q_{22} & q_{23} & \\dots & q_{2H} \\\\\n",
        "q_{31} & q_{32} & q_{33} & \\dots & q_{3H} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "q_{T1} & q_{T2} & q_{T3} & \\dots & q_{TH}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "k_{11} & k_{12} & k_{13} & \\dots & k_{1T} \\\\\n",
        "k_{21} & k_{22} & k_{23} & \\dots & k_{2T} \\\\\n",
        "k_{31} & k_{32} & k_{33} & \\dots & k_{3T} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "k_{H1} & k_{H2} & k_{H3} & \\dots & k_{HT}\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "Next, we apply masking to prevent future tokens from influencing the current token, setting those affinities to \\(-\\infty\\):\n",
        "\n",
        "\\[\n",
        "\\begin{bmatrix}\n",
        "a_{11} & -\\infty & -\\infty & -\\infty & -\\infty \\\\\n",
        "a_{21} & a_{22} & -\\infty & -\\infty & -\\infty \\\\\n",
        "a_{31} & a_{32} & a_{33} & -\\infty & -\\infty \\\\\n",
        "a_{41} & a_{42} & a_{43} & a_{44} & -\\infty \\\\\n",
        "a_{51} & a_{52} & a_{53} & a_{54} & a_{55}\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "Finally, we apply softmax normalization to convert these affinities into **attention weights**, which determine how much attention each token pays to others:\n",
        "\n",
        "\\[\n",
        "\\begin{bmatrix}\n",
        "w_{11} & 0 & 0 & 0 & 0 \\\\\n",
        "w_{21} & w_{22} & 0 & 0 & 0 \\\\\n",
        "w_{31} & w_{32} & w_{33} & 0 & 0 \\\\\n",
        "w_{41} & w_{42} & w_{43} & w_{44} & 0 \\\\\n",
        "w_{51} & w_{52} & w_{53} & w_{54} & w_{55}\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### Additional Notes:\n",
        "\n",
        "- To generate the **query** and **key** matrices (`q` and `k`), we use the Linear modules `key` and `query`, which simply apply matrix multiplication between the input data and learned weight matrices.\n",
        "\n",
        "- When we perform the attention aggregation, we don't directly combine the inputs (tokens). Instead, we aggregate **values** associated with each token. The **value matrix** (`v`) is obtained similarly, using the Linear module `value`, which multiplies the input by another weight matrix.\n",
        "\n",
        "- What we have created here is a **self-attention block**, because the queries, keys, and values all originate from the same input (denoted as `x`). In contrast, a **cross-attention block** uses queries from the input `x` but gets the keys and values from another source (such as an encoder module in a transformer model).\n",
        "\n",
        "- We are using **scaled attention** in this process, meaning that we divide the computed affinities (`wei`) by \\( \\frac{1}{\\sqrt{\\text{head\\_size}}} \\). This scaling is crucial because it prevents the softmax from producing extremely sharp (or saturated) distributions when the query and key matrices have unit variance. Without scaling, large values could cause softmax to concentrate most of the attention on one element.\n",
        "\n",
        "---\n",
        "\n",
        "This version makes the process clearer, explains how self-attention works in more detail, and highlights key points like scaling and the use of linear modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y9y1AofyMCN"
      },
      "source": [
        "## Single-head self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReKygJAkqNkm"
      },
      "source": [
        "The Head module implements a single head of self-attention.\n",
        "\n",
        "<div style=\"width: 270px\">\n",
        "    <img src=\"https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png\">\n",
        "</div>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OzOfzDA5oYc"
      },
      "source": [
        "**Note:** `trill` is not a parameter of the module so it is a **buffer** according to PyTorch naming conventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rfShVmhtyLwW"
      },
      "outputs": [],
      "source": [
        "n_embd = 32 # number of embedding dimensions\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        _, T, _ = x.shape # (B, T, n_embd)\n",
        "\n",
        "        # compute keys and queries\n",
        "        k = self.key(x)   # (B, T, hs)\n",
        "        q = self.query(x) # (B, T, hs)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T) = (B, T, hs) @ (B, hs, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B, T, hs)\n",
        "        out = wei @ v     # (B, T, hs) = (B, T, T) @ (B, T, hs)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdlHNtsXeIBZ"
      },
      "source": [
        "In attention, there is **no notion of space**, it simply acts over a set of vectors. That is why we need to positionally encode the tokens.\n",
        "\n",
        "Now, the **token embedding table** encodes the indices based on the **identity** of the tokens and gives the **token embeddings** instead of the logits. The **position embedding table** encodes the indices based on the **position** of the tokens and gives the **position embeddings**. To get the logits form the token and position embeddings we need the **language modeling head** which is just a linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RE7OGWzhdVQ5"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # token and position embeddings from embedding tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # apply one head of self-attention\n",
        "        x = self.sa_head(x)   # (B, T, n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOIcG7uC_zJs"
      },
      "source": [
        "Using single-head self-attention the validation loss went down from 2.45 to 2.35."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgPgjVli8ljO"
      },
      "source": [
        "## Multi-head self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEYNVOv38lCA"
      },
      "source": [
        "Multi-head attention is just applying **multiple attentions in parallel** and concatenating the results. It usually helps to have **multiple communication channels** of heads with smaller head size than just a single communication channel of one head.\n",
        "\n",
        "<div style=\"width: 500px\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1010/0*0KPEV8QidHkteKeY.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jR4KZkrA9G5A"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ZW__QLBRx2sf"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # 4 heads of 8-dimensional self-attention\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # token and position embeddings from embedding tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # apply multi-head self-attention\n",
        "        x = self.sa_heads(x)  # (B, T, n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l4Q8PiE_I4w"
      },
      "source": [
        "Using multi-head self-attention the validation loss went down from 2.35 to 2.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqHFnbrmGdtP"
      },
      "source": [
        "## Feedforward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8iesh6fIc94"
      },
      "source": [
        "The multi-headed self-attention did the communication, so the tokens looked at each other, but they didn't really think on what they found from the other tokens. The feedforward network acts on a per token level so once they have gathered all the data they **think on that data** individually.\n",
        "\n",
        "<div style=\"width: 800px\">\n",
        "    <img src=\"https://pbs.twimg.com/media/ESnE4IvUYAAopRf.jpg\">\n",
        "</div>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "m_ufHOCAGlto"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "13gClUAEGl_v"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # token and position embeddings from embedding tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # apply multi-head self-attention\n",
        "        x = self.sa_heads(x)  # (B, T, n_embd)\n",
        "\n",
        "        # apply feedfoward network\n",
        "        x = self.ffwd(x)      # (B, T, n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9DHYOj0Jtwz"
      },
      "source": [
        "Using a single feeforward layer the validation loss went down from 2.2 to 2.16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsSPYUfsLVff"
      },
      "source": [
        "## Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z6EagnULblp"
      },
      "source": [
        "A block **intersperses communication** (done using multi-headed self-attention) **and computation** (done using the feedforward network on all the tokens):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oxTpGBo1QAp8"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # communication\n",
        "        x = self.sa(x)\n",
        "\n",
        "        # computation\n",
        "        x = self.ffwd(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "X7yZQREAQAcP"
      },
      "outputs": [],
      "source": [
        "n_layer = 3 # number of blocks\n",
        "n_head = 4  # number of heads\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # token and position embeddings from embedding tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # apply blocks\n",
        "        x = self.blocks(x)    # (B, T, n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icIeiVxsamZP"
      },
      "source": [
        "Using 3 blocks the validation loss went down from 2.16 to 2.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-yNpAZIRpJ7"
      },
      "source": [
        "## Residual connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJMiPo9vRp4S"
      },
      "source": [
        "We are starting to get a deep neural net that suffers from optimization issues. The residual connections or skip connections are described in the paper [He et al. (2015). *Deep Residual Learning for Image Recognition*](https://arxiv.org/abs/1512.03385) and help with the optimization. The idea is you transform the data but then you have a **skip connection with addition** from the previous features **(Image 1)**.\n",
        "\n",
        "The opposite way to visualize residual connections is that you have a **residual pathway** and you have some **residual blocks** that fork off, perform some computation, and project back via addition **(Image 2)**. During the optimization the residual blocks start to contribute over time. The advantage is that at least at the initialization, during backpropagation the **gradient just flows unimpeded** from the supervision to the input.\n",
        "\n",
        "<div style=\"width: 800px\">\n",
        "    <img src=\"https://pbs.twimg.com/media/ESnE4IvUYAAopRf.jpg\">\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "It is necessary to add a projection layer after the transformations before they join back to the residual pathway. The projection layer is just a linear transformation ( *y = x · w +b* ) of the output of the tranformation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Dgp8IXTndKof"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # multi-headed self-attention\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # projection layer\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "MKRCA0rkeqyW"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # growing inner-layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rDXNPQFqNky"
      },
      "source": [
        "Apply the residual connections ( *x = x + F(x)* ) when calling the transformations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iRaEJP3BcyPW"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # communication with residual connections\n",
        "        x = x + self.sa(x)\n",
        "\n",
        "        # computation with residual connections\n",
        "        x = x + self.ffwd(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWFQJqpfh341"
      },
      "source": [
        "Using residual connections the validation loss went down from 2.1 to 1.94."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-OSSHcbiJ_2"
      },
      "source": [
        "## Layer normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPb69gYmiL0P"
      },
      "source": [
        "Layer normalization also helps with the optimization of deep neural networks and is described in the paper [Ba et al. (2016). *Layer Normalization*](https://arxiv.org/abs/1607.06450). Remember that batch normalization made sure that across the batch dimension any individual neuron had a unit Gaussian distribution (0 mean and 1 standard deviation output) at initialization.\n",
        "\n",
        "Layer normalization is identical to batch bormalization but normalizes across the rows instead of the columns, does not need the running mean and the running variance buffers, and there is no distinction beteween train and test time. The layer normalization acts on a per token level and **normalizes the features** making them unit Gaussian at initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4zQVO-obnzb_"
      },
      "outputs": [],
      "source": [
        "class LayerNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    xmean = x.mean(1, keepdim=True)\n",
        "    xvar = x.var(1, keepdim=True)\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhbWHI5eqqBy"
      },
      "source": [
        "In the paper [Vaswani et al. (2017). *Attention is All You Need paper*](https://arxiv.org/abs/1706.03762) the \"Add & Norm\" is applied after the transformations.\n",
        "\n",
        "<div style=\"width: 300px\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:824/1*RHZg5-dP9ZgvzZa73HVt2g.png\">\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "Nowadays, it is more common to apply the layer normalization **before the tranformations** (this is called **pre-norm formulation**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "z9phYV-PqRE9"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # communication with residual connections and layer norm\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "\n",
        "        # computation with residual connections and layer norm\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTNBKvL12IjL"
      },
      "source": [
        "It is also common to add a layer normalization **at the end of the transformer** and right before the final linear layer that decodes into vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YyTBVcfOuWZp"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # token and position embeddings from embedding tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # apply blocks\n",
        "        x = self.blocks(x)    # (B, T, n_embd)\n",
        "\n",
        "        # final layer norm\n",
        "        x = self.ln_f(x)      # (B, T, n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGcxackZsr6l"
      },
      "source": [
        "Using layer normalization the validation loss went down from 1.94 to 1.93."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixb71Sr0vIqC"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvi7012BxtFT"
      },
      "source": [
        "Dropout is a **regularization technique** described in the paper [Srivastava et al. (2014). *Dropout: A Simple Way to Prevent Neural Networks from Overfitting*](https://jmlr.org/papers/v15/srivastava14a.html) that consists on, in every step, **randomly shut off** some subset of neurons and train without them.\n",
        "\n",
        "<div style=\"width: 550px\">\n",
        "    <img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_6.19.24_PM.png\">\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "Dropout is added when we calculate the affinities after Softmax so we randomly prevent some of the nodes from communicating:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "llJGGH7gy6yk"
      },
      "outputs": [],
      "source": [
        "dropout = 0.2\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        _, T, _ = x.shape # (B, T, n_embd)\n",
        "\n",
        "        # compute keys and queries\n",
        "        k = self.key(x)   # (B, T, hs)\n",
        "        q = self.query(x) # (B, T, hs)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T) = (B, T, hs) @ (B, hs, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B, T, hs)\n",
        "        out = wei @ v     # (B, T, hs) = (B, T, T) @ (B, T, hs)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHYuGJrV0e8Q"
      },
      "source": [
        "Dropout is also added right after the projection back to the residual pathway:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mqU7JkU6yX6V"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # multi-headed self-attention\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # projection and dropout layers\n",
        "        out = self.dropout(self.proj(out))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "N6Jlvr6mx-XR"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # growing inner-layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZcyHhwT29o4"
      },
      "source": [
        "## Scaling up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "oBlY-vM93A-L"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 64       # number of chunks per bacth\n",
        "block_size = 256      # chunks maximum context length\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200      # how many iterations used to calculate the loss\n",
        "eval_interval = 1000   # every how many iterations calculate the loss\n",
        "learning_rate = 3e-4\n",
        "max_iters = 5000\n",
        "n_embd = 384          # number of embedding dimensions\n",
        "n_layer = 6           # number of blocks\n",
        "n_head = 6            # number of heads\n",
        "dropout = 0.2         # dropout percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlsZ4Shz_-B8",
        "outputId": "b158bbbe-90cd-4170-89fc-1694b15ccc89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.788929 M parameters\n"
          ]
        }
      ],
      "source": [
        "# intialize the model\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "3zf7rmyHABR5"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gT2So2XAIX7",
        "outputId": "2d557568-ba84-4b0c-81f9-59aef07b34bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:    0/5000   Train loss: 4.2729   Val loss: 4.2790\n",
            "Step: 1000/5000   Train loss: 1.5539   Val loss: 1.7391\n",
            "Step: 2000/5000   Train loss: 1.3202   Val loss: 1.5549\n",
            "Step: 3000/5000   Train loss: 1.2108   Val loss: 1.4956\n",
            "Step: 4000/5000   Train loss: 1.1322   Val loss: 1.4848\n",
            "Step: 4999/5000   Train loss: 1.0593   Val loss: 1.4869\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step: {iter:4d}/{max_iters:4d}   Train loss: {losses['train']:.4f}   Val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0LnkzyqCn48",
        "outputId": "dc9bd062-4d36-4b22-bcca-2aea20c2aea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "LEONTES:\n",
            "Whither, this? Hasting foe!\n",
            "This chair that banish'd his battle spirits,\n",
            "And diso mers it tellingn their devils.\n",
            "Good not! I know myself-sper, for where he they\n",
            "Command. Besides, quoth deids us the hasty ture.\n",
            "Procks shall be salinting, wanting by their death\n",
            "The whitness, wite-arwarting up with rashing feast\n",
            "With record the heed memorsel of our souls, make with chape,\n",
            "Most of the watchest hath moved, we must.\n",
            "\n",
            "Nurse:\n",
            "Provost, you speak; again, love.\n",
            "Ah, fellow, thus farewell medder-tirrion!\n",
            "And now I know follows me for my heads mark'd thee!\n",
            "Mach's spoon-bear! come on their bosoms\n",
            "And pale and pebble their own posing treats,\n",
            "Or wealth em or a scepsed, friend and bight\n",
            "Most time have done an heart,\n",
            "Forthwell amplot till way upon his soul talls,\n",
            "False their beadins his womble.\n",
            "\n",
            "SLY:\n",
            "Though he must have found you all fitter up,\n",
            "Shall we have abser'd the state of a fiend.\n",
            "But ke drowns, I with tumbly manaters\n",
            "Forbade, as deceit for this dust of rear,\n",
            "For Juliet's courtesy, for h\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
