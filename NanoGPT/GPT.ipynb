{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "# Generatively Pretrained Transformer (GPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LueVRTGOdGD"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6medjfRsLD9",
        "outputId": "1eac6327-c055-494b-f2de-f2dd9b0b11c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of dataset: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f'Length of dataset: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID6u03TkOdGJ"
      },
      "source": [
        "The first 400 characters are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "cd5bd670-93cd-4efd-caca-725d5439b67d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it \n"
          ]
        }
      ],
      "source": [
        "print(text[:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msGLLvVyOdGM"
      },
      "source": [
        "## Vocabulary size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_xVppKzOdGN"
      },
      "source": [
        "**Vocabulary size** is the number of unique characters.\n",
        "\n",
        "**Note:** chars[0] is the new line character, '\\n'. chars[1] is the space character, ' '."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "f73a5312-2ad4-4e81-f307-5a64163f624d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocabulary size: 65\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print('Unique characters:', ''.join(chars))\n",
        "print('Vocabulary size:', vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovlcDjjLOdGP"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuFBmxWMOdGQ"
      },
      "source": [
        "**Tokenize** means convert the raw text as a string to some sequence of integers according to some vocabulary of possible elements.\n",
        "\n",
        "We are building a character level language model so our tokenizer is going to simply translate individual characters into integers using a **lookup table**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "1b609536-df09-4d16-c653-fc3bcbae98dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder output: [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "Decoder output: hii there\n"
          ]
        }
      ],
      "source": [
        "# mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# Encoder: take a string, output a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "# Decoder: take a list of integers, output a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print('Encoder output:', encode('hii there'))\n",
        "print('Decoder output:', decode([46, 47, 47, 1, 58, 46, 43, 56, 43]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1rOlGPyOdGT"
      },
      "source": [
        "Google uses [SentencePiece](https://github.com/google/sentencepiece?tab=readme-ov-file) tokenizer to also encode text into integers but in a different schema and using a different vocabulary. SentencePiece does not encode indivual characters or entire words but **sub-words units**. ChatGPT uses the OpenAI's [tiktoken](https://github.com/openai/tiktoken) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKmR8r96OdGT",
        "outputId": "573be1e3-143d-4888-a473-0c1bd658b8f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 50257\n",
            "Encoder output: [71, 4178, 612]\n",
            "Decoder output: hii there\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "print('Vocabulary size:', enc.n_vocab)\n",
        "\n",
        "print('Encoder output:', enc.encode('hii there'))\n",
        "print('Decoder output:', enc.decode([71, 4178, 612]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9jYxS-hOdGU"
      },
      "source": [
        "## Build dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbNGNQqFOdGV"
      },
      "source": [
        "We can now encode the entire text dataset and store it into a PyTorch tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "b06988e7-1929-46cd-e81c-6ec03407c2b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(data[:400]) # the earlier 400 characters will look like this to the GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1ywTbw6OdGW"
      },
      "source": [
        "Split up the data into train and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n] # 90%\n",
        "val_data = data[n:]   # 10%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMIU-QLXOdGX"
      },
      "source": [
        "## Time dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASlKAAWBOdGY"
      },
      "source": [
        "We cannot feed the entire training set into a transformer all at once because that would be computationally very expensive. Thus, when training the transformer we sample random little **chunks** out of the training set and train just a chunk at a time. The maximum length of those chunks is the **block size**.\n",
        "\n",
        "\n",
        "In a chunk of nine characters there's actually eight individual examples packed into it (see below). That's because all of these characters follow each other so when we plug the chunk into a transformer, it simultaneously trains to make a prediction for every example.\n",
        "\n",
        "\n",
        "We train on all the eight examples not just for efficiency but to make the transformer network used to seeing **contexts of size 1 up to block size**. That is useful later during inference because we can start the sampling generation with just 1 character. Then, the transformer knows how to predict the next characters all the way up to block size. After block size, we start truncating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "56c26062-d13e-4d6a-ac44-0131b6f98920"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk: [18, 47, 56, 57, 58, 1, 15, 47, 58]\n",
            "\n",
            "Examples:\n",
            "1) When context is [18] the target is 47\n",
            "2) When context is [18, 47] the target is 56\n",
            "3) When context is [18, 47, 56] the target is 57\n",
            "4) When context is [18, 47, 56, 57] the target is 58\n",
            "5) When context is [18, 47, 56, 57, 58] the target is 1\n",
            "6) When context is [18, 47, 56, 57, 58, 1] the target is 15\n",
            "7) When context is [18, 47, 56, 57, 58, 1, 15] the target is 47\n",
            "8) When context is [18, 47, 56, 57, 58, 1, 15, 47] the target is 58\n"
          ]
        }
      ],
      "source": [
        "block_size = 8\n",
        "\n",
        "print('Chunk:', train_data[:block_size+1].tolist())\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "print('\\nExamples:')\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f'{t+1}) When context is {context.tolist()} the target is {target}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwD3jGngSIpz"
      },
      "source": [
        "## CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgNo5yyhSLwl"
      },
      "source": [
        "CUDA exploits the advantages of GPUs over CPUs by utilizing the **parallelism** offered by GPUs multiple cores, which allow to launch a high number of simultaneous threads.\n",
        "\n",
        "We are going to add the ability to run on a GPU if it is available so everything is faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM4M9ykhSIXH",
        "outputId": "a9af8745-3484-4757-f079-f45074adde23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print('Device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7UvOK7DOdGa"
      },
      "source": [
        "## Batch dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibh1l9SqOdGa"
      },
      "source": [
        "Because GPUs are very good at **parallel processing** of data, we have **batches of multiple chunks** stacked up in a single tensor feeding into a transformer. Thus, multiple chunks can be processed at the same time but completely independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "2ba0a9a9-1b07-452b-9ce0-2c47f6dfde14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs shape: (4, 8)\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "\n",
            "Targets shape: (4, 8)\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 4 # number of chunks per bacth\n",
        "block_size = 8 # chunks maximum context length\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "print('Inputs shape:', tuple(xb.shape))\n",
        "print(xb)\n",
        "print('\\nTargets shape:', tuple(yb.shape))\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpxxU1kcOdGc"
      },
      "source": [
        "# Bigram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv_h-eRYOdGd"
      },
      "source": [
        "Although a Bigram model is a simple model, it will be a good start for the architecture of the GPT.\n",
        "\n",
        "**Note:**\n",
        "- B = Batch (batch_size)\n",
        "- T = Time (block_size)\n",
        "- C = Channels (vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nql_1ER53oCf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B, T) tensor of integers\n",
        "\n",
        "        # each token reads off the logits for the next token from a lookup table\n",
        "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fB8BJyYCOdGf"
      },
      "outputs": [],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOt7NSCEZxze"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4qb4eC8Z0SF"
      },
      "source": [
        "Instead of printing the bacth loss, the **estimate_loss()** function averages up the loss over multiple batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tIpe9xadZr2k"
      },
      "outputs": [],
      "source": [
        "eval_iters = 200      # how many iterations used to calculate the loss\n",
        "eval_interval = 10000 # every how many iterations calculate the loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() # put the model in evaluation mode\n",
        "\n",
        "    # calculate train loss and evaulation loss\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "    model.train() # put the model back in train mode\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN4Zsn3sOdGf"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "ac3067b4-9005-4437-9775-954d9b309d6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:     0 / 40000   Train loss: 4.6355   Val loss: 4.6491\n",
            "Step: 10000 / 40000   Train loss: 2.4726   Val loss: 2.4923\n",
            "Step: 20000 / 40000   Train loss: 2.4551   Val loss: 2.4864\n",
            "Step: 30000 / 40000   Train loss: 2.4519   Val loss: 2.4940\n",
            "Step: 39999 / 40000   Train loss: 2.4518   Val loss: 2.4949\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "max_iters = 40000\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step:{iter:6d} /{max_iters:6d}   Train loss: {losses['train']:.4f}   Val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW-2Yfl5OdGj"
      },
      "source": [
        "## Generate from the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbXzA3uYOdGk"
      },
      "source": [
        "\n",
        "**Note:** We start the generation with the tensor [[0]]. That is fine because 0 is the new line character. The generate method will generate more characters up to `max_new_tokens`.\n",
        "\n",
        "**Note:** We are feeding the entire growing context (whatever is generated) into the model. However, because it is a bigram model we are only using the last previous character to predict the next charater."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "a91bdcd7-5292-4ac0-cbf3-34c91f7a5ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "atecotofothe ofory.\n",
            "S:\n",
            "ICASI All n my their blil toce: ton?\n",
            "Thigous m:\n",
            "I tsours, gan thand fais ifif nowe? tipil\n",
            "Thetour y an ounlirounch drit lat mavets spawind wous by fr. id,\n",
            "Whad butheetlathithe u k I fand ng, witolm he outhatigu bery the n m ben,\n",
            "Wh MESithay.\n",
            "\n",
            "\n",
            "\n",
            "LI, pa it t icee wow bautimin th\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=300)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzrmNDrTOdG4"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2Eu7L7tjMmD"
      },
      "source": [
        "Attention is a **communication mechanism** that can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ypogaP0s4CX"
      },
      "source": [
        "## Intro to attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdzUhFQJOdG5"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "The tokens in each chunk are currently \"not talking to each other\" so we would like to **\"couple them\"**. In particular, we want to couple them so that the tokens **only communicate with the tokens before them** in the sequence.\n",
        "\n",
        "The easiest way to achieve that is computing the **average** of the preceding tokens. Thus, the fourth token should take its channels and also the channels from the third step, the second step, and the first step, and average those up.\n",
        "\n",
        "<br>\n",
        "\n",
        "Consider the following chunk where every row would be a token and the columns the channels:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "a_{21} & a_{22} & a_{23} & ... & a_{2C} \\\\\n",
        "a_{31} & a_{32} & a_{33} & ... & a_{3C} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & ... & a_{TC}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The tokens would communicate as follows:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "\\frac{a_{11} + a_{21}}{2} & \\frac{a_{12} + a_{22}}{2} & \\frac{a_{13} + a_{23}}{2} & ... & \\frac{a_{1C} + a_{2C}}{2} \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31}}{3} & \\frac{a_{12} + a_{22} + a_{32}}{3} & \\frac{a_{13} + a_{23} + a_{33}}{3} & ... & \\frac{a_{1C} + a_{2C} + a_{3C}}{3} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31} + ... + a_{T1}}{T} & \\frac{a_{12} + a_{22} + a_{32} + ... + a_{T2}}{T} & \\frac{a_{13} + a_{23} + a_{33} + ... + a_{T3}}{T} & ... & \\frac{a_{1C} + a_{2C} + a_{3C} + ... + a_{TC}}{T}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "We can implement that with two nested for loops:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Hs_E24uRE8kr"
      },
      "outputs": [],
      "source": [
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "xbow = torch.zeros((B,T,C))\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0) # (C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLcgROsEOdG6"
      },
      "source": [
        "Looking only at the first chunk of the batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAiqA5ASOdG7",
        "outputId": "4e5082e2-1e15-48b8-bd3b-f88bef1ab8a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.8385,  0.7409],\n",
            "        [ 0.5635, -0.6097],\n",
            "        [ 1.8720,  0.0590],\n",
            "        [ 0.1869, -0.2114],\n",
            "        [ 1.5499,  0.2369],\n",
            "        [-2.3956, -0.3363],\n",
            "        [ 2.2205, -0.1176],\n",
            "        [ 0.0070, -1.0434]])\n",
            "tensor([[-0.8385,  0.7409],\n",
            "        [-0.1375,  0.0656],\n",
            "        [ 0.5323,  0.0634],\n",
            "        [ 0.4460, -0.0053],\n",
            "        [ 0.6668,  0.0431],\n",
            "        [ 0.1564, -0.0201],\n",
            "        [ 0.4512, -0.0340],\n",
            "        [ 0.3957, -0.1602]])\n"
          ]
        }
      ],
      "source": [
        "print(x[0])\n",
        "print(xbow[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ITaaVkuOdG8"
      },
      "source": [
        "We can be more efficient using matrix multiplication and a lower triangular matrix like the one below:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "\\frac{a_{11} + a_{21}}{2} & \\frac{a_{12} + a_{22}}{2} & \\frac{a_{13} + a_{23}}{2} & ... & \\frac{a_{1C} + a_{2C}}{2} \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31}}{3} & \\frac{a_{12} + a_{22} + a_{32}}{3} & \\frac{a_{13} + a_{23} + a_{33}}{3} & ... & \\frac{a_{1C} + a_{2C} + a_{3C}}{3} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31} + ... + a_{T1}}{T} & \\frac{a_{12} + a_{22} + a_{32} + ... + a_{T2}}{T} & \\frac{a_{13} + a_{23} + a_{33} + ... + a_{T3}}{T} & ... & \\frac{a_{1C} + a_{2C} + a_{3C} + ... + a_{TC}}{T}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "a_{21} & a_{22} & a_{23} & ... & a_{2C} \\\\\n",
        "a_{31} & a_{32} & a_{33} & ... & a_{3C} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & ... & a_{TC}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:** PyTorch will apply the matrix multiplication in all the batch elements (chunks) at the same time and independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "b60f836e-34ad-4ec2-a0cd-86a938968f57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei /= wei.sum(1, keepdim=True) # (T, T)\n",
        "xbow2 = wei @ x # (B, T, C) = (B, T, T) x (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJbu25s0swJg"
      },
      "source": [
        "## Decoder attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leA0XMKrOdG_"
      },
      "source": [
        "In ffact, the lower triangular matrix contains the weights for the weighted sum of the past elements. Those weights are **the interaction strength or affinity**, how much of each token from the past we want to aggregate and average up. We are going to modify the way we construct the lower triangular matrix. It will start being:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Then, we will stop the communication with the future tokens by initializing those affinities to -inf:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & -inf & -inf & -inf & -inf \\\\\n",
        "0 & 0 & -inf & -inf & -inf \\\\\n",
        "0 & 0 & 0 & -inf & -inf \\\\\n",
        "0 & 0 & 0 & 0 & -inf \\\\\n",
        "0 & 0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, we will normalize with Softmax and get:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:** We just created a **decoder attention block** because it has triangular masking so that nodes from the future never talk to the past. It is usually used in autoregressive settings, like language modeling. In an encoder attention block there is no masking, allowing all tokens to communicate with each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "863af79e-2254-4dbc-dd71-f8987dc0d14f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M78tmbwFpPoF"
      },
      "source": [
        "## Single-head self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niSrDDZ0OdHB"
      },
      "source": [
        "As we saw before, when we initialize the affinities between all the different tokens to 0, stop the communication with futurw tokens, and apply Softmax, we get the same weights in every row:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "We want the weights in every row to change depending on the affinities between the tokens. Different tokens will find other tokens more or less interesting. Thus, we want to **gather data from the past in a data dependent way**.\n",
        "\n",
        "**Self-attention** achieves that with a **query matrix** (what am I looking for) and a **key matrix** (what do I contain) for every token. The matrix multiplication of the querys with the transpose of the keys produces the affinities. Thus, the queries of a token dot product with the keys of all the other tokens. If the key and the query align, the affinity will be high for that iteraction.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1T} \\\\\n",
        "a_{21} & a_{22} & a_{23} & ... & a_{2T} \\\\\n",
        "a_{31} & a_{32} & a_{33} & ... & a_{3T} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & ... & a_{TT}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "q_{11} & q_{12} & q_{13} & ... & q_{1H} \\\\\n",
        "q_{21} & q_{22} & q_{23} & ... & q_{2H} \\\\\n",
        "q_{31} & q_{32} & q_{33} & ... & q_{3H} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "q_{T1} & q_{T2} & q_{T3} & ... & q_{TH}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "k_{11} & k_{12} & k_{13} & ... & k_{1T} \\\\\n",
        "k_{21} & k_{22} & k_{23} & ... & k_{2T} \\\\\n",
        "k_{31} & k_{32} & k_{33} & ... & k_{3T} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "k_{H1} & k_{H2} & k_{H3} & ... & k_{HT}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Then, we will stop the communication with the future tokens by initializing those affinities to -inf:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "a_{11} & -inf & -inf & -inf & -inf \\\\\n",
        "a_{21} & a_{22} & -inf & -inf & -inf \\\\\n",
        "a_{31} & a_{32} & a_{33} & -inf & -inf \\\\\n",
        "a_{41} & a_{42} & a_{43} & a_{44} & -inf \\\\\n",
        "a_{51} & a_{52} & a_{53} & a_{54} & a_{55}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, we will normalize with Softmax and get:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "w_{11} & 0 & 0 & 0 & 0 \\\\\n",
        "w_{21} & w_{22} & 0 & 0 & 0 \\\\\n",
        "w_{31} & w_{32} & w_{33} & 0 & 0 \\\\\n",
        "w_{41} & w_{42} & w_{43} & w_{44} & 0 \\\\\n",
        "w_{51} & w_{52} & w_{53} & w_{54} & w_{55}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:** To get `q` and `k` we are using the Linear modules `key` and `query`. They are just going to apply matrix multiplication of the input with some weights.\n",
        "\n",
        "**Note:** When we do the agregation in a singe Head, we do not agregate the inputs directly. We instead agregate the values. To get `v` we use the Linear module `value` that just applies matrix multiplication of the input with some weights.\n",
        "\n",
        "**Note:** We just created a **self-attention block** because the keys, the queries, and the values are all produced from the same source x. In a cross-attention block, the queries still get produced from x, but the keys and the values come from some other external source (e.g. an encoder module).\n",
        "\n",
        "**Note:** We also used **scaled attention** because we divided `wey` by 1/sqrt(head_size). It is important because when `q` are `k` are unit variance, `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much a value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EDarxEWIRMKq"
      },
      "outputs": [],
      "source": [
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# single Head performing self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "# compute keys and queries\n",
        "k = key(x)   # (B, T, hs)\n",
        "q = query(x) # (B, T, hs)\n",
        "\n",
        "# compute attention scores (\"affinities\")\n",
        "# for every bacth element, a square matrix contains the affinities\n",
        "wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T) = (B, T, hs) @ (B, hs, T)\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))  # (B, T, T)\n",
        "wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "# perform the weighted aggregation of the values\n",
        "v = value(x)     # (B, T, hs)\n",
        "xbow4 = wei @ v  # (B, T, hs) = (B, T, T) @ (B, T, hs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5PVmHOiOdHD",
        "outputId": "99d9b255-f731-43eb-b20d-ef0aae749456"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5869, 0.4131, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3840, 0.2195, 0.3965, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2434, 0.1903, 0.2100, 0.3563, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1603, 0.3165, 0.1600, 0.1751, 0.1880, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1917, 0.1537, 0.2532, 0.1182, 0.1714, 0.1118, 0.0000, 0.0000],\n",
              "        [0.1065, 0.2038, 0.1227, 0.0485, 0.2147, 0.1975, 0.1062, 0.0000],\n",
              "        [0.1073, 0.1170, 0.1459, 0.0803, 0.0903, 0.2218, 0.1404, 0.0971]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y9y1AofyMCN"
      },
      "source": [
        "## Head module (Fix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OzOfzDA5oYc"
      },
      "source": [
        "**Note:** `trill` is not a parameter of the module so it is a **buffer** according to PyTorch naming conventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rfShVmhtyLwW"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        # compute keys and queries\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T) = (B, T, hs) @ (B, hs, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B, T, hs)\n",
        "        out = wei @ v     # (B, T, hs) = (B, T, T) @ (B, T, hs)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdlHNtsXeIBZ"
      },
      "source": [
        "**Note:** In attention, there is **no notion of space**, it simply acts over a set of vectors. That is why we need to positionally encode the tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RE7OGWzhdVQ5"
      },
      "outputs": [],
      "source": [
        "n_embd = 32 # number of embedding dimensions\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # identity and positional embeddings from lookup tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,n_embd)\n",
        "        x = tok_emb + pos_emb # (B,T,n_embd)\n",
        "\n",
        "        # apply one head of self-attention\n",
        "        x = self.sa_head(x) # (B,T,n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOIcG7uC_zJs"
      },
      "source": [
        "Using single-head self-attention the validation loss went down from 2.45 to 2.35."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgPgjVli8ljO"
      },
      "source": [
        "## Multi-head self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEYNVOv38lCA"
      },
      "source": [
        "Multi-head attention is just applying **multiple attentions in parallel** and concatenating the results. It usually helps to have multiple communication channels of heads with smaller head size than just a single communication channel of one head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jR4KZkrA9G5A"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ZW__QLBRx2sf"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # 4 heads of 8-dimensional self-attention\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # identity and positional embeddings from lookup tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,n_embd)\n",
        "        x = tok_emb + pos_emb # (B,T,n_embd)\n",
        "\n",
        "        # apply multi-head self-attention\n",
        "        x = self.sa_heads(x) # (B,T,n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l4Q8PiE_I4w"
      },
      "source": [
        "Using multi-head self-attention the validation loss went down from 2.35 to 2.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqHFnbrmGdtP"
      },
      "source": [
        "## Feedforward layer (fix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8iesh6fIc94"
      },
      "source": [
        "The multi-headed self-attention did the communication, so the tokens looked at each other, but they didn't really think on what they found from the other tokens.\n",
        "\n",
        "The feedforward layer acts on a per token level so once they have gathered all the data they **think on that data** individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "m_ufHOCAGlto"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "13gClUAEGl_v"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # identity and positional embeddings from lookup tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,n_embd)\n",
        "        x = tok_emb + pos_emb # (B,T,n_embd)\n",
        "\n",
        "        # apply multi-head self-attention\n",
        "        x = self.sa_heads(x) # (B,T,n_embd)\n",
        "\n",
        "        # apply the FeedFoward layer\n",
        "        x = self.ffwd(x) # (B,T,n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9DHYOj0Jtwz"
      },
      "source": [
        "Using a single feeforward layer the validation loss went down from 2.2 to 2.16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsSPYUfsLVff"
      },
      "source": [
        "## Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z6EagnULblp"
      },
      "source": [
        "A block **intersperses communication** (done using multi-headed self-attention) **and computation** (done using the feedforward network on all the tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oxTpGBo1QAp8"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # communication\n",
        "        x = self.sa(x)\n",
        "\n",
        "        # computation\n",
        "        x = self.ffwd(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "X7yZQREAQAcP"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "            Block(n_embd, n_head=4),\n",
        "        )\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # identity and positional embeddings from lookup tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,n_embd)\n",
        "        x = tok_emb + pos_emb # (B,T,n_embd)\n",
        "\n",
        "        # apply blocks\n",
        "        x = self.blocks(x) # (B,T,n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icIeiVxsamZP"
      },
      "source": [
        "Using 3 blocks the validation loss went down from 2.16 to 2.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-yNpAZIRpJ7"
      },
      "source": [
        "## Residual connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJMiPo9vRp4S"
      },
      "source": [
        "We are starting to get a deep neural net that suffers from optimization issues.\n",
        "\n",
        "The residual connections or skip connections were introduced in the paper [He et al. (2015). *Deep Residual Learning for Image Recognition*](https://arxiv.org/abs/1512.03385) and help with the optimization. The idea is you transform the data but then you have a skip connection with addition from the previous features:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Another way to visualize it is that you have a residual pathway and you have some residual blocks (that fork off, perform some computation, and project back via addition). During the optimization the residual blocks start to contribute over time. The advantage is that at least at the initialization, during backpropagation the gradient just flows unimpeded from the supervision to the input.\n",
        "\n",
        "[IMAGE]\n",
        "\n",
        "**Note:** The projection is just a linear transformation (*y=xw+b*) of the output when going back to the residual pathway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Dgp8IXTndKof"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # multi-headed self-attention\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # projection\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "MKRCA0rkeqyW"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # growing inner-layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "iRaEJP3BcyPW"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # communication with residual connections\n",
        "        x = x + self.sa(x)\n",
        "\n",
        "        # computation with residual connections\n",
        "        x = x + self.ffwd(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWFQJqpfh341"
      },
      "source": [
        "Using residual connections the validation loss went down from 2.1 to 1.94."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-OSSHcbiJ_2"
      },
      "source": [
        "## Layer Norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPb69gYmiL0P"
      },
      "source": [
        "Layer Norm also helps with the optimization of deep neural networks and is described in the paper [Ba et al. (2016). *Layer Normalization*](https://arxiv.org/abs/1607.06450). Remember that Batch Normalization made sure that across the batch dimension any individual neuron had unit gaussian distribution (0 mean and 1 standard deviation output).\n",
        "\n",
        "Layer Normalization is identical to Batch Normalization but normalizes across the rows instead of the columns, does not need the running mean and the running variance buffers, and there is no distinction beteween train and test time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "4zQVO-obnzb_"
      },
      "outputs": [],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    xmean = x.mean(1, keepdim=True)\n",
        "    xvar = x.var(1, keepdim=True)\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhbWHI5eqqBy"
      },
      "source": [
        "In the paper [Vaswani et al. (2017). *Attention is All You Need paper*](https://arxiv.org/abs/1706.03762) the \"Add & Norm\" is applied after the transformation. Nowadays, it is more common to apply the layer norm before the tranformation (this is called **pre-norm formulation**).\n",
        "\n",
        "**Note:** The layer normalization acts on a per token level and normalizes the features making them unit Gaussian at initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "z9phYV-PqRE9"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # communication with residual connections and layer norm\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "\n",
        "        # computation with residual connections and layer norm\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTNBKvL12IjL"
      },
      "source": [
        "It is also common to add a layer norm at the end of the transformer and right before the final linear layer that decodes into vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "YyTBVcfOuWZp"
      },
      "outputs": [],
      "source": [
        "n_layer = 3 # number of blocks\n",
        "n_head = 4  # number of heads\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # identity and positional embeddings from lookup tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,n_embd)\n",
        "        x = tok_emb + pos_emb # (B,T,n_embd)\n",
        "\n",
        "        # apply blocks\n",
        "        x = self.blocks(x) # (B,T,n_embd)\n",
        "\n",
        "        # final layer norm\n",
        "        x = self.ln_f(x) # (B,T,n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGcxackZsr6l"
      },
      "source": [
        "Using layer normalization the validation loss went down from 1.94 to 1.94."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixb71Sr0vIqC"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvi7012BxtFT"
      },
      "source": [
        "Dropout is a regularization technique described in the paper [Srivastava et al. (2014). *Dropout: A Simple Way to Prevent Neural Networks from Overfitting*](https://jmlr.org/papers/v15/srivastava14a.html)\n",
        "and it consists on randomly shut off some subset of neurons every forward backward pass and train without them.\n",
        "\n",
        "IMAGE\n",
        "\n",
        "Dropout is added when we calculate the affinities after Softmax so we randomly prevent some of the nodes from communicating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llJGGH7gy6yk"
      },
      "outputs": [],
      "source": [
        "dropout = 0.2\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        # compute keys and queries\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T) = (B, T, hs) @ (B, hs, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B, T, hs)\n",
        "        out = wei @ v     # (B, T, hs) = (B, T, T) @ (B, T, hs)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHYuGJrV0e8Q"
      },
      "source": [
        "Dropout is also added right after the projection back to the residual pathway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqU7JkU6yX6V"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # multi-headed self-attention\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # projection\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6Jlvr6mx-XR"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # growing inner-layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZcyHhwT29o4"
      },
      "source": [
        "## Scaling up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBlY-vM93A-L"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 64  # number of chunks per bacth\n",
        "block_size = 256 # chunks maximum context length\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200      # how many iterations used to calculate the loss\n",
        "eval_interval = 500   # every how many iterations calculate the loss\n",
        "learning_rate = 3e-4\n",
        "max_iters = 5000\n",
        "n_embd = 384 # number of embedding dimensions\n",
        "n_layer = 6 # number of blocks\n",
        "n_head = 6  # number of heads\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "MlsZ4Shz_-B8"
      },
      "outputs": [],
      "source": [
        "# intialize the model\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "3zf7rmyHABR5"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gT2So2XAIX7",
        "outputId": "a1590a8f-148d-4adb-9b2c-9c7c9e0cca27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:     0 / 40000   Train loss: 4.3091   Val loss: 4.3191\n",
            "Step: 10000 / 40000   Train loss: 1.8911   Val loss: 2.0113\n",
            "Step: 20000 / 40000   Train loss: 1.8433   Val loss: 1.9633\n",
            "Step: 30000 / 40000   Train loss: 1.8120   Val loss: 1.9491\n",
            "Step: 39999 / 40000   Train loss: 1.7939   Val loss: 1.9615\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step:{iter:5d} /{max_iters:5d}   Train loss: {losses['train']:.4f}   Val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0LnkzyqCn48",
        "outputId": "161a1d6b-ff16-4990-cbe7-ee9768e31344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "NORFOLK:\n",
            "Hell mad; be look'd can this prepard's,\n",
            "Lord:\n",
            "It slain; but this Edward as of have flell night?\n",
            "\n",
            "QUE:\n",
            "She fepon.\n",
            "\n",
            "KING XI:\n",
            "If that cousin?\n",
            "\n",
            "FRIZEL:\n",
            "God's god,\n",
            "My slord.\n",
            "\n",
            "BULINGANIA:\n",
            "What, mle own but speak your his foor spatch'd free accenged Heaven be would in that gefected,\n",
            "What is despe\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=300)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
