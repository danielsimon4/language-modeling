{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "# Generatively Pretrained Transformer (GPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to create a Generatively Pretrained Transformer (GPT) and train it on the [Shakespeare text](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt) dataset which contains around 1M characters. Once the model is trained, it will genearte Shakespeare like text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The **transformer** model was introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017) by Vaswani et al. It is a neural network architecture based on the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when processing text.\n",
        "\n",
        "The transformer has two parts:\n",
        "- The **encoder** reads and processes input data (like a sentence).\n",
        "- The **decoder** generates an output based on that processed input.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "A **Generative Pre-trained Transformer (GPT)** is a specific type of transformer model that uses **only the decoder** part of the transformer architecture. GPT models are autoregressive, meaning they generate text one word at a time, using the previous words to predict the next.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LueVRTGOdGD"
      },
      "source": [
        "## 7.2 Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjrYsZyeqNkC",
        "outputId": "aada9961-aa03-4f8a-d6b0-5fff2b1c864a"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6medjfRsLD9",
        "outputId": "cd79a565-7ad1-4fe6-c031-0650e8e13bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of the dataset: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# load dataset\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f'Length of the dataset: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "c0b60970-085d-4f97-bb94-ba80aff17115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n"
          ]
        }
      ],
      "source": [
        "# print first 200 characters\n",
        "print(text[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msGLLvVyOdGM"
      },
      "source": [
        "## 7.3 Vocabulary size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_xVppKzOdGN"
      },
      "source": [
        "The vocabulary size is the **number of unique characters**.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:**\n",
        "- chars[0] is the new line character, '\\n'.\n",
        "- chars[1] is the space character, ' '."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "724bc3d1-5322-435b-c165-74b52740a164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocabulary size: 65\n"
          ]
        }
      ],
      "source": [
        "# unique characters and vocabulary size\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print('Unique characters:', ''.join(chars))\n",
        "print('Vocabulary size:', vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovlcDjjLOdGP"
      },
      "source": [
        "## 7.4 Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuFBmxWMOdGQ"
      },
      "source": [
        "Tokenize means **convert the raw text** as a string **to some sequence of integers** according to some vocabulary of possible elements. \n",
        "\n",
        "We are building a character level language model so our tokenizer is going to simply translate individual characters into integers using a **lookup table**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "6fa8f6e4-1d1a-4f65-e36d-cdce1d16337b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder output: [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "Decoder output: hii there\n"
          ]
        }
      ],
      "source": [
        "# mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# encoder: take a string, output a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "# decoder: take a list of integers, output a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print('Encoder output:', encode('hii there'))\n",
        "print('Decoder output:', decode([46, 47, 47, 1, 58, 46, 43, 56, 43]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1rOlGPyOdGT"
      },
      "source": [
        "Large language models (LLMs) also encode text into integers but in a different schema and using a different vocabulary. They use a tokenization process that splits the text into subword units, known as **tokens**, rather than individual characters or entire words. LLMs use a bigger vocabulary size so the encoder tensors are smaller. For example, Google uses the [SentencePiece](https://github.com/google/sentencepiece?tab=readme-ov-file) tokenizer and OpenAI uses the [Tiktoken](https://github.com/openai/tiktoken) tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVGbu3PBqNkI",
        "outputId": "2f3cc769-e506-433e-d40e-68bee358bfe7"
      },
      "outputs": [],
      "source": [
        "# pip install tiktoken;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKmR8r96OdGT",
        "outputId": "e09b2ed6-360a-4189-c095-4d22ad5efd64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 50257\n",
            "Encoder output: [71, 4178, 612]\n",
            "Decoder output: hii there\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "# load gpt2 encoder\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "print('Vocabulary size:', enc.n_vocab)\n",
        "\n",
        "print('Encoder output:', enc.encode('hii there'))\n",
        "print('Decoder output:', enc.decode([71, 4178, 612]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9jYxS-hOdGU"
      },
      "source": [
        "## 7.5 Build dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "4f773a79-3d13-41b5-839a-885901f5b130"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# encode the entire text dataset and store it into a PyTorch tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# split the data into train and validation sets\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n] # 90%\n",
        "val_data = data[n:]   # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
              "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
              "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
              "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
              "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
              "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
              "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
              "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
              "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
              "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
              "        53, 59])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the earlier 200 characters would look like this to the GPT\n",
        "data[:200]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMIU-QLXOdGX"
      },
      "source": [
        "## 7.6 Chunk, block size, and context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASlKAAWBOdGY"
      },
      "source": [
        "We cannot feed the entire training set into a transformer all at once because that would be computationally very expensive. Therefore, when training a transformer, we sample random smaller **chunks** from the training set and train on just one chunk at a time. The length of these chunks is referred to as the **block size**, which represents the number of tokens (in this case, characters) that the transformer can process at once.\n",
        "\n",
        "In a chunk of nine tokens, there are actually eight training examples packed into it (see below). This is because, for each token, the transformer learns how to predict the next token based on its **context**, which consists of the preceding tokens in the sequence. Please note that, as we will see later, thanks to the self-attention mechanism, these examples are processed simultaneously, allowing the transformer to efficiently learn from the relationships between all the tokens in the sequence.\n",
        "\n",
        "Since the transformer is trained with contexts of varying lengths (from 1 up to the block size), during inference we can start generating text with just one token. The transformer will know how to predict the next tokens as the sequence grows, up to the block size. Once the sequence reaches the block size, we start truncating older tokens from the context to mantain the sequence length to block size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "c03a5f3b-8a30-48e4-aa35-95097d647ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk: [18, 47, 56, 57, 58, 1, 15, 47, 58]\n",
            "\n",
            "Examples:\n",
            "1) When context is [18] the target is 47\n",
            "2) When context is [18, 47] the target is 56\n",
            "3) When context is [18, 47, 56] the target is 57\n",
            "4) When context is [18, 47, 56, 57] the target is 58\n",
            "5) When context is [18, 47, 56, 57, 58] the target is 1\n",
            "6) When context is [18, 47, 56, 57, 58, 1] the target is 15\n",
            "7) When context is [18, 47, 56, 57, 58, 1, 15] the target is 47\n",
            "8) When context is [18, 47, 56, 57, 58, 1, 15, 47] the target is 58\n"
          ]
        }
      ],
      "source": [
        "block_size = 8\n",
        "\n",
        "print('Chunk:', train_data[:block_size+1].tolist())\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "print('\\nExamples:')\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f'{t+1}) When context is {context.tolist()} the target is {target}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwD3jGngSIpz"
      },
      "source": [
        "## 7.8 CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgNo5yyhSLwl"
      },
      "source": [
        "CUDA exploits the advantages of GPUs over CPUs by utilizing the **parallelism** offered by GPUs' multiple cores. Unlike CPUs, which are optimized for sequential processing, GPUs have thousands of cores that can launch a large number of simultaneous threads, allowing for highly parallel execution of tasks.\n",
        "\n",
        "We are going to add the capability to run computations on a GPU, if available, to significantly speed up operations. This is particularly beneficial for tasks that can be parallelized, such as data processing and matrix operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM4M9ykhSIXH",
        "outputId": "849b11e9-53a9-477f-f560-bcffe811463c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print('Device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7UvOK7DOdGa"
      },
      "source": [
        "## 7.9 Batch dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibh1l9SqOdGa"
      },
      "source": [
        "Because GPUs excel at parallel processing of data, we can stack chunks in a single tensor, known as a **batch**, that feeds into the transformer. Thus, multiple chunks can be processed simultaneously and completely independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "70d9901f-a04b-46a0-a1d4-7de6c9e98eb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs shape: (4, 8)\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "\n",
            "Targets shape: (4, 8)\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 4 # number of chunks per bacth\n",
        "block_size = 8 # chunks maximum context length\n",
        "\n",
        "# generate a small batch of chunks of inputs x and targets y\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "print('Inputs shape:', tuple(xb.shape))\n",
        "print(xb)\n",
        "print('\\nTargets shape:', tuple(yb.shape))\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.10 Calculate loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "def calculate_loss(logits, targets):\n",
        "    \"Calculate the cross-entropy loss if targets are available.\"\n",
        "    \n",
        "    if targets is None:\n",
        "        return None\n",
        "    else:\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B * T, C)\n",
        "        targets = targets.view(B * T)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.11 Generate tokens function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_tokens(model, idx, max_new_tokens, block_size):\n",
        "    \"\"\"Generate tokens by predicting one token at a time.\"\"\"\n",
        "    \n",
        "    # idx is the current context\n",
        "    # in each iteration idx will grow:\n",
        "    # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # crop the context to the last block_size tokens\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "\n",
        "        # get logits for current context (calling forward method)\n",
        "        logits, _ = model(idx_cond) # (B, T, C)\n",
        "        \n",
        "        # focus only on the last time-step because\n",
        "        # those are the predictions for what comes next\n",
        "        logits = logits[:, -1, :] # (B, C)\n",
        "        \n",
        "        # apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "        \n",
        "        # sample the next token from the distribution\n",
        "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "        # append the sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    \n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpxxU1kcOdGc"
      },
      "source": [
        "## 7.12 Bigram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv_h-eRYOdGd"
      },
      "source": [
        "Although a bigram model is a very simple model, it is a good starting point to begin building the GPT architecture.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:**\n",
        "\n",
        "[**nn.Module**](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) is a base class in PyTorch that provides a way to define complex models by encapsulating parameters and methods that are used during training and evaluation. Some of its key features and functionalities are:\n",
        "- Parameter Management: It allows you to define parameters (weights and biases) that can be automatically registered and tracked.\n",
        "- Forward Method: This method defines how the input data flows through the model.\n",
        "- Backward Pass: The nn.Module automatically supports backpropagation through the layers defined in it, allowing you to compute gradients easily.\n",
        "- Model Evaluation: It provides methods like train() and eval() to set the mode of the model.\n",
        "- Built-in Layers: PyTorch provides a wide range of pre-defined layers (like nn.Linear) that inherit from nn.Module.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "*super().\\_\\_init\\_\\_()* calls the constructor of the parent class (in this case, nn.Module).\n",
        "\n",
        "**Note:** \n",
        "\n",
        "[**nn.Embedding**](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) is a class in PyTorch that creates a simple lookup table that stores embeddings of a fixed dictionary and size. The primary purpose of embedding layers is to map sequences of token indices into dense vector representations, knowns as embeddings. In this case, however, since we are implementing a bigram model, we are going to use an embedding table so that each token reads off the logits for the next token.\n",
        "\n",
        "**Note:**\n",
        "- B = Batch dimension (batch_size)\n",
        "- T = Time dimension (block_size)\n",
        "- C = Channels (vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nql_1ER53oCf"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BigramModel(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # get logits from lookup table\n",
        "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "        \n",
        "        # calculate loss\n",
        "        loss = calculate_loss(logits, targets)\n",
        "        \n",
        "        return logits, loss\n",
        "    \n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \n",
        "        return generate_tokens(self, idx, max_new_tokens, block_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fB8BJyYCOdGf"
      },
      "outputs": [],
      "source": [
        "model = BigramModel()\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOt7NSCEZxze"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4qb4eC8Z0SF"
      },
      "source": [
        "Instead of printing the bacth loss in every iteration, the *estimate_loss()* function averages up the **loss over multiple batches**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tIpe9xadZr2k"
      },
      "outputs": [],
      "source": [
        "eval_iters = 200      # how many iterations are used to calculate the loss\n",
        "eval_interval = 10000 # every how many iterations calculate the loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() # put the model in evaluation mode\n",
        "\n",
        "    # calculate train loss and evaulation loss\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "    model.train() # put the model back in train mode\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN4Zsn3sOdGf"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "4341ef57-07cf-4efb-81e8-8c7155ac87ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:     0 / 40000   Train loss: 4.6355   Val loss: 4.6491\n",
            "Step: 10000 / 40000   Train loss: 2.4726   Val loss: 2.4923\n",
            "Step: 20000 / 40000   Train loss: 2.4551   Val loss: 2.4864\n",
            "Step: 30000 / 40000   Train loss: 2.4519   Val loss: 2.4940\n",
            "Step: 39999 / 40000   Train loss: 2.4518   Val loss: 2.4949\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32 # number of chunks per bacth\n",
        "learning_rate = 1e-3\n",
        "max_iters = 40000\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step:{iter:6d} /{max_iters:6d}   Train loss: {losses['train']:.4f}   Val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW-2Yfl5OdGj"
      },
      "source": [
        "## Generate from the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbXzA3uYOdGk"
      },
      "source": [
        "We are going to start the inference with the tensor [[0]]. The reason for thit is that index 0 corresponds to the new line character, '\\n'. The generate method will produce additional characters up to max_new_tokens.\n",
        "\n",
        "Please note that we are currently feeding the entire growing context (whatever is generated) into the model. However, because it is a bigram model, we are only using the last character to predict the next character, which explains the poor results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "41f459b7-4774-4093-8adf-aafc3ad0b49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "zdt:\n",
            "3dwQgjtYc-P'Tgxz'Cwfdk-u&$hOHyrZBIBWiOO;l-whmQukjPoJiz;wCuGCKo'KBaxN SkYslyuIBRN mgbzPhOJ&TpV&THiFtMF.K&:LS'NByti-Ou;;OWOqXfwPWHjPBXh.?OoVKvlZl&dC i\n",
            "'zjGAtwBrxTGxJ,Ce'KVm\n",
            "B!JSkxUlZMiM'-h Q':HVR JS;&UorEu'Puew'whq3HKkWjhg3epNfytAPl\n",
            "yGiK Zk?X.lkmHhZdYxL$!pbI$'Jx:v\n",
            ",Pw3Fq\n",
            "Un::ZesgODtMQz$f-gHcwVD3p\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device) # tensor [[0]]\n",
        "print(decode(m.generate(context, max_new_tokens=300)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzrmNDrTOdG4"
      },
      "source": [
        "## Introduction to attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdzUhFQJOdG5"
      },
      "source": [
        "Attention is a **communication mechanism** that can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "\n",
        "In our bigram model, the tokens in each chunk are currently not interacting with each other, so we would like to implement a mechanism to allow them to **communicate**. In particular, we want to ensure that each token only interacts with the tokens before it in the sequence.\n",
        "\n",
        "<div style=\"width: 570px; margin: 0 auto;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/danielsimon4/language-modeling/refs/heads/main/Images/attention-graph.png\">\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "The simplest way to achieve this is by computing the **average** of the preceding tokens. For instance, the fourth token should aggregate its channels with those of the third, second, and first tokens, averaging their values.\n",
        "\n",
        "\n",
        "Consider the following chunk where every row represents a token and the columns represent the channels:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & x_{13} & \\dots & x_{1C} \\\\\n",
        "x_{21} & x_{22} & x_{23} & \\dots & x_{2C} \\\\\n",
        "x_{31} & x_{32} & x_{33} & \\dots & x_{3C} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{T1} & x_{T2} & x_{T3} & \\dots & x_{TC}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Performing this algorithm, the tokens would interact as follows:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & x_{13} & \\dots & x_{1C} \\\\\n",
        "\\frac{x_{11} + x_{21}}{2} & \\frac{x_{12} + x_{22}}{2} & \\frac{x_{13} + x_{23}}{2} & \\dots & \\frac{x_{1C} + x_{2C}}{2} \\\\\n",
        "\\frac{x_{11} + x_{21} + x_{31}}{3} & \\frac{x_{12} + x_{22} + x_{32}}{3} & \\frac{x_{13} + x_{23} + x_{33}}{3} & \\dots & \\frac{x_{1C} + x_{2C} + x_{3C}}{3} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{x_{11} + x_{21} + x_{31} + ... + x_{T1}}{T} & \\frac{x_{12} + x_{22} + x_{32} + ... + x_{T2}}{T} & \\frac{x_{13} + x_{23} + x_{33} + ... + x_{T3}}{T} & \\dots & \\frac{x_{1C} + x_{2C} + x_{3C} + ... + x_{TC}}{T}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "We can implement this averaging process with **two nested for loops**, where the inner loop iterates over each token in the chunk and computes the average of the channels for all the preceding tokens and the outer loop iterates over each chunk in the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Hs_E24uRE8kr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.8385,  0.7409],\n",
            "        [ 0.5635, -0.6097],\n",
            "        [ 1.8720,  0.0590],\n",
            "        [ 0.1869, -0.2114],\n",
            "        [ 1.5499,  0.2369],\n",
            "        [-2.3956, -0.3363],\n",
            "        [ 2.2205, -0.1176],\n",
            "        [ 0.0070, -1.0434]])\n",
            "tensor([[-0.8385,  0.7409],\n",
            "        [-0.1375,  0.0656],\n",
            "        [ 0.5323,  0.0634],\n",
            "        [ 0.4460, -0.0053],\n",
            "        [ 0.6668,  0.0431],\n",
            "        [ 0.1564, -0.0201],\n",
            "        [ 0.4512, -0.0340],\n",
            "        [ 0.3957, -0.1602]])\n"
          ]
        }
      ],
      "source": [
        "B, T, C = 4, 8, 2 # batch, time, channels\n",
        "\n",
        "# random imput tensor\n",
        "x = torch.randn(B,T,C) # (B, T, C)\n",
        "\n",
        "# matrix of all zeros\n",
        "xbow = torch.zeros((B,T,C)) # (B, T, C)\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (T, C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0) # (C)\n",
        "\n",
        "# print only the first chunk of the batch\n",
        "print(x[0])\n",
        "\n",
        "# print how the tokens of that chunk would interact\n",
        "print(xbow[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix multiplication efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ITaaVkuOdG8"
      },
      "source": [
        "We can improve efficiency by replacing the inner for loop with **matrix multiplication** and a **lower triangular matrix** like the one below. In addtion, PyTorch can perform multiple matrix multiplications simultaneously and independently, even when using only a CPU. This allows us to process several chunks more efficiently than with the outer loop.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & x_{13} & \\dots & x_{1C} \\\\\n",
        "x_{21} & x_{22} & x_{23} & \\dots & x_{2C} \\\\\n",
        "x_{31} & x_{32} & x_{33} & \\dots & x_{3C} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{T1} & x_{T2} & x_{T3} & \\dots & x_{TC}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Performing this matrix multiplication, the tokens would interact in a similar way as before:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & x_{13} & \\dots & x_{1C} \\\\\n",
        "\\frac{x_{11} + x_{21}}{2} & \\frac{x_{12} + x_{22}}{2} & \\frac{x_{13} + x_{23}}{2} & \\dots & \\frac{x_{1C} + x_{2C}}{2} \\\\\n",
        "\\frac{x_{11} + x_{21} + x_{31}}{3} & \\frac{x_{12} + x_{22} + x_{32}}{3} & \\frac{x_{13} + x_{23} + x_{33}}{3} & \\dots & \\frac{x_{1C} + x_{2C} + x_{3C}}{3} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{x_{11} + x_{21} + x_{31} + ... + x_{T1}}{T} & \\frac{x_{12} + x_{22} + x_{32} + ... + x_{T2}}{T} & \\frac{x_{13} + x_{23} + x_{33} + ... + x_{T3}}{T} & \\dots & \\frac{x_{1C} + x_{2C} + x_{3C} + ... + x_{TC}}{T}\n",
        "\\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "fbe4968f-1875-4752-830a-ddeca93668d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# lower triangular matrix of all ones\n",
        "wei = torch.tril(torch.ones(T, T)) # (T, T)\n",
        "\n",
        "# lower triangular matrix\n",
        "wei /= wei.sum(1, keepdim=True) # (T, T)\n",
        "\n",
        "# matrix multiplication\n",
        "xbow2 = wei @ x # (B, T, C) = (B, T, T) x (B, T, C)\n",
        "\n",
        "# compare results\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJbu25s0swJg"
      },
      "source": [
        "## Decoder attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leA0XMKrOdG_"
      },
      "source": [
        "In fact, the lower triangular matrix contains the weights for the weighted sum of the past elements. Those **attention weights** control how much influence each past token should have on the current token. We are going to modify the way we construct this lower triangular matrix.\n",
        "\n",
        "Initially, the matrix is going to be completely zeroed out, indicating no attention is being paid yet.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Next, we are going to apply a **masking** to prevent future tokens from interacting with the past. To accomplish this, we can set those positions to −∞, so after applying Softmax their attention weights are zero.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & -∞ & -∞ & -∞ & -∞ \\\\\n",
        "0 & 0 & -∞ & -∞ & -∞ \\\\\n",
        "0 & 0 & 0 & -∞ & -∞ \\\\\n",
        "0 & 0 & 0 & 0 & -∞ \\\\\n",
        "0 & 0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, we are going to normalize the attention weights and convert them into probabilities using **Softmax**, which ensures that the attention weights in each row sum to 1, distributing influence only among the past tokens. Please note we got a lower triangular matrix similar to the one before.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "We just created a **decoder attention block** because we used a triangular masking to ensure that tokens from the future cannot influence tokens from the past. Such a setup is commonly used in autoregressive models, like language modeling, where predictions are made token by token, without access to future context.\n",
        "\n",
        "In contrast, in an **encoder attention block**, there is no masking. All tokens are free to interact with each another, allowing information to flow bidirectionally across the entire sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "c84f38ab-3b4f-46f0-e501-208855752ce6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# apply masking\n",
        "tril = torch.tril(torch.ones(T, T)) # (T, T)\n",
        "wei = torch.zeros((T,T))            # (T, T)\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # (T, T)\n",
        "\n",
        "# apply softmax across each row\n",
        "wei = F.softmax(wei, dim=-1)        # (T, T)\n",
        "\n",
        "# matrix multiplication\n",
        "xbow3 = wei @ x # (B, T, C) = (B, T, T) x (B, T, C)\n",
        "\n",
        "# compare results\n",
        "torch.allclose(xbow, xbow3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M78tmbwFpPoF"
      },
      "source": [
        "## Scaled self-attention\n",
        "But we want the attention weights in each row to vary based on the relationships between the tokens. Different tokens might find certain other tokens more relevant or important, and we need to capture these dynamic relationships. In other words, we want to gather information from previous tokens in a **data-dependent way**.\n",
        "\n",
        "We can achieve this using a **query matrix** (representing \"what am I looking for\"), a **key matrix** (representing \"what information do I have\"), and a **values matrix** for each chunk. To get these matrices, we are just going to perform matrix multiplication between the input and some weights.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\begin{bmatrix}\n",
        "q_{11} & q_{12} & q_{13} & \\dots & q_{1H} \\\\\n",
        "q_{21} & q_{22} & q_{23} & \\dots & q_{2H} \\\\\n",
        "q_{31} & q_{32} & q_{33} & \\dots & q_{3H} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "q_{T1} & q_{T2} & q_{T3} & \\dots & q_{TH}\n",
        "\\end{bmatrix}\n",
        "&=\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & x_{13} & \\dots & x_{1C} \\\\\n",
        "x_{21} & x_{22} & x_{23} & \\dots & x_{2C} \\\\\n",
        "x_{31} & x_{32} & x_{33} & \\dots & x_{3C} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{T1} & x_{T2} & x_{T3} & \\dots & x_{TC}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "qw_{11} & qw_{12} & qw_{13} & \\dots & qw_{1H} \\\\\n",
        "qw_{21} & qw_{22} & qw_{23} & \\dots & qw_{2H} \\\\\n",
        "qw_{31} & qw_{32} & qw_{33} & \\dots & qw_{3H} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "qw_{C1} & qw_{C2} & qw_{C3} & \\dots & qw_{CH}\n",
        "\\end{bmatrix}\n",
        "\n",
        "\\\\\n",
        "\\\\\n",
        "\n",
        "\\begin{bmatrix}\n",
        "k_{11} & k_{12} & k_{13} & \\dots & k_{1H} \\\\\n",
        "k_{21} & k_{22} & k_{23} & \\dots & k_{2H} \\\\\n",
        "k_{31} & k_{32} & k_{33} & \\dots & k_{3H} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "k_{T1} & k_{T2} & k_{T3} & \\dots & k_{TH}\n",
        "\\end{bmatrix}\n",
        "&=\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & x_{13} & \\dots & x_{1C} \\\\\n",
        "x_{21} & x_{22} & x_{23} & \\dots & x_{2C} \\\\\n",
        "x_{31} & x_{32} & x_{33} & \\dots & x_{3C} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{T1} & x_{T2} & x_{T3} & \\dots & x_{TC}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "kw_{11} & kw_{12} & kw_{13} & \\dots & kw_{1H} \\\\\n",
        "kw_{21} & kw_{22} & kw_{23} & \\dots & kw_{2H} \\\\\n",
        "kw_{31} & kw_{32} & kw_{33} & \\dots & kw_{3H} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "kw_{C1} & kw_{C2} & kw_{C3} & \\dots & kw_{CH}\n",
        "\\end{bmatrix}\n",
        "\n",
        "\\\\\n",
        "\\\\\n",
        "\n",
        "\\begin{bmatrix}\n",
        "v_{11} & v_{12} & v_{13} & \\dots & v_{1H} \\\\\n",
        "v_{21} & v_{22} & v_{23} & \\dots & v_{2H} \\\\\n",
        "v_{31} & v_{32} & v_{33} & \\dots & v_{3H} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "v_{T1} & v_{T2} & v_{T3} & \\dots & v_{TH}\n",
        "\\end{bmatrix}\n",
        "&=\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & x_{13} & \\dots & x_{1C} \\\\\n",
        "x_{21} & x_{22} & x_{23} & \\dots & x_{2C} \\\\\n",
        "x_{31} & x_{32} & x_{33} & \\dots & x_{3C} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{T1} & x_{T2} & x_{T3} & \\dots & x_{TC}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "vw_{11} & vw_{12} & vw_{13} & \\dots & vw_{1H} \\\\\n",
        "vw_{21} & vw_{22} & vw_{23} & \\dots & vw_{2H} \\\\\n",
        "vw_{31} & vw_{32} & vw_{33} & \\dots & vw_{3H} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "vw_{C1} & vw_{C2} & vw_{C3} & \\dots & vw_{CH}\n",
        "\\end{bmatrix}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The **affinity** between tokens is computed by performing matrix multiplication between the queries of one token and the transposed keys of all the other tokens. This results in an **affinity matrix**, where the query of each token measures how closely it aligns with the keys of the other tokens.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & \\dots & a_{1T} \\\\\n",
        "a_{21} & a_{22} & a_{23} & \\dots & a_{2T} \\\\\n",
        "a_{31} & a_{32} & a_{33} & \\dots & a_{3T} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & \\dots & a_{TT}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "q_{11} & q_{12} & q_{13} & \\dots & q_{1H} \\\\\n",
        "q_{21} & q_{22} & q_{23} & \\dots & q_{2H} \\\\\n",
        "q_{31} & q_{32} & q_{33} & \\dots & q_{3H} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "q_{T1} & q_{T2} & q_{T3} & \\dots & q_{TH}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "k_{11} & k_{12} & k_{13} & \\dots & k_{1H} \\\\\n",
        "k_{21} & k_{22} & k_{23} & \\dots & k_{2H} \\\\\n",
        "k_{31} & k_{32} & k_{33} & \\dots & k_{3H} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "k_{T1} & k_{T2} & k_{T3} & \\dots & k_{TH}\n",
        "\\end{bmatrix}^T\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Then, we are going to divide all the computed affinities by the square root of the head_size. This action, know as **scaled attention**, is essential because it prevents us from getting extremely sharp distributions that concentrate too much attention in one element when we apply Softmax later.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & \\dots & a_{1T} \\\\\n",
        "a_{21} & a_{22} & a_{23} & \\dots & a_{2T} \\\\\n",
        "a_{31} & a_{32} & a_{33} & \\dots & a_{3T} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & \\dots & a_{TT}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & \\dots & a_{1T} \\\\\n",
        "a_{21} & a_{22} & a_{23} & \\dots & a_{2T} \\\\\n",
        "a_{31} & a_{32} & a_{33} & \\dots & a_{3T} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & \\dots & a_{TT}\n",
        "\\end{bmatrix}\n",
        "·\n",
        "\\frac{1}{\\sqrt{\\text{head\\_size}}}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Next, we are goint to apply a masking, similar to the one from before, to prevent future tokens from interacting with the past by setting those affinities to −∞.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "a_{11} & -∞ & -∞ & -∞ & -∞ \\\\\n",
        "a_{21} & a_{22} & -∞ & -∞ & -∞ \\\\\n",
        "a_{31} & a_{32} & a_{33} & -∞ & -∞ \\\\\n",
        "a_{41} & a_{42} & a_{43} & a_{44} & -∞ \\\\\n",
        "a_{51} & a_{52} & a_{53} & a_{54} & a_{55}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, we are going to apply softmax to convert these affinities into attention weights, which determine how much attention each token pays to others.\n",
        "\n",
        "\\begin{bmatrix}\n",
        "w_{11} & 0 & 0 & 0 & 0 \\\\\n",
        "w_{21} & w_{22} & 0 & 0 & 0 \\\\\n",
        "w_{31} & w_{32} & w_{33} & 0 & 0 \\\\\n",
        "w_{41} & w_{42} & w_{43} & w_{44} & 0 \\\\\n",
        "w_{51} & w_{52} & w_{53} & w_{54} & w_{55}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Once we have our lower triangular matrix containing the attention weights, we are just going to perform matrix multiplication between this matrix and the values matrix.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "w_{11} & 0 & 0 & 0 & 0 \\\\\n",
        "w_{21} & w_{22} & 0 & 0 & 0 \\\\\n",
        "w_{31} & w_{32} & w_{33} & 0 & 0 \\\\\n",
        "w_{41} & w_{42} & w_{43} & w_{44} & 0 \\\\\n",
        "w_{51} & w_{52} & w_{53} & w_{54} & w_{55}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "v_{11} & v_{12} & v_{13} & \\dots & v_{1H} \\\\\n",
        "v_{21} & v_{22} & v_{23} & \\dots & v_{2H} \\\\\n",
        "v_{31} & v_{32} & v_{33} & \\dots & v_{3H} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "v_{T1} & v_{T2} & v_{T3} & \\dots & v_{TH}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Performing this matrix multiplication, the tokens would interact as follows:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "w_{11}v_{11} & w_{11}v_{12} & w_{11}v_{13} & \\dots & w_{11}v_{1H} \\\\\n",
        "w_{21}v_{11} + w_{22}v_{21} & w_{21}v_{12} + w_{22}v_{22} & w_{21}v_{13} + w_{22}v_{23} & \\dots & w_{21}v_{1H} + w_{22}v_{2H} \\\\\n",
        "w_{31}v_{11} + w_{32}v_{21} + w_{33}v_{31}  & w_{31}v_{12} + w_{32}v_{22} + w_{33}v_{32} & w_{31}v_{13} + w_{32}v_{23} + w_{33}v_{33} & \\dots & w_{31}v_{1H} + w_{32}v_{2H} + w_{33}v_{3H} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "w_{T1}v_{11} + w_{T2}v_{21} + w_{T3}v_{31} + ... + w_{TT}v_{T1} & w_{T1}v_{12} + w_{T2}v_{22} + w_{T3}v_{32} + ... + w_{TT}v_{T2} & w_{T1}v_{13} + w_{T2}v_{23} + w_{T3}v_{33} + ... + w_{TT}v_{T3} & \\dots & w_{T1}v_{1H} + w_{T2}v_{2H} + w_{T3}v_{3H} + ... + w_{TT}v_{TH}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "As we can see, we have just implemented the attention mechanism, which is essentially a **weighted sum with data-dependent weights**.\n",
        "\n",
        "<br>\n",
        "\n",
        "We just created a **self-attention block** because the queries, keys, and values are all originated from the same input. In contrast, in a **cross-attention block**, the keys and values come from a source different from the one of the queries (such as an encoder module in a transformer model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EDarxEWIRMKq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5369, 0.4631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2816, 0.5207, 0.1977, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3511, 0.2647, 0.1072, 0.2770, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1816, 0.1325, 0.2561, 0.2885, 0.1413, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1181, 0.1913, 0.1732, 0.1039, 0.1065, 0.3070, 0.0000, 0.0000],\n",
              "        [0.2005, 0.0781, 0.1450, 0.2032, 0.1033, 0.1038, 0.1661, 0.0000],\n",
              "        [0.1348, 0.1106, 0.1388, 0.2255, 0.0614, 0.0745, 0.0976, 0.1568]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "B, T, C = 4, 8, 32\n",
        "head_size = C\n",
        "\n",
        "# random imput tensor\n",
        "x = torch.randn(B,T,C) # (B, T, C)\n",
        "\n",
        "# define three linear layers\n",
        "query = nn.Linear(head_size, head_size, bias=False)\n",
        "key = nn.Linear(head_size, head_size, bias=False)\n",
        "value = nn.Linear(head_size, head_size, bias=False)\n",
        "\n",
        "# compute queries, keys, and values\n",
        "q = query(x) # (B, T, hs)\n",
        "k = key(x)   # (B, T, hs)\n",
        "v = value(x) # (B, T, hs)\n",
        "\n",
        "# compute affinities and scale them\n",
        "wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T) = (B, T, hs) @ (B, hs, T)\n",
        "\n",
        "# apply masking\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) # (B, T, T)\n",
        "\n",
        "# apply softmax across each row\n",
        "wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "# matrix multiplication\n",
        "xbow4 = wei @ v  # (B, T, hs) = (B, T, T) @ (B, T, hs)\n",
        "\n",
        "# print the lower triangular matrix for the first chunk\n",
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y9y1AofyMCN"
      },
      "source": [
        "## Head module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReKygJAkqNkm"
      },
      "source": [
        "We just created a **single head of self-attention** which allows the model to focus on different parts of the chunk simultaneously, extracting information about the **relationships between the tokens**. \n",
        "\n",
        "Now, we are just going to incorporate the previous code into the `Head` module.\n",
        "\n",
        "<div style=\"width: 220px; margin: 0 auto;\">\n",
        "    <img src=\"https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rfShVmhtyLwW"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" One head of self-attention. \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # define three linear layers\n",
        "        self.query = nn.Linear(head_size, head_size, bias=False)\n",
        "        self.key = nn.Linear(head_size, head_size, bias=False)\n",
        "        self.value = nn.Linear(head_size, head_size, bias=False)\n",
        "        \n",
        "        # trill is not a parameter of the module so it has to be registered as a\n",
        "        # buffer according to PyTorch naming conventions\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        _, T, _ = x.shape # (B, T, n_embd)\n",
        "\n",
        "        # compute queries, keys, and values\n",
        "        q = query(x) # (B, T, hs)\n",
        "        k = key(x)   # (B, T, hs)\n",
        "        v = value(x) # (B, T, hs)\n",
        "\n",
        "        # compute attention weights\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T) = (B, T, hs) @ (B, hs, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "        # matrix multiplication\n",
        "        out = wei @ v     # (B, T, hs) = (B, T, T) @ (B, T, hs)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In language models, tokenization is usually the first step, which converts input text into a sequence of token indices. After tokenization, an **embedding layer** typically maps these sequences of token indices into dense vector representations, knowns as **embeddings**, which can be used as input to a neural network.\n",
        "\n",
        "Right now, the bigram model is using an embedding table of size (vocab_size, vocab_size) so that each token reads off the logits for the next token. We are going to modify this embedding table to return **token embeddings** instead of logits. The new **token embedding table** of size (vocab_size, n_embd) is going to encode the tokens based on their **identity**, providing a more meaningful representation of each token in a continuous vector space.\n",
        "\n",
        "<div style=\"width: 650px; margin: 0 auto;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/0*cgpKoFocSYm6bLHw.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Positional embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdlHNtsXeIBZ"
      },
      "source": [
        "In attention mechanisms, there is **no inherent sense of position** among the input tokens, they simply act over a set of vectors. This is why we need to introduce **positional embeddings** to take into account the order of the tokens.\n",
        "\n",
        "The **position embedding table** of size (block_size, n_embd) is going to encode the tokens based on their **position**, providing information of the token's place within the sequence to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single head of self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to modify the bigram model to implement a single head of self-attention.\n",
        "\n",
        "After obtaining the token and positional embeddings from the input to the model, we combine them by **adding the two embeddings**. This combined representation of the identity and position of the tokens is going to be the input to the single head of self-attention.\n",
        "\n",
        "To generate the logits for predicting the next token, we are going to apply a linear layer, often referred to as the **language modeling head**, to the output of the single head of self-attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RE7OGWzhdVQ5"
      },
      "outputs": [],
      "source": [
        "n_embd = 32 # number of embedding dimensions\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        _, T = idx.shape\n",
        "\n",
        "        # token and positional embeddings\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # single head of self-attention\n",
        "        x = self.sa_head(x)   # (B, T, n_embd)\n",
        "\n",
        "        # language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = calculate_loss(logits, targets)\n",
        "            \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "\n",
        "        return generate_tokens(self, idx, max_new_tokens, block_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOIcG7uC_zJs"
      },
      "source": [
        "Using a single head of self-attention, the validation loss went down from 2.45 to 2.35."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgPgjVli8ljO"
      },
      "source": [
        "## Multi-head self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEYNVOv38lCA"
      },
      "source": [
        "Multi-head attention involves applying **multiple attention mechanisms in parallel** and then concatenating their results. \n",
        "\n",
        "This approach allows the model to capture different types of relationships between the tokens across **multiple communication channels**. Using several heads with smaller dimensions enables the model to learn diverse patterns more effectively than relying on a single head with a larger dimension.\n",
        "\n",
        "\n",
        "<div style=\"width: 450px; margin: 0 auto;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1010/0*0KPEV8QidHkteKeY.png\">\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:**\n",
        "\n",
        "[**nn.ModuleList**](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html) is a container in PyTorch used to hold a list of submodules, where each module can be indexed using an integer. It is useful when you have a variable number of modules that need to be stored and managed together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jR4KZkrA9G5A"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multiple heads of self-attention in parallel. \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to modify the GPT model to implement multi-head self-attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ZW__QLBRx2sf"
      },
      "outputs": [],
      "source": [
        "n_head = 4  # number of heads\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(n_embd, n_head)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        _, T = idx.shape\n",
        "\n",
        "        # token and positional embeddings\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # multi-head self-attention\n",
        "        x = self.sa_heads(x)  # (B, T, n_embd)\n",
        "\n",
        "        # language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = calculate_loss(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "\n",
        "        return generate_tokens(self, idx, max_new_tokens, block_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l4Q8PiE_I4w"
      },
      "source": [
        "Using multi-head self-attention, the validation loss went down from 2.35 to 2.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqHFnbrmGdtP"
      },
      "source": [
        "## Feedforward neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8iesh6fIc94"
      },
      "source": [
        "The **multi-headed self-attention** mechanism allows each token in a sequence to **attend to** (look at) every other token. It computes the relationships or dependencies between the tokens by assigning different attention scores. However, at this stage, the tokens **are not processing** the information they have gathered. They are simply identifying which tokens are relevant to them.\n",
        "\n",
        "Once the tokens have collected this information through self-attention, the output of each token is passed through a **feedforward neural network (FFN)**. The FFN processes the information independently for each token, allowing them to **analyze** the data they have previously gathered. This is where the model transforms the attended information into something meaningful for downstream tasks.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:**\n",
        "\n",
        "[**nn.Sequential**](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is a container in PyTorch that provides an easy way to build neural network architectures by stacking layers or operations in sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "m_ufHOCAGlto"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" A simple linear layer followed by a non-linearity. \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to modify the GPT model to add a FFN after the multi-head self-attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "13gClUAEGl_v"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(n_embd, n_head)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        _, T = idx.shape\n",
        "\n",
        "        # token and positional embeddings\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # multi-head self-attention\n",
        "        x = self.sa_heads(x)  # (B, T, n_embd)\n",
        "\n",
        "        # feedforward network\n",
        "        x = self.ffwd(x)      # (B, T, n_embd)\n",
        "\n",
        "        # language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = calculate_loss(logits, targets)\n",
        "        \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "\n",
        "        return generate_tokens(self, idx, max_new_tokens, block_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9DHYOj0Jtwz"
      },
      "source": [
        "Using a FFN after the multi-head self-attention, the validation loss went down from 2.2 to 2.16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsSPYUfsLVff"
      },
      "source": [
        "## Block module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z6EagnULblp"
      },
      "source": [
        "We are going to incorporate both the multi-headed self-attention mechanism and the feedforward network in the `Block` module.\n",
        "\n",
        "A block performs **communication** (handled by the multi-headed self-attention, where the tokens attento to each other) and **computation** (handled by the feedforward network, where the tokens individually analyze the gathered data).\n",
        "\n",
        "<div style=\"width: 180px; margin: 0 auto;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/danielsimon4/language-modeling/refs/heads/main/Images/block-1.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oxTpGBo1QAp8"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation. \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.sa_heads = MultiHeadAttention(n_embd, n_head)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # communication\n",
        "        x = self.sa_heads(x)\n",
        "\n",
        "        # computation\n",
        "        x = self.ffwd(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to modify the GPT model so it implements several blocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "X7yZQREAQAcP"
      },
      "outputs": [],
      "source": [
        "n_layer = 3 # number of blocks\n",
        "\n",
        "class GPTLModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.ModuleList([Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        _, T = idx.shape\n",
        "\n",
        "        # token and positional embeddings\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        x = self.blocks(x)    # (B, T, n_embd)\n",
        "        \n",
        "        # language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = calculate_loss(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        \n",
        "        return generate_tokens(self, idx, max_new_tokens, block_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icIeiVxsamZP"
      },
      "source": [
        "Using 3 blocks, the validation loss went down from 2.16 to 2.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-yNpAZIRpJ7"
      },
      "source": [
        "## Residual connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJMiPo9vRp4S"
      },
      "source": [
        "We are starting to get a deep neural net that suffers from optimization issues. To address these, **residual connections**, introduced in the paper [*Deep Residual Learning for Image Recognition*](https://arxiv.org/abs/1512.03385), can be highly effective. The core idea is that while the data undergoes transformations, a **skip connection** adds the original features back to the transformed data via element-wise addition (Image 1).\n",
        "\n",
        "Another way to conceptualize residual connections is through a **residual pathway**. Along this pathway, **residual blocks** branch off, perform computations, and then merged back with the original pathway via addition (Image 2). As the network is trained, these blocks gradually begin to contribute to the final result. The key advantage of residual connections is that, during the early stages of optimization, the **gradient flows smoothly** from the output layer back to the input, avoiding the vanishing gradient problem.\n",
        "\n",
        "Before merging the transformed data back into the residual pathway, a **projection layer** is typically added. This layer applies a simple linear transformation (*y = x · w + b*) to the output of the transformation to ensure the dimensions match.\n",
        "\n",
        "\n",
        "<div style=\"width: 650px; margin: 0 auto;\">\n",
        "    <img src=\"https://pbs.twimg.com/media/ESnE4IvUYAAopRf.jpg\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to add a projection layer in the `MultiHeadAttention` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Dgp8IXTndKof"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multiple heads of self-attention in parallel. \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
        "        self.proj = nn.Linear(head_size * n_head, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # multi-head self-attention\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # projection layer\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "COMPLETE!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "MKRCA0rkeqyW"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" A simple linear layer followed by a non-linearity. \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # growing inner-layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rDXNPQFqNky"
      },
      "source": [
        "We are going to incorporate residual connections (x = x + F(x)) into the transformations within the `Block` module to ensure smoother gradient flow and improve optimization.\n",
        "\n",
        "<div style=\"width: 180px; margin: 0 auto;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/danielsimon4/language-modeling/refs/heads/main/Images/block-2.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iRaEJP3BcyPW"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation. \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sa_heads = MultiHeadAttention(n_embd, n_head)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # communication with residual connections\n",
        "        x = x + self.sa_heads(x)\n",
        "        \n",
        "        # computation with residual connections\n",
        "        x = x + self.ffwd(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWFQJqpfh341"
      },
      "source": [
        "Using residual connections, the validation loss went down from 2.1 to 1.94."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-OSSHcbiJ_2"
      },
      "source": [
        "## Layer normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPb69gYmiL0P"
      },
      "source": [
        "Layer normalization also helps with the optimization of deep neural networks and is described in the paper [Ba et al. (2016). *Layer Normalization*](https://arxiv.org/abs/1607.06450). Remember that batch normalization made sure that across the batch dimension any individual neuron had a unit Gaussian distribution (0 mean and 1 standard deviation output) at initialization.\n",
        "\n",
        "Layer normalization is identical to batch bormalization but normalizes across the rows instead of the columns, does not need the running mean and the running variance buffers, and there is no distinction beteween train and test time. The layer normalization acts on a per token level and **normalizes the features** making them unit Gaussian at initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4zQVO-obnzb_"
      },
      "outputs": [],
      "source": [
        "class LayerNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    xmean = x.mean(1, keepdim=True)\n",
        "    xvar = x.var(1, keepdim=True)\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhbWHI5eqqBy"
      },
      "source": [
        "In the [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762) paper, **layer normalization** is applied after the application of attention and feed forward. Nowadays, it is more common to apply the layer normalization **before the tranformations** (this is called **pre-norm formulation**).\n",
        "\n",
        "In addition, layer normalization occurs within the residual pathway. However, it is preferable to maintain a single clean residual stream all the way down from supervison to the inputs, which facilitates smoother gradient flow during backpropagation. This ensures that even the shallow layers receive direct supervision, preventing the network from suffering from vanishing gradients.\n",
        "\n",
        "<div style=\"width: 160px; margin: 0 auto;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/danielsimon4/language-modeling/refs/heads/main/Images/block-architecture.png\">\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "We are going to apply layer normalization before the transformations and outside of the residual pathway.\n",
        "\n",
        "<div style=\"width: 180px; margin: 0 auto;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/danielsimon4/language-modeling/refs/heads/main/Images/block-3.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "z9phYV-PqRE9"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # communication with residual connections and layer norm\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "\n",
        "        # computation with residual connections and layer norm\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTNBKvL12IjL"
      },
      "source": [
        "It is also common to add a layer normalization **at the end of the transformer** and right before the final linear layer that decodes into vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YyTBVcfOuWZp"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        _, T = idx.shape\n",
        "\n",
        "        # token and position embeddings from embedding tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # apply blocks\n",
        "        x = self.blocks(x)    # (B, T, n_embd)\n",
        "\n",
        "        # final layer norm\n",
        "        x = self.ln_f(x)      # (B, T, n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = calculate_loss(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGcxackZsr6l"
      },
      "source": [
        "Using layer normalization the validation loss went down from 1.94 to 1.93."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixb71Sr0vIqC"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvi7012BxtFT"
      },
      "source": [
        "Dropout is a **regularization technique** described in the paper [Srivastava et al. (2014). *Dropout: A Simple Way to Prevent Neural Networks from Overfitting*](https://jmlr.org/papers/v15/srivastava14a.html) that consists on, in every step, **randomly shut off** some subset of neurons and train without them.\n",
        "\n",
        "<div style=\"width: 550px\">\n",
        "    <img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_6.19.24_PM.png\">\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "Dropout is added when we calculate the affinities after Softmax so we randomly prevent some of the nodes from communicating:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "llJGGH7gy6yk"
      },
      "outputs": [],
      "source": [
        "dropout = 0.2\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        _, T, _ = x.shape # (B, T, n_embd)\n",
        "\n",
        "        # compute keys and queries\n",
        "        k = self.key(x)   # (B, T, hs)\n",
        "        q = self.query(x) # (B, T, hs)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T) = (B, T, hs) @ (B, hs, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B, T, hs)\n",
        "        out = wei @ v     # (B, T, hs) = (B, T, T) @ (B, T, hs)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHYuGJrV0e8Q"
      },
      "source": [
        "Dropout is also added right after the projection back to the residual pathway:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mqU7JkU6yX6V"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # multi-headed self-attention\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # projection and dropout layers\n",
        "        out = self.dropout(self.proj(out))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "N6Jlvr6mx-XR"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # growing inner-layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZcyHhwT29o4"
      },
      "source": [
        "## Scaling up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "oBlY-vM93A-L"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 64       # number of chunks per bacth\n",
        "block_size = 256      # chunks maximum context length\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200      # how many iterations used to calculate the loss\n",
        "eval_interval = 1000   # every how many iterations calculate the loss\n",
        "learning_rate = 3e-4\n",
        "max_iters = 5000\n",
        "n_embd = 384          # number of embedding dimensions\n",
        "n_layer = 6           # number of blocks\n",
        "n_head = 6            # number of heads\n",
        "dropout = 0.2         # dropout percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlsZ4Shz_-B8",
        "outputId": "b158bbbe-90cd-4170-89fc-1694b15ccc89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.788929 M parameters\n"
          ]
        }
      ],
      "source": [
        "# intialize the model\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "3zf7rmyHABR5"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gT2So2XAIX7",
        "outputId": "2d557568-ba84-4b0c-81f9-59aef07b34bf"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[45], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m----> 6\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_iters\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[18], line 15\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m     14\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m---> 15\u001b[0m     _, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     18\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[38], line 22\u001b[0m, in \u001b[0;36mGPTLanguageModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B, T, n_embd)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# apply blocks\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# (B, T, n_embd)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# final layer norm\u001b[39;00m\n\u001b[0;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)      \u001b[38;5;66;03m# (B, T, n_embd)\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[37], line 19\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# computation with residual connections and layer norm\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[41], line 15\u001b[0m, in \u001b[0;36mFeedFoward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step: {iter:4d}/{max_iters:4d}   Train loss: {losses['train']:.4f}   Val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0LnkzyqCn48",
        "outputId": "dc9bd062-4d36-4b22-bcca-2aea20c2aea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "LEONTES:\n",
            "Whither, this? Hasting foe!\n",
            "This chair that banish'd his battle spirits,\n",
            "And diso mers it tellingn their devils.\n",
            "Good not! I know myself-sper, for where he they\n",
            "Command. Besides, quoth deids us the hasty ture.\n",
            "Procks shall be salinting, wanting by their death\n",
            "The whitness, wite-arwarting up with rashing feast\n",
            "With record the heed memorsel of our souls, make with chape,\n",
            "Most of the watchest hath moved, we must.\n",
            "\n",
            "Nurse:\n",
            "Provost, you speak; again, love.\n",
            "Ah, fellow, thus farewell medder-tirrion!\n",
            "And now I know follows me for my heads mark'd thee!\n",
            "Mach's spoon-bear! come on their bosoms\n",
            "And pale and pebble their own posing treats,\n",
            "Or wealth em or a scepsed, friend and bight\n",
            "Most time have done an heart,\n",
            "Forthwell amplot till way upon his soul talls,\n",
            "False their beadins his womble.\n",
            "\n",
            "SLY:\n",
            "Though he must have found you all fitter up,\n",
            "Shall we have abser'd the state of a fiend.\n",
            "But ke drowns, I with tumbly manaters\n",
            "Forbade, as deceit for this dust of rear,\n",
            "For Juliet's courtesy, for h\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
