{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "# Generatively Pretrained Transformer (GPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LueVRTGOdGD"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjrYsZyeqNkC",
        "outputId": "aada9961-aa03-4f8a-d6b0-5fff2b1c864a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-07-30 13:45:04--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-07-30 13:45:04 (27.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6medjfRsLD9",
        "outputId": "cd79a565-7ad1-4fe6-c031-0650e8e13bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of dataset: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# load dataset\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(f'Length of dataset: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID6u03TkOdGJ"
      },
      "source": [
        "The first 400 characters are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "c0b60970-085d-4f97-bb94-ba80aff17115"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it \n"
          ]
        }
      ],
      "source": [
        "print(text[:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msGLLvVyOdGM"
      },
      "source": [
        "## Vocabulary size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_xVppKzOdGN"
      },
      "source": [
        "Vocabulary size is the **number of unique characters**.\n",
        "\n",
        "**Note:**\n",
        "- chars[0] is the new line character, '\\n'.\n",
        "- chars[1] is the space character, ' '."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "724bc3d1-5322-435b-c165-74b52740a164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocabulary size: 65\n"
          ]
        }
      ],
      "source": [
        "# unique characters and vocabulary size\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print('Unique characters:', ''.join(chars))\n",
        "print('Vocabulary size:', vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovlcDjjLOdGP"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuFBmxWMOdGQ"
      },
      "source": [
        "Tokenize means **convert the raw text** as a string **to some sequence of integers** according to some vocabulary of possible elements.\n",
        "\n",
        "We are building a character level language model so our tokenizer is going to simply translate individual characters into integers using a **lookup table**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "6fa8f6e4-1d1a-4f65-e36d-cdce1d16337b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder output: [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "Decoder output: hii there\n"
          ]
        }
      ],
      "source": [
        "# mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# encoder: take a string, output a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "# decoder: take a list of integers, output a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print('Encoder output:', encode('hii there'))\n",
        "print('Decoder output:', decode([46, 47, 47, 1, 58, 46, 43, 56, 43]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1rOlGPyOdGT"
      },
      "source": [
        "Google uses [SentencePiece](https://github.com/google/sentencepiece?tab=readme-ov-file) tokenizer to also encode text into integers but in a different schema and using a different vocabulary. SentencePiece does not encode indivual characters or entire words but **sub-words units**. ChatGPT uses the OpenAI's [tiktoken](https://github.com/openai/tiktoken) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVGbu3PBqNkI",
        "outputId": "2f3cc769-e506-433e-d40e-68bee358bfe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "# pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKmR8r96OdGT",
        "outputId": "e09b2ed6-360a-4189-c095-4d22ad5efd64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 50257\n",
            "Encoder output: [71, 4178, 612]\n",
            "Decoder output: hii there\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "print('Vocabulary size:', enc.n_vocab)\n",
        "\n",
        "print('Encoder output:', enc.encode('hii there'))\n",
        "print('Decoder output:', enc.decode([71, 4178, 612]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9jYxS-hOdGU"
      },
      "source": [
        "## Build dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbNGNQqFOdGV"
      },
      "source": [
        "We can now encode the entire text dataset and store it into a PyTorch tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "4f773a79-3d13-41b5-839a-885901f5b130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(data[:400]) # the earlier 400 characters will look like this to the GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1ywTbw6OdGW"
      },
      "source": [
        "Split up the data into train and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n] # 90%\n",
        "val_data = data[n:]   # 10%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMIU-QLXOdGX"
      },
      "source": [
        "## Time dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASlKAAWBOdGY"
      },
      "source": [
        "We cannot feed the entire training set into a transformer all at once because that would be computationally very expensive. Thus, when training the transformer we sample random little **chunks** out of the training set and train just a chunk at a time. The maximum length of those chunks is the **block size**.\n",
        "\n",
        "\n",
        "In a chunk of nine characters there's actually eight individual examples packed into it (see below). That's because all of these characters follow each other so when we plug the chunk into a transformer, it simultaneously trains to make a prediction for every example.\n",
        "\n",
        "\n",
        "We train on all the eight examples not just for efficiency but to make the transformer network used to seeing **contexts of size 1 up to block size**. That is useful later during inference because we can start the sampling generation with just 1 character. Then, the transformer knows how to predict the next characters all the way up to block size. After block size, we start truncating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "c03a5f3b-8a30-48e4-aa35-95097d647ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk: [18, 47, 56, 57, 58, 1, 15, 47, 58]\n",
            "\n",
            "Examples:\n",
            "1) When context is [18] the target is 47\n",
            "2) When context is [18, 47] the target is 56\n",
            "3) When context is [18, 47, 56] the target is 57\n",
            "4) When context is [18, 47, 56, 57] the target is 58\n",
            "5) When context is [18, 47, 56, 57, 58] the target is 1\n",
            "6) When context is [18, 47, 56, 57, 58, 1] the target is 15\n",
            "7) When context is [18, 47, 56, 57, 58, 1, 15] the target is 47\n",
            "8) When context is [18, 47, 56, 57, 58, 1, 15, 47] the target is 58\n"
          ]
        }
      ],
      "source": [
        "block_size = 8\n",
        "\n",
        "print('Chunk:', train_data[:block_size+1].tolist())\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "print('\\nExamples:')\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f'{t+1}) When context is {context.tolist()} the target is {target}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwD3jGngSIpz"
      },
      "source": [
        "## CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgNo5yyhSLwl"
      },
      "source": [
        "CUDA exploits the advantages of GPUs over CPUs by utilizing the **parallelism** offered by GPUs multiple cores, which allow to launch a high number of simultaneous threads.\n",
        "\n",
        "We are going to add the ability to run on a GPU if it is available so everything is faster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM4M9ykhSIXH",
        "outputId": "849b11e9-53a9-477f-f560-bcffe811463c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print('Device:', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7UvOK7DOdGa"
      },
      "source": [
        "## Batch dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibh1l9SqOdGa"
      },
      "source": [
        "Because GPUs are very good at **parallel processing** of data, we have **batches of multiple chunks** stacked up in a single tensor feeding into a transformer. Thus, multiple chunks can be processed at the same time but completely independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "70d9901f-a04b-46a0-a1d4-7de6c9e98eb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs shape: (4, 8)\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')\n",
            "\n",
            "Targets shape: (4, 8)\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "batch_size = 4 # number of chunks per bacth\n",
        "block_size = 8 # chunks maximum context length\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "print('Inputs shape:', tuple(xb.shape))\n",
        "print(xb)\n",
        "print('\\nTargets shape:', tuple(yb.shape))\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpxxU1kcOdGc"
      },
      "source": [
        "# Bigram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv_h-eRYOdGd"
      },
      "source": [
        "Although a Bigram model is a simple model, it will be a good start for the architecture of the GPT.\n",
        "\n",
        "**Note:**\n",
        "- B = Batch (batch_size)\n",
        "- T = Time (block_size)\n",
        "- C = Channels (vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nql_1ER53oCf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B, T) tensor of integers\n",
        "\n",
        "        # each token reads off the logits for the next token from a lookup table\n",
        "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fB8BJyYCOdGf"
      },
      "outputs": [],
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOt7NSCEZxze"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4qb4eC8Z0SF"
      },
      "source": [
        "Instead of printing the bacth loss, the **estimate_loss()** function averages up the loss over multiple batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tIpe9xadZr2k"
      },
      "outputs": [],
      "source": [
        "eval_iters = 200      # how many iterations used to calculate the loss\n",
        "eval_interval = 10000 # every how many iterations calculate the loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() # put the model in evaluation mode\n",
        "\n",
        "    # calculate train loss and evaulation loss\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "    model.train() # put the model back in train mode\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN4Zsn3sOdGf"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "4341ef57-07cf-4efb-81e8-8c7155ac87ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:     0 / 40000   Train loss: 4.6355   Val loss: 4.6491\n",
            "Step: 10000 / 40000   Train loss: 2.4726   Val loss: 2.4923\n",
            "Step: 20000 / 40000   Train loss: 2.4551   Val loss: 2.4864\n",
            "Step: 30000 / 40000   Train loss: 2.4519   Val loss: 2.4940\n",
            "Step: 39999 / 40000   Train loss: 2.4518   Val loss: 2.4949\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "max_iters = 40000\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step:{iter:6d} /{max_iters:6d}   Train loss: {losses['train']:.4f}   Val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW-2Yfl5OdGj"
      },
      "source": [
        "## Generate from the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbXzA3uYOdGk"
      },
      "source": [
        "\n",
        "We start the generation with the tensor [[0]]. That is fine because 0 is the new line character, '\\n'. The generate method will generate more characters up to `max_new_tokens`.\n",
        "\n",
        "**Note:** We are feeding the entire growing context (whatever is generated) into the model. However, because it is a bigram model we are only using the last previous character to predict the next charater."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "41f459b7-4774-4093-8adf-aafc3ad0b49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "CExthy brid owindakis s, bth\n",
            "\n",
            "Hiset bube t e.\n",
            "S:\n",
            "O:\n",
            "IS:\n",
            "Falatanss:\n",
            "Wanthar u qur, vet?\n",
            "F dilasoate awice my.\n",
            "\n",
            "Hastarom oroup\n",
            "Yowhthetof isth ble mil ndill, ath iree sengmin lat Heridrovets, and Win nghirileranousel lind me l.\n",
            "MAshe ce hiry:\n",
            "Supr aisspllw y.\n",
            "Herindu n Boopetelaves\n",
            "MP:\n",
            "\n",
            "Pl, d mothak\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=300)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzrmNDrTOdG4"
      },
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2Eu7L7tjMmD"
      },
      "source": [
        "Attention is a **communication mechanism** that can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "\n",
        "<div style=\"width: 350px\">\n",
        "    <img src=\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/SCC1.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ypogaP0s4CX"
      },
      "source": [
        "## Intro to attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdzUhFQJOdG5"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "The tokens in each chunk are currently \"not talking to each other\" so we would like to **\"couple them\"**. In particular, we want to couple them so that the tokens **only communicate with the tokens before them** in the sequence.\n",
        "\n",
        "The easiest way to achieve that is computing the **average** of the preceding tokens. Thus, the fourth token should take its channels and also the channels from the third step, the second step, and the first step, and average those up.\n",
        "\n",
        "<br>\n",
        "\n",
        "Consider the following chunk where every row would be a token and the columns the channels:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "a_{21} & a_{22} & a_{23} & ... & a_{2C} \\\\\n",
        "a_{31} & a_{32} & a_{33} & ... & a_{3C} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & ... & a_{TC}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The tokens would communicate as follows:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "\\frac{a_{11} + a_{21}}{2} & \\frac{a_{12} + a_{22}}{2} & \\frac{a_{13} + a_{23}}{2} & ... & \\frac{a_{1C} + a_{2C}}{2} \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31}}{3} & \\frac{a_{12} + a_{22} + a_{32}}{3} & \\frac{a_{13} + a_{23} + a_{33}}{3} & ... & \\frac{a_{1C} + a_{2C} + a_{3C}}{3} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31} + ... + a_{T1}}{T} & \\frac{a_{12} + a_{22} + a_{32} + ... + a_{T2}}{T} & \\frac{a_{13} + a_{23} + a_{33} + ... + a_{T3}}{T} & ... & \\frac{a_{1C} + a_{2C} + a_{3C} + ... + a_{TC}}{T}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "We can implement that with two nested for loops:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Hs_E24uRE8kr"
      },
      "outputs": [],
      "source": [
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "xbow = torch.zeros((B,T,C))\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0) # (C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLcgROsEOdG6"
      },
      "source": [
        "Looking only at the first chunk of the batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAiqA5ASOdG7",
        "outputId": "430bdfce-b3fe-4334-89c5-9b9c8ee060ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1518,  0.0195],\n",
            "        [ 0.0091, -0.1151],\n",
            "        [ 0.0770,  0.4633],\n",
            "        [ 0.1647,  0.5700],\n",
            "        [-0.8842, -0.0263],\n",
            "        [-0.0381, -2.2398],\n",
            "        [-0.6152, -0.4561],\n",
            "        [ 0.1873,  0.6703]])\n",
            "tensor([[ 0.1518,  0.0195],\n",
            "        [ 0.0805, -0.0478],\n",
            "        [ 0.0793,  0.1226],\n",
            "        [ 0.1007,  0.2344],\n",
            "        [-0.0963,  0.1823],\n",
            "        [-0.0866, -0.2214],\n",
            "        [-0.1621, -0.2549],\n",
            "        [-0.1184, -0.1393]])\n"
          ]
        }
      ],
      "source": [
        "print(x[0])\n",
        "print(xbow[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dot-product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ITaaVkuOdG8"
      },
      "source": [
        "We can be more efficient using matrix multiplication and a lower triangular matrix like the one below:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "\\frac{a_{11} + a_{21}}{2} & \\frac{a_{12} + a_{22}}{2} & \\frac{a_{13} + a_{23}}{2} & ... & \\frac{a_{1C} + a_{2C}}{2} \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31}}{3} & \\frac{a_{12} + a_{22} + a_{32}}{3} & \\frac{a_{13} + a_{23} + a_{33}}{3} & ... & \\frac{a_{1C} + a_{2C} + a_{3C}}{3} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31} + ... + a_{T1}}{T} & \\frac{a_{12} + a_{22} + a_{32} + ... + a_{T2}}{T} & \\frac{a_{13} + a_{23} + a_{33} + ... + a_{T3}}{T} & ... & \\frac{a_{1C} + a_{2C} + a_{3C} + ... + a_{TC}}{T}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "a_{21} & a_{22} & a_{23} & ... & a_{2C} \\\\\n",
        "a_{31} & a_{32} & a_{33} & ... & a_{3C} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & ... & a_{TC}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:** PyTorch will apply the matrix multiplication in all the batch elements (chunks) at the same time and independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "fbe4968f-1875-4752-830a-ddeca93668d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei /= wei.sum(1, keepdim=True) # (T, T)\n",
        "xbow2 = wei @ x # (B, T, C) = (B, T, T) x (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJbu25s0swJg"
      },
      "source": [
        "## Decoder attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leA0XMKrOdG_"
      },
      "source": [
        "In fact, the lower triangular matrix contains the weights for the weighted sum of the past elements. Those weights are **the interaction strength or affinity**, how much of each token from the past we want to aggregate and average up. We are going to modify the way we construct the lower triangular matrix. It will start being:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Then, we will stop the communication with the future tokens by initializing those affinities to -inf:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & -inf & -inf & -inf & -inf \\\\\n",
        "0 & 0 & -inf & -inf & -inf \\\\\n",
        "0 & 0 & 0 & -inf & -inf \\\\\n",
        "0 & 0 & 0 & 0 & -inf \\\\\n",
        "0 & 0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, we will normalize with Softmax and get:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:** We just created a **decoder attention block** because it has triangular masking so that nodes from the future never talk to the past. It is usually used in autoregressive settings, like language modeling. In an **encoder attention block** there is no masking, allowing all tokens to communicate with each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "c84f38ab-3b4f-46f0-e501-208855752ce6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M78tmbwFpPoF"
      },
      "source": [
        "## Scaled self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niSrDDZ0OdHB"
      },
      "source": [
        "As we saw before, when we initialize the affinities between all the different tokens to 0, stop the communication with future tokens, and apply Softmax, we get the same weights in every row:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "We want the weights in every row to change depending on the affinities between the tokens. Different tokens will find other tokens more or less interesting. Thus, we want to **gather data from the past in a data dependent way**.\n",
        "\n",
        "**Self-attention** achieves that with a **query matrix** (what am I looking for) and a **key matrix** (what do I contain) for every token. The matrix multiplication of the querys with the transpose of the keys produces the affinities. Thus, the queries of a token dot product with the keys of all the other tokens. If the key and the query align, the affinity will be high for that iteraction.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1T} \\\\\n",
        "a_{21} & a_{22} & a_{23} & ... & a_{2T} \\\\\n",
        "a_{31} & a_{32} & a_{33} & ... & a_{3T} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & ... & a_{TT}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "q_{11} & q_{12} & q_{13} & ... & q_{1H} \\\\\n",
        "q_{21} & q_{22} & q_{23} & ... & q_{2H} \\\\\n",
        "q_{31} & q_{32} & q_{33} & ... & q_{3H} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "q_{T1} & q_{T2} & q_{T3} & ... & q_{TH}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "k_{11} & k_{12} & k_{13} & ... & k_{1T} \\\\\n",
        "k_{21} & k_{22} & k_{23} & ... & k_{2T} \\\\\n",
        "k_{31} & k_{32} & k_{33} & ... & k_{3T} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "k_{H1} & k_{H2} & k_{H3} & ... & k_{HT}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Then, we will stop the communication with the future tokens by initializing those affinities to -inf:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "a_{11} & -inf & -inf & -inf & -inf \\\\\n",
        "a_{21} & a_{22} & -inf & -inf & -inf \\\\\n",
        "a_{31} & a_{32} & a_{33} & -inf & -inf \\\\\n",
        "a_{41} & a_{42} & a_{43} & a_{44} & -inf \\\\\n",
        "a_{51} & a_{52} & a_{53} & a_{54} & a_{55}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "Finally, we will normalize with Softmax and get:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "w_{11} & 0 & 0 & 0 & 0 \\\\\n",
        "w_{21} & w_{22} & 0 & 0 & 0 \\\\\n",
        "w_{31} & w_{32} & w_{33} & 0 & 0 \\\\\n",
        "w_{41} & w_{42} & w_{43} & w_{44} & 0 \\\\\n",
        "w_{51} & w_{52} & w_{53} & w_{54} & w_{55}\n",
        "\\end{bmatrix}\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:** To get `q` and `k` we are using the Linear modules `key` and `query`. They are just going to apply matrix multiplication of the input with some weights.\n",
        "\n",
        "**Note:** When we do the agregation in a singe Head, we do not agregate the inputs directly. We instead agregate the values. To get `v` we use the Linear module `value` that just applies matrix multiplication of the input with some weights.\n",
        "\n",
        "**Note:** We just created a **self-attention block** because the keys, the queries, and the values are all produced from the same source x. In a **cross-attention block**, the queries still get produced from x, but the keys and the values come from some other external source (e.g. an encoder module).\n",
        "\n",
        "**Note:** We also used **scaled attention** because we divided `wey` by 1/sqrt(head_size). That is important because when `q` are `k` are unit variance, `wei` will be unit variance too and Softmax will stay diffuse and not saturate too much a value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EDarxEWIRMKq"
      },
      "outputs": [],
      "source": [
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# single Head performing self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "# compute keys and queries\n",
        "k = key(x)   # (B, T, hs)\n",
        "q = query(x) # (B, T, hs)\n",
        "\n",
        "# compute attention scores (\"affinities\")\n",
        "# for every bacth element, a square matrix contains the affinities\n",
        "wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T) = (B, T, hs) @ (B, hs, T)\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))  # (B, T, T)\n",
        "wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "# perform the weighted aggregation of the values\n",
        "v = value(x)     # (B, T, hs)\n",
        "xbow4 = wei @ v  # (B, T, hs) = (B, T, T) @ (B, T, hs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5PVmHOiOdHD",
        "outputId": "2c04c938-b7a9-43e2-e0d6-67d7ec2f379b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3450, 0.6550, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3097, 0.3256, 0.3647, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3294, 0.1735, 0.1863, 0.3109, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2001, 0.2052, 0.2034, 0.1750, 0.2164, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0998, 0.3419, 0.2265, 0.0703, 0.1754, 0.0861, 0.0000, 0.0000],\n",
              "        [0.1515, 0.0746, 0.1231, 0.1933, 0.1203, 0.2302, 0.1069, 0.0000],\n",
              "        [0.1019, 0.1119, 0.1405, 0.0578, 0.1043, 0.3486, 0.0534, 0.0816]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y9y1AofyMCN"
      },
      "source": [
        "## Single-head self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReKygJAkqNkm"
      },
      "source": [
        "The Head module implements a single head of self-attention.\n",
        "\n",
        "<div style=\"width: 270px\">\n",
        "    <img src=\"https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png\">\n",
        "</div>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OzOfzDA5oYc"
      },
      "source": [
        "**Note:** `trill` is not a parameter of the module so it is a **buffer** according to PyTorch naming conventions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "rfShVmhtyLwW"
      },
      "outputs": [],
      "source": [
        "n_embd = 32 # number of embedding dimensions\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        _, T, _ = x.shape # (B, T, n_embd)\n",
        "\n",
        "        # compute keys and queries\n",
        "        k = self.key(x)   # (B, T, hs)\n",
        "        q = self.query(x) # (B, T, hs)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T) = (B, T, hs) @ (B, hs, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B, T, hs)\n",
        "        out = wei @ v     # (B, T, hs) = (B, T, T) @ (B, T, hs)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdlHNtsXeIBZ"
      },
      "source": [
        "In attention, there is **no notion of space**, it simply acts over a set of vectors. That is why we need to positionally encode the tokens.\n",
        "\n",
        "Now, the **token embedding table** encodes the indices based on the **identity** of the tokens and gives the **token embeddings** instead of the logits. The **position embedding table** encodes the indices based on the **position** of the tokens and gives the **position embeddings**. To get the logits form the token and position embeddings we need the **language modeling head** which is just a linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RE7OGWzhdVQ5"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # token and position embeddings from embedding tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # apply one head of self-attention\n",
        "        x = self.sa_head(x)   # (B, T, n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOIcG7uC_zJs"
      },
      "source": [
        "Using single-head self-attention the validation loss went down from 2.45 to 2.35."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgPgjVli8ljO"
      },
      "source": [
        "## Multi-head self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEYNVOv38lCA"
      },
      "source": [
        "Multi-head attention is just applying **multiple attentions in parallel** and concatenating the results. It usually helps to have **multiple communication channels** of heads with smaller head size than just a single communication channel of one head.\n",
        "\n",
        "<div style=\"width: 500px\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:1010/0*0KPEV8QidHkteKeY.png\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jR4KZkrA9G5A"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ZW__QLBRx2sf"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # 4 heads of 8-dimensional self-attention\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # token and position embeddings from embedding tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # apply multi-head self-attention\n",
        "        x = self.sa_heads(x)  # (B, T, n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l4Q8PiE_I4w"
      },
      "source": [
        "Using multi-head self-attention the validation loss went down from 2.35 to 2.2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqHFnbrmGdtP"
      },
      "source": [
        "## Feedforward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8iesh6fIc94"
      },
      "source": [
        "The multi-headed self-attention did the communication, so the tokens looked at each other, but they didn't really think on what they found from the other tokens. The feedforward network acts on a per token level so once they have gathered all the data they **think on that data** individually.\n",
        "\n",
        "<div style=\"width: 800px\">\n",
        "    <img src=\"https://pbs.twimg.com/media/ESnE4IvUYAAopRf.jpg\">\n",
        "</div>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "m_ufHOCAGlto"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "13gClUAEGl_v"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # token and position embeddings from embedding tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # apply multi-head self-attention\n",
        "        x = self.sa_heads(x)  # (B, T, n_embd)\n",
        "\n",
        "        # apply feedfoward network\n",
        "        x = self.ffwd(x)      # (B, T, n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9DHYOj0Jtwz"
      },
      "source": [
        "Using a single feeforward layer the validation loss went down from 2.2 to 2.16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsSPYUfsLVff"
      },
      "source": [
        "## Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z6EagnULblp"
      },
      "source": [
        "A block **intersperses communication** (done using multi-headed self-attention) **and computation** (done using the feedforward network on all the tokens):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oxTpGBo1QAp8"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # communication\n",
        "        x = self.sa(x)\n",
        "\n",
        "        # computation\n",
        "        x = self.ffwd(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "X7yZQREAQAcP"
      },
      "outputs": [],
      "source": [
        "n_layer = 3 # number of blocks\n",
        "n_head = 4  # number of heads\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # token and position embeddings from embedding tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # apply blocks\n",
        "        x = self.blocks(x)    # (B, T, n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icIeiVxsamZP"
      },
      "source": [
        "Using 3 blocks the validation loss went down from 2.16 to 2.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-yNpAZIRpJ7"
      },
      "source": [
        "## Residual connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJMiPo9vRp4S"
      },
      "source": [
        "We are starting to get a deep neural net that suffers from optimization issues. The residual connections or skip connections are described in the paper [He et al. (2015). *Deep Residual Learning for Image Recognition*](https://arxiv.org/abs/1512.03385) and help with the optimization. The idea is you transform the data but then you have a **skip connection with addition** from the previous features **(Image 1)**.\n",
        "\n",
        "The opposite way to visualize residual connections is that you have a **residual pathway** and you have some **residual blocks** that fork off, perform some computation, and project back via addition **(Image 2)**. During the optimization the residual blocks start to contribute over time. The advantage is that at least at the initialization, during backpropagation the **gradient just flows unimpeded** from the supervision to the input.\n",
        "\n",
        "<div style=\"width: 800px\">\n",
        "    <img src=\"https://pbs.twimg.com/media/ESnE4IvUYAAopRf.jpg\">\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "It is necessary to add a projection layer after the transformations before they join back to the residual pathway. The projection layer is just a linear transformation ( *y = x · w +b* ) of the output of the tranformation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Dgp8IXTndKof"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # multi-headed self-attention\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # projection layer\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "MKRCA0rkeqyW"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # growing inner-layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rDXNPQFqNky"
      },
      "source": [
        "Apply the residual connections ( *x = x + F(x)* ) when calling the transformations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iRaEJP3BcyPW"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # communication with residual connections\n",
        "        x = x + self.sa(x)\n",
        "\n",
        "        # computation with residual connections\n",
        "        x = x + self.ffwd(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWFQJqpfh341"
      },
      "source": [
        "Using residual connections the validation loss went down from 2.1 to 1.94."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-OSSHcbiJ_2"
      },
      "source": [
        "## Layer normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPb69gYmiL0P"
      },
      "source": [
        "Layer normalization also helps with the optimization of deep neural networks and is described in the paper [Ba et al. (2016). *Layer Normalization*](https://arxiv.org/abs/1607.06450). Remember that batch normalization made sure that across the batch dimension any individual neuron had a unit Gaussian distribution (0 mean and 1 standard deviation output) at initialization.\n",
        "\n",
        "Layer normalization is identical to batch bormalization but normalizes across the rows instead of the columns, does not need the running mean and the running variance buffers, and there is no distinction beteween train and test time. The layer normalization acts on a per token level and **normalizes the features** making them unit Gaussian at initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4zQVO-obnzb_"
      },
      "outputs": [],
      "source": [
        "class LayerNorm1d:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    xmean = x.mean(1, keepdim=True)\n",
        "    xvar = x.var(1, keepdim=True)\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhbWHI5eqqBy"
      },
      "source": [
        "In the paper [Vaswani et al. (2017). *Attention is All You Need paper*](https://arxiv.org/abs/1706.03762) the \"Add & Norm\" is applied after the transformations.\n",
        "\n",
        "<div style=\"width: 300px\">\n",
        "    <img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-21.png\">\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "Nowadays, it is more common to apply the layer normalization **before the tranformations** (this is called **pre-norm formulation**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "z9phYV-PqRE9"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # communication with residual connections and layer norm\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "\n",
        "        # computation with residual connections and layer norm\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTNBKvL12IjL"
      },
      "source": [
        "It is also common to add a layer normalization **at the end of the transformer** and right before the final linear layer that decodes into vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YyTBVcfOuWZp"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # token and position embeddings from embedding tables\n",
        "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embd)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, n_embd)\n",
        "        x = tok_emb + pos_emb # (B, T, n_embd)\n",
        "\n",
        "        # apply blocks\n",
        "        x = self.blocks(x)    # (B, T, n_embd)\n",
        "\n",
        "        # final layer norm\n",
        "        x = self.ln_f(x)      # (B, T, n_embd)\n",
        "\n",
        "        # logits from language modeling head\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGcxackZsr6l"
      },
      "source": [
        "Using layer normalization the validation loss went down from 1.94 to 1.93."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixb71Sr0vIqC"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvi7012BxtFT"
      },
      "source": [
        "Dropout is a **regularization technique** described in the paper [Srivastava et al. (2014). *Dropout: A Simple Way to Prevent Neural Networks from Overfitting*](https://jmlr.org/papers/v15/srivastava14a.html) that consists on, in every step, **randomly shut off** some subset of neurons and train without them.\n",
        "\n",
        "<div style=\"width: 550px\">\n",
        "    <img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-23_at_6.19.24_PM.png\">\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "Dropout is added when we calculate the affinities after Softmax so we randomly prevent some of the nodes from communicating:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "llJGGH7gy6yk"
      },
      "outputs": [],
      "source": [
        "dropout = 0.2\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        _, T, _ = x.shape # (B, T, n_embd)\n",
        "\n",
        "        # compute keys and queries\n",
        "        k = self.key(x)   # (B, T, hs)\n",
        "        q = self.query(x) # (B, T, hs)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, T) = (B, T, hs) @ (B, hs, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B, T, hs)\n",
        "        out = wei @ v     # (B, T, hs) = (B, T, T) @ (B, T, hs)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHYuGJrV0e8Q"
      },
      "source": [
        "Dropout is also added right after the projection back to the residual pathway:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "mqU7JkU6yX6V"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # multi-headed self-attention\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # projection and dropout layers\n",
        "        out = self.dropout(self.proj(out))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "N6Jlvr6mx-XR"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # growing inner-layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZcyHhwT29o4"
      },
      "source": [
        "## Scaling up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "oBlY-vM93A-L"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 64       # number of chunks per bacth\n",
        "block_size = 256      # chunks maximum context length\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200      # how many iterations used to calculate the loss\n",
        "eval_interval = 1000   # every how many iterations calculate the loss\n",
        "learning_rate = 3e-4\n",
        "max_iters = 5000\n",
        "n_embd = 384          # number of embedding dimensions\n",
        "n_layer = 6           # number of blocks\n",
        "n_head = 6            # number of heads\n",
        "dropout = 0.2         # dropout percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlsZ4Shz_-B8",
        "outputId": "b158bbbe-90cd-4170-89fc-1694b15ccc89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.788929 M parameters\n"
          ]
        }
      ],
      "source": [
        "# intialize the model\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "3zf7rmyHABR5"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gT2So2XAIX7",
        "outputId": "2d557568-ba84-4b0c-81f9-59aef07b34bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:    0/5000   Train loss: 4.2729   Val loss: 4.2790\n",
            "Step: 1000/5000   Train loss: 1.5539   Val loss: 1.7391\n",
            "Step: 2000/5000   Train loss: 1.3202   Val loss: 1.5549\n",
            "Step: 3000/5000   Train loss: 1.2108   Val loss: 1.4956\n",
            "Step: 4000/5000   Train loss: 1.1322   Val loss: 1.4848\n",
            "Step: 4999/5000   Train loss: 1.0593   Val loss: 1.4869\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step: {iter:4d}/{max_iters:4d}   Train loss: {losses['train']:.4f}   Val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0LnkzyqCn48",
        "outputId": "dc9bd062-4d36-4b22-bcca-2aea20c2aea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "LEONTES:\n",
            "Whither, this? Hasting foe!\n",
            "This chair that banish'd his battle spirits,\n",
            "And diso mers it tellingn their devils.\n",
            "Good not! I know myself-sper, for where he they\n",
            "Command. Besides, quoth deids us the hasty ture.\n",
            "Procks shall be salinting, wanting by their death\n",
            "The whitness, wite-arwarting up with rashing feast\n",
            "With record the heed memorsel of our souls, make with chape,\n",
            "Most of the watchest hath moved, we must.\n",
            "\n",
            "Nurse:\n",
            "Provost, you speak; again, love.\n",
            "Ah, fellow, thus farewell medder-tirrion!\n",
            "And now I know follows me for my heads mark'd thee!\n",
            "Mach's spoon-bear! come on their bosoms\n",
            "And pale and pebble their own posing treats,\n",
            "Or wealth em or a scepsed, friend and bight\n",
            "Most time have done an heart,\n",
            "Forthwell amplot till way upon his soul talls,\n",
            "False their beadins his womble.\n",
            "\n",
            "SLY:\n",
            "Though he must have found you all fitter up,\n",
            "Shall we have abser'd the state of a fiend.\n",
            "But ke drowns, I with tumbly manaters\n",
            "Forbade, as deceit for this dust of rear,\n",
            "For Juliet's courtesy, for h\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
