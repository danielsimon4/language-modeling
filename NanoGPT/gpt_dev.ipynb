{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "# Building a GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of dataset in characters: 1115394\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print('Length of dataset in characters:', len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first 400 characters are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it \n"
          ]
        }
      ],
      "source": [
        "print(text[:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vocabulary size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Vocabulary size** is the number of unique characters.\n",
        "\n",
        "**Note:** *chars[0]* is the new line character, *'\\n'*. *chars[1]* is the space character, *' '*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocabulary size: 65\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print('Unique characters:', ''.join(chars))\n",
        "print('Vocabulary size:', vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tokenize** means convert the raw text as a string to some sequence of integers according to some vocabulary of possible elements. \n",
        "\n",
        "We are building a character level language model so our tokenizer is going to simply translate individual characters into integers using a **lookup table**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoder output: [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "Decoder output: hii there\n"
          ]
        }
      ],
      "source": [
        "# mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# Encoder: take a string, output a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "\n",
        "# Decoder: take a list of integers, output a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print('Encoder output:', encode('hii there'))\n",
        "print('Decoder output:', decode([46, 47, 47, 1, 58, 46, 43, 56, 43]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Google uses [SentencePiece](https://github.com/google/sentencepiece?tab=readme-ov-file) tokenizer to also encode text into integers but in a different schema and using a different vocabulary. SentencePiece does not encode indivual characters or entire words but **sub-words units**. ChatGPT uses the OpenAI's [tiktoken](https://github.com/openai/tiktoken) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 50257\n",
            "Encoder output: [71, 4178, 612]\n",
            "Decoder output: hii there\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "print('Vocabulary size:', enc.n_vocab)\n",
        "\n",
        "print('Encoder output:', enc.encode('hii there'))\n",
        "print('Decoder output:', enc.decode([71, 4178, 612]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now encode the entire text dataset and store it into a PyTorch tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data[:400]) # the earlier 400 characters will look like this to the GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split up the data into train and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n] # 90%\n",
        "val_data = data[n:]   # 10%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We cannot feed the entire training set into a transformer all at once because that would be computationally very expensive. Thus, when training the transformer we sample random little **chunks** out of the training set and train just a chunk at a time. The maximum length of those chunks is the **block size**.\n",
        "\n",
        "\n",
        "In a chunk of nine characters there's actually eight individual examples packed into it (see below). That's because all of these characters follow each other so when we plug the chunk into a transformer, it simultaneously trains to make a prediction for every example.\n",
        "\n",
        "\n",
        "We train on all the eight examples not just for efficiency but to make the transformer network used to seeing **contexts of size 1 up to block size**. That is useful later during inference because we can start the sampling generation with just 1 character. Then, the transformer knows how to predict the next characters all the way up to block size. After block size, it starts truncating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk: [18, 47, 56, 57, 58, 1, 15, 47, 58] \n",
            "\n",
            "1) When context is [18] the target is 47\n",
            "2) When context is [18, 47] the target is 56\n",
            "3) When context is [18, 47, 56] the target is 57\n",
            "4) When context is [18, 47, 56, 57] the target is 58\n",
            "5) When context is [18, 47, 56, 57, 58] the target is 1\n",
            "6) When context is [18, 47, 56, 57, 58, 1] the target is 15\n",
            "7) When context is [18, 47, 56, 57, 58, 1, 15] the target is 47\n",
            "8) When context is [18, 47, 56, 57, 58, 1, 15, 47] the target is 58\n"
          ]
        }
      ],
      "source": [
        "block_size = 8\n",
        "\n",
        "print(f'Chunk: {train_data[:block_size+1].tolist()} \\n')\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f'{t+1}) When context is {context.tolist()} the target is {target}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batch dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because GPUs are very good at **parallel processing** of data, we have **batches of multiple chunks** stacked up in a single tensor feeding into a transformer. Thus, multiple chunks are processed at the same time but completely independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inputs shape: (4, 8)\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "\n",
            "Targets shape: (4, 8)\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "\n",
            "\n",
            "\n",
            "1) When context is [24] the target is 43\n",
            "2) When context is [24, 43] the target is 58\n",
            "3) When context is [24, 43, 58] the target is 5\n",
            "4) When context is [24, 43, 58, 5] the target is 57\n",
            "5) When context is [24, 43, 58, 5, 57] the target is 1\n",
            "6) When context is [24, 43, 58, 5, 57, 1] the target is 46\n",
            "7) When context is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
            "8) When context is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
            "1) When context is [44] the target is 53\n",
            "2) When context is [44, 53] the target is 56\n",
            "3) When context is [44, 53, 56] the target is 1\n",
            "4) When context is [44, 53, 56, 1] the target is 58\n",
            "5) When context is [44, 53, 56, 1, 58] the target is 46\n",
            "6) When context is [44, 53, 56, 1, 58, 46] the target is 39\n",
            "7) When context is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
            "8) When context is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
            "1) When context is [52] the target is 58\n",
            "2) When context is [52, 58] the target is 1\n",
            "3) When context is [52, 58, 1] the target is 58\n",
            "4) When context is [52, 58, 1, 58] the target is 46\n",
            "5) When context is [52, 58, 1, 58, 46] the target is 39\n",
            "6) When context is [52, 58, 1, 58, 46, 39] the target is 58\n",
            "7) When context is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
            "8) When context is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
            "1) When context is [25] the target is 17\n",
            "2) When context is [25, 17] the target is 27\n",
            "3) When context is [25, 17, 27] the target is 10\n",
            "4) When context is [25, 17, 27, 10] the target is 0\n",
            "5) When context is [25, 17, 27, 10, 0] the target is 21\n",
            "6) When context is [25, 17, 27, 10, 0, 21] the target is 1\n",
            "7) When context is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
            "8) When context is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4 # number of chunks per bacth\n",
        "block_size = 8 # chunks maximum context length\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('Inputs shape:', tuple(xb.shape))\n",
        "print(xb)\n",
        "print('\\nTargets shape:', tuple(yb.shape))\n",
        "print(yb)\n",
        "print('\\n\\n')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f'{t+1}) When context is {context.tolist()} the target is {target}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bigram model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:**\n",
        "- B = Bacth (batch_size), in our case 32\n",
        "- T = Time (block_size), in our case 8\n",
        "- C = Channel (vocab_size), in our case 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape: (32, 65)\n",
            "Loss: 4.878634929656982\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B, T) tensor of integers\n",
        "        \n",
        "        # each token reads off the logits for the next token from a lookup table\n",
        "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            \n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        \n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BigramLanguageModel(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a PyTorch optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead of printing the bacth loss, the **estimate_loss()** function averages up the loss over multiple batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_iters = 200    # how many iterations used to calculate the loss\n",
        "eval_interval = 10000 # every how many iterations calculate the loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() # put the model in evaluation mode\n",
        "\n",
        "    # calculate train loss and evaulation loss\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        \n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            _, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        \n",
        "        out[split] = losses.mean()\n",
        "    \n",
        "    model.train() # put the model back in train mode\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step:     0/10000   Train loss: 4.6381   Val loss: 4.6211\n",
            "Step:  1000/10000   Train loss: 4.6243   Val loss: 4.6218\n",
            "Step:  2000/10000   Train loss: 4.6418   Val loss: 4.6272\n",
            "Step:  3000/10000   Train loss: 4.6380   Val loss: 4.6173\n",
            "Step:  4000/10000   Train loss: 4.6357   Val loss: 4.6167\n",
            "Step:  5000/10000   Train loss: 4.6380   Val loss: 4.6136\n",
            "Step:  6000/10000   Train loss: 4.6365   Val loss: 4.6206\n",
            "Step:  7000/10000   Train loss: 4.6407   Val loss: 4.6151\n",
            "Step:  8000/10000   Train loss: 4.6361   Val loss: 4.6232\n",
            "Step:  9000/10000   Train loss: 4.6260   Val loss: 4.6194\n",
            "Step:  9999/10000   Train loss: 4.6340   Val loss: 4.6206\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "max_iters = 100000\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step:{iter:6d}/{max_iters:7d}   Train loss: {losses['train']:.4f}   Val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # forward pass\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    # backward pass\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # update\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate from the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Note:** We start the generation with the tensor [[0]]. That is fine because 0 is the new line character. The generate method will generate more characters up to `max_new_tokens`.\n",
        "\n",
        "**Note:** We are feeding the entire growing context (whatever is generated) into the model. However, because it is a bigram model we are only using the last previous character to predict the next charater."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ".NS3P'ikHs.GiLhQaZQy:g;Xsh.baJFGOE,eLP;XS\n",
            "lvGaa Egu!Fnx$GLIemJmeg?wDjkiaBDxcURUfW,KVAd'CTN'LxlI3,m?HtKnJrvGemUKnBB\n",
            "\n",
            "xcxPDbaLQM:'CrF.lPvTUgVKVrNSOEpWcQ'aQog'\n",
            "hgMEot:\n",
            "$s&QMBddvXQyA\n",
            ".rGrRmQ'fGrlIxXiY-dvmw:RbdcCo'SqSp$gzf cZl!vzGeRVrRAvTss::fLpflfG$$GII3-,KXEnZZrFR!eRBK\n",
            "W3TIxxkRBZqEcbBgjq\n",
            "siKbcU! Sds!WITo:eSunrB3AHqTJ& h mvTUCkuvKfzQfIM'&XpHIw.Njj!jkNaYRAP;YkO3Nzic:a;Kpu!dL3T;LGfup$qEOSbkRGlMSZlgwNUDIM?Wugui;JRRN'u!pbCIYgRIrTfGLIy$WkeNXL'Gnja3o3fgMTURLTw vzfEOXQQE'jjUKFYnHE'SrI& $qpaEdssQE.bFOsOzfnp\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ability to run on a GPU using **CUDA** instead of the CPU so everything is faster.\n",
        "\n",
        "**Note:** CUDA attempts to exploit the advantages of GPUs over CPUs by utilizing the parallelism offered by their multiple cores, which allow the launch of a very high number of simultaneous threads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The tokens in each chunk are currently \"not talking to each other\" so we would like to **\"couple them\"**. In particular, we want to couple them so that the tokens **only communicate with the tokens before them** in the sequence, so information only flows from previous context to the current time step.\n",
        "\n",
        "The easiest way for tokens to communicate with their previous tokens is to compute the **average** of the preceding tokens. Thus, the fifth token should take its channels and also the channels from the fourth step, the third step, the second step, and the first step, and average those up.\n",
        "\n",
        "Consider the following chunk where every row would be a token and the columns the channels:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "a_{21} & a_{22} & a_{23} & ... & a_{2C} \\\\\n",
        "a_{31} & a_{32} & a_{33} & ... & a_{3C} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & ... & a_{TC}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The **bag of words** on that chunk would be:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "\\frac{a_{11} + a_{21}}{2} & \\frac{a_{12} + a_{22}}{2} & \\frac{a_{13} + a_{23}}{2} & ... & \\frac{a_{1C} + a_{2C}}{2} \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31}}{3} & \\frac{a_{12} + a_{22} + a_{32}}{3} & \\frac{a_{13} + a_{23} + a_{33}}{3} & ... & \\frac{a_{1C} + a_{2C} + a_{3C}}{3} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31} + ... + a_{T1}}{T} & \\frac{a_{12} + a_{22} + a_{32} + ... + a_{T2}}{T} & \\frac{a_{13} + a_{23} + a_{33} + ... + a_{T3}}{T} & ... & \\frac{a_{1C} + a_{2C} + a_{3C} + ... + a_{TC}}{T}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We can implement this with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
      },
      "outputs": [],
      "source": [
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "xbow = torch.zeros((B,T,C))\n",
        "\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0) # (C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking only at the first chunk of the batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.4788,  1.3537],\n",
            "        [-0.1593, -0.4249],\n",
            "        [ 0.9442, -0.1849],\n",
            "        [ 1.0608,  0.2083],\n",
            "        [ 1.3065,  0.4598],\n",
            "        [ 0.2618, -0.7599],\n",
            "        [-2.0461, -1.5295],\n",
            "        [ 0.4049,  0.6319]])\n",
            "tensor([[ 0.4788,  1.3537],\n",
            "        [ 0.1597,  0.4644],\n",
            "        [ 0.4212,  0.2479],\n",
            "        [ 0.5811,  0.2380],\n",
            "        [ 0.7262,  0.2824],\n",
            "        [ 0.6488,  0.1087],\n",
            "        [ 0.2638, -0.1253],\n",
            "        [ 0.2814, -0.0307]])\n"
          ]
        }
      ],
      "source": [
        "print(x[0])\n",
        "print(xbow[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matrix multiplication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can be more efficient using matrix multiplication and a lower triangular matrix `wei` like the one below:\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "\\frac{a_{11} + a_{21}}{2} & \\frac{a_{12} + a_{22}}{2} & \\frac{a_{13} + a_{23}}{2} & ... & \\frac{a_{1C} + a_{2C}}{2} \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31}}{3} & \\frac{a_{12} + a_{22} + a_{32}}{3} & \\frac{a_{13} + a_{23} + a_{33}}{3} & ... & \\frac{a_{1C} + a_{2C} + a_{3C}}{3} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "\\frac{a_{11} + a_{21} + a_{31} + ... + a_{T1}}{T} & \\frac{a_{12} + a_{22} + a_{32} + ... + a_{T2}}{T} & \\frac{a_{13} + a_{23} + a_{33} + ... + a_{T3}}{T} & ... & \\frac{a_{1C} + a_{2C} + a_{3C} + ... + a_{TC}}{T}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1C} \\\\\n",
        "a_{21} & a_{22} & a_{23} & ... & a_{2C} \\\\\n",
        "a_{31} & a_{32} & a_{33} & ... & a_{3C} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & ... & a_{TC}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We can implement this with the following code:\n",
        "\n",
        "**Note:** PyTorch will apply the matrix multiplication in all the batch elements (chunks) at the same time and independently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei /= wei.sum(1, keepdim=True) # (T, T)\n",
        "xbow2 = wei @ x # (B, T, C) = (B, T, T) x (B, T, C) \n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that `wei` contains the weights for the aggregation of the past elements. Those weights are **the interaction strength or affinity**, how much of each token from the past we want to aggregate and average up. We are going to modify the way we construct the lower triangular matrix `wei`. It will start being:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0 \\\\\n",
        "0 & 0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "Then, we will stop the communication with the future tokens by initializing those weights to -inf:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 & -inf & -inf & -inf & -inf \\\\\n",
        "0 & 0 & -inf & -inf & -inf \\\\\n",
        "0 & 0 & 0 & -inf & -inf \\\\\n",
        "0 & 0 & 0 & 0 & -inf \\\\\n",
        "0 & 0 & 0 & 0 & 0\n",
        "\\end{bmatrix}\n",
        "\n",
        "Finally, we will normalize with Softmax and get:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we saw before, when we initialize the affinities between all the different tokens to 0 and apply Softmax, `wei` has the same weights in every row:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "1 & 0 & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n",
        "\\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} & 0 & 0 \\\\\n",
        "\\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & 0 \\\\\n",
        "\\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{5}\n",
        "\\end{bmatrix}\n",
        "\n",
        "We want the weights in every row to change depending on the affinities between the tokens. Different tokens will find other tokens more or less interesting. Thus, we want to **gather data from the past in a data dependent way**. \n",
        "\n",
        "**Self-attention** achieves that with a **query matrix** (what am I looking for) and a **key matrix** (what do I contain) for every token. The matrix multiplication of the querys with the transpose of the keys produces the affinities. Thus, the queries of a token dot product with the keys of all the other tokens. If the key and the query align, the affinity will be high for that iteraction.\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & a_{13} & ... & a_{1T} \\\\\n",
        "a_{21} & a_{22} & a_{23} & ... & a_{2T} \\\\\n",
        "a_{31} & a_{32} & a_{33} & ... & a_{3T} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "a_{T1} & a_{T2} & a_{T3} & ... & a_{TT}\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "q_{11} & q_{12} & q_{13} & ... & q_{1H} \\\\\n",
        "q_{21} & q_{22} & q_{23} & ... & q_{2H} \\\\\n",
        "q_{31} & q_{32} & q_{33} & ... & q_{3H} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "q_{T1} & q_{T2} & q_{T3} & ... & q_{TH}\n",
        "\\end{bmatrix}\n",
        "\\times\n",
        "\\begin{bmatrix}\n",
        "k_{11} & k_{12} & k_{13} & ... & k_{1T} \\\\\n",
        "k_{21} & k_{22} & k_{23} & ... & k_{2T} \\\\\n",
        "k_{31} & k_{32} & k_{33} & ... & k_{3T} \\\\\n",
        "... & ... & ... & ... & ... \\\\\n",
        "k_{H1} & k_{H2} & k_{H3} & ... & k_{HT}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "**Note:** H stands for head size.\n",
        "\n",
        "**Note:** To get `q` and `k` we are using the Linear modules `key` and `query`. They are just going to apply matrix multiplication of the input with some weights. \n",
        "\n",
        "Then, we will stop the communication with the future tokens by initializing those affinities to -inf:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "a_{11} & -inf & -inf & -inf & -inf \\\\\n",
        "a_{21} & a_{22} & -inf & -inf & -inf \\\\\n",
        "a_{31} & a_{32} & a_{33} & -inf & -inf \\\\\n",
        "a_{41} & a_{42} & a_{43} & a_{44} & -inf \\\\\n",
        "a_{51} & a_{52} & a_{53} & a_{54} & a_{55}\n",
        "\\end{bmatrix}\n",
        "\n",
        "Finally, we will normalize with Softmax and get:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "w_{11} & 0 & 0 & 0 & 0 \\\\\n",
        "w_{21} & w_{22} & 0 & 0 & 0 \\\\\n",
        "w_{31} & w_{32} & w_{33} & 0 & 0 \\\\\n",
        "w_{41} & w_{42} & w_{43} & w_{44} & 0 \\\\\n",
        "w_{51} & w_{52} & w_{53} & w_{54} & w_{55}\n",
        "\\end{bmatrix}\n",
        "\n",
        "**Note:** When we do the agregation in a singe Head, we do not agregate `x`. We instead agregate `v`. To get `v` we use the Linear modules `value` that is just applies matrix multiplication of the input with some weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
      },
      "outputs": [],
      "source": [
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# single Head performing self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "\n",
        "# for every bacth element, a square \n",
        "# matrix contains the affinities\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, T) = (B, T, 16) @ (B, 16, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1) \n",
        "\n",
        "v = value(x) # (B, T, 16)\n",
        "xbow4 = wei @ v  # (B, T, 16) = (B, T, T) @ (B, T, 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0154, 0.9846, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4255, 0.2942, 0.2802, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.8240, 0.0122, 0.0983, 0.0655, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2672, 0.0975, 0.0874, 0.1344, 0.4135, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3537, 0.0531, 0.3724, 0.0449, 0.0433, 0.1327, 0.0000, 0.0000],\n",
              "        [0.0768, 0.4748, 0.0674, 0.0025, 0.2437, 0.0254, 0.1093, 0.0000],\n",
              "        [0.3156, 0.0121, 0.0655, 0.1236, 0.0502, 0.1782, 0.1888, 0.0660]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attention notes"
      ]
    },
    {
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Attention is a **communication mechanism** that can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "In attention, there is **no notion of space**, it simply acts over a set of vectors. That is why we need to positionally encode the tokens. Also note that each batch element (chunk) is **processed completely independently**.\n",
        "\n",
        "We created a **\"decoder\" attention block** because it has triangular masking so that nodes from the future never talk to the past. It is usually used in autoregressive settings, like language modeling. In an \"encoder\" attention block there is no masking, allowing all tokens to communicate with each other.\n",
        "\n",
        "We created a **\"self-attention\" block** because the keys, the queries, and the values are all produced from the same source x. In a \"cross-attention\" block, the queries still get produced from x, but the keys and the values come from some other external source (e.g. an encoder module).\n",
        "\n",
        "**\"Scaled\" attention** additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much a value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the previous code is in the head module in the gpt.py script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Multi-head attention** is just applying **multiple attentions in parallel** and concatenating the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_heads = MultiHeadAttention(4, n_embd//4)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "\n",
        "        B, T = idx.shape\n",
        "\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "\n",
        "        # x holds the token identity and the position at which the tokens occur\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "\n",
        "        # apply multi-head aattention\n",
        "        x = self.sa_heads(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is the current context\n",
        "        # in each iteration idx will grow:\n",
        "        # (B, T), (B, T+1), (B, T+2), ..., (B, T+max_new_tokens)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            # crop the context to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "\n",
        "            # get logits for current context (calling forward method)\n",
        "            logits, _ = self(idx_cond) # (B, T, C)\n",
        "\n",
        "            # focus only on the last time-step because\n",
        "            # those are the predictions for what comes next\n",
        "            logits = logits[:, -1, :] # (B, C)\n",
        "\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        \n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.209729 M parameters\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 198\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m    195\u001b[0m \n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 198\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[32], line 58\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m     57\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m---> 58\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     60\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[32], line 155\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m    153\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[0;32m    154\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m    156\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m    157\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x) \u001b[38;5;66;03m# (B,T,vocab_size)\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[32], line 133\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    132\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x))\n\u001b[1;32m--> 133\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffwd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[32], line 117\u001b[0m, in \u001b[0;36mFeedFoward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
