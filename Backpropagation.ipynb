{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call ***loss.backward()*** we use PyTorch autograd to calculate all the gradients along the way. In this notebook, we are going to remove this line and instead we are going to **write all the backward pass** manually on the level of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n"
     ]
    }
   ],
   "source": [
    "with open('names.txt', 'r') as file:\n",
    "    words = file.read().splitlines()\n",
    "\n",
    "print(len(words))\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # context length\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix]\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])   # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])   # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializing MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are initializing `b1`, `b2`, `bngain`, `bnbias` in **non-standard ways** because sometimes initializating with all zeros could mask an incorrect implementation of the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10   # dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden)) * 0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden)) * 0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "print('Number of parameters:', sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass, \"chunkated\" into **smaller steps** that are possible to backpropagate one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "\n",
    "# mini-batch construct\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.3120362758636475\n"
     ]
    }
   ],
   "source": [
    "emb = C[Xb]                         # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1           # hidden layer pre-activation\n",
    "\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*bndiff2.sum(0, keepdim=True) # Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact)             # hidden layer\n",
    "\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2                # output layer\n",
    "\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1     # same as 1.0 / counts_sum\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "print('loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The [Bessel's correction](https://math.oxford.emory.edu/site/math117/besselCorrection/) paper explains why dividing by n-1 gives a better estimate of the variance when the population size or samples for the population are very small. Using 1/n almost always underestimates the variance and it is a biased estimator. Hence, it is advised using the **unbiased version of the variance** and divide by 1-n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "  p.grad = None\n",
    "\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropopagate manually through all of the variables as they are defined in the forward pass above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to backpropagate the loss by calculating the derivatives of each variable so we get matrices or vectors containing the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dlogprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss` is the negative mean of all the **log probabilities of the correct next character** in a sequence:\n",
    "\n",
    "*loss = -logprobs[range(n), Yb].mean()*\n",
    "\n",
    "<br>\n",
    "\n",
    "`dlogprobs` will hold the derivative of the loss with respect to all the elements of `logprobs`.\n",
    "\n",
    "- Let us simplify the problem to *loss = -(a + b + c) / 3*. \n",
    "\n",
    "- Algebraically, *loss = -1/3a -1/3b -1/3c*. \n",
    "\n",
    "- Then, the derivative of the loss with respect to a (or b or c) is *dloss/da = -1/3*. \n",
    "\n",
    "- More generally *dloss/da = -1/n*.\n",
    "\n",
    "Thus, the derivative of the loss with respect to the **log probabilities of the correct next characters** is -1/n.\n",
    "\n",
    "The **log probabilties of the incorrect next characters** do not participate in the calculation of the loss. Thus, the derivative of the loss with respect to them is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs) # matrix of zeros with the shape of logprobs\n",
    "dlogprobs[range(n), Yb] = -1.0/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logprobs` is calculated taking the logarithm element wise of all the elements of `probs`:\n",
    "\n",
    "*logprobs = log(probs)*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we multiply the local derivative of the above operation (*d/dx log(x) = 1/x*) times `dlogprobs`.\n",
    "\n",
    "**Note:** This line is taking the examples that have a very low probability currently assigned and it's boosting their gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprobs = (1.0 / probs) * dlogprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dcounts (1) and dcounts_sum_inv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`probs` is calculated muliplying the matrix `counts` and the column vector `counts_sum_inv`:\n",
    "\n",
    "*probs = counts * counts_sum_inv*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{11}x_1 & a_{12}x_1 & a_{13}x_1 \\\\\n",
    "a_{21}x_2 & a_{22}x_2 & a_{23}x_2 \\\\\n",
    "a_{31}x_3 & a_{32}x_3 & a_{33}x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "·\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `probs` with respect to `counts` is `counts_sum_inv` and with respect to `counts_sum_inv` is `counts`. Thus, we multiply these matrices times `dprobs`. In addition, the column vector `counts_sum_inv` is broadcasted so we have to sum horizontally across the rows.\n",
    "\n",
    "**Note:** *If a node is used multiple times, the gradients of all of it uses will be summed during backpropagation.* We are multiplying some elements multiple times so we have to **sum horizontally across the rows**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dcounts_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`counts_sum_inv` is calculated rising element wise all the elements of `counts_sum` to the power of -1:\n",
    "\n",
    "*counts_sum_inv = counts_sum<sup>-1</sup>*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we multiply the local derivative of the above operation (d/dx x<sup>-1</sup> = -x<sup>-2</sup>) times `dcounts_sum_inv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dcounts (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`counts_sum` is calculated summing up the rows of the matrix `counts`:\n",
    "\n",
    "*counts_sum = counts.sum(1, keepdims=True)*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} + a_{12} + a_{13} \\\\\n",
    "a_{21} + a_{22} + a_{23} \\\\\n",
    "a_{31} + a_{32} + a_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "The local derivative of `counts_sum` with respect to `counts` is 1. Thus, we multiply a matrix of ones with the shape of `counts` times `dcounts_sum`.\n",
    "\n",
    "**Note:** We already calculated `dcounts` before so we have to add these gradients with the ones calculated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcounts += torch.ones_like(counts) * dcounts_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dnorm_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`counts` is calculated exponentiating element wise all the elements of `norm_logits`:\n",
    "\n",
    "*counts = e<sup>norm_logits</sup>*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we multiply the local derivative of the above operation (d/dx e<sup>x</sup> = e<sup>x</sup>) times `dcounts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnorm_logits = counts * dcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dlogits (1) and dlogit_maxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`norm_logits` is calculated subtracting the matrix `logits` with the column vector `logit_maxes`.\n",
    "\n",
    "*norm_logits = logits - logit_maxes*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{11}-x_1 & a_{12}-x_1 & a_{13}-x_1 \\\\\n",
    "a_{21}-x_2 & a_{22}-x_2 & a_{23}-x_2 \\\\\n",
    "a_{31}-x_3 & a_{32}-x_3 & a_{33}-x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `norm_logits` with respect to `logits` is 1 and with respect to `logit_maxes` is -1. Thus, we multiply 1 and -1 times `dnorm_logits`. In addition, the column vector `logit_maxes` is broadcasted so we have to sum horizontally across the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits = dnorm_logits.clone() # copy of dnorm_logits\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dlogits (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logit_maxes` is calculated pluging out the max number of each row of the matrix `logits`:\n",
    "\n",
    "*logit_maxes = logits.max(1, keepdim=True).values*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "max(a_{11} & a_{12} & a_{13}) \\\\\n",
    "max(a_{21} & a_{22} & a_{23}) \\\\\n",
    "max(a_{31} & a_{32} & a_{33})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `logit_maxes` with respect to the maximum `logits` is 1. Thus, we only multiply those maximum `logits` times `dlogit_maxes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dh, dW2, and db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logits` is calculated matrix multypling the matrix `h` and `W2` and adding the broadcasted vector `b2`:\n",
    "\n",
    "*logits = h @ W2 + b2*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "d_{11} & d_{12} \\\\\n",
    "d_{21} & d_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "c_{1} & c_{2} \\\\\n",
    "c_{1} & c_{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Multiplying and adding:\n",
    "\n",
    "$$\n",
    "d_{11} = a_{11} b_{11} + a_{12} b_{21} + c_{1} \\\\\n",
    "d_{12} = a_{11} b_{12} + a_{12} b_{22} + c_{2} \\\\\n",
    "d_{21} = a_{21} b_{11} + a_{22} b_{21} + c_{1} \\\\\n",
    "d_{22} = a_{21} b_{12} + a_{22} b_{22} + c_{2}\n",
    "$$\n",
    "\n",
    "Derivating with respect to a:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial a_{11}} = \\frac{\\partial L}{\\partial d_{11}} · b_{11} + \\frac{\\partial L}{\\partial d_{12}} · b_{12} \\\\\n",
    "\\frac{\\partial L}{\\partial a_{12}} = \\frac{\\partial L}{\\partial d_{11}} · b_{21} + \\frac{\\partial L}{\\partial d_{12}} · b_{22} \\\\\n",
    "\\frac{\\partial L}{\\partial a_{21}} = \\frac{\\partial L}{\\partial d_{21}} · b_{11} + \\frac{\\partial L}{\\partial d_{22}} · b_{12} \\\\\n",
    "\\frac{\\partial L}{\\partial a_{22}} = \\frac{\\partial L}{\\partial d_{21}} · b_{21} + \\frac{\\partial L}{\\partial d_{22}} · b_{22}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial a_{11}} & \\frac{\\partial L}{\\partial a_{12}} \\\\\n",
    "\\frac{\\partial L}{\\partial a_{21}} & \\frac{\\partial L}{\\partial a_{22}}\n",
    "\\end{align*}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial d_{11}} & \\frac{\\partial L}{\\partial d_{12}} \\\\\n",
    "\\frac{\\partial L}{\\partial d_{21}} & \\frac{\\partial L}{\\partial d_{22}}\n",
    "\\end{align*}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{21} \\\\\n",
    "b_{12} & b_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{\\partial L}{\\partial d} \\times b^{T}\n",
    "$$\n",
    "\n",
    "Similarly:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = a^{T} \\times \\frac{\\partial L}{\\partial d}\n",
    "$$\n",
    "\n",
    "Finally:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial c_{1}} &= \\frac{\\partial L}{\\partial d_{11}} · 1 + \\frac{\\partial L}{\\partial d_{21}} · 1 \\\\\n",
    "\\frac{\\partial L}{\\partial c_{2}} &= \\frac{\\partial L}{\\partial d_{12}} · 1 + \\frac{\\partial L}{\\partial d_{22}} · 1 \\\\\n",
    "\\\\\n",
    "\\Rightarrow\n",
    "\\frac{\\partial L}{\\partial c} &= \\frac{\\partial L}{\\partial d} · sum(0)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dhpreact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`h` is calculated applying the tanh function element wise to all the elements of `hpreact`:\n",
    "\n",
    "*h = torch.tanh(hpreact)*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we multiply the local derivative of the above operation (d/dx tanh(x) = 1-tanh<sup>2</sup>(x)) times `dh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhpreact = (1.0 - h**2) * dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbngain, dbnraw, and dbnbias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hpreact` is calculated muliplying the row vector `bngain` and the matrix `bnraw` and adding the row vector `bnbias`:\n",
    "\n",
    "*hpreact = bngain * bnraw + bnbias*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1a_{11}+b_1 & x_2a_{12}+b_2 & x_3a_{13}+b_3 \\\\\n",
    "x_1a_{21}+b_1 & x_2a_{22}+b_2 & x_3a_{23}+b_3 \\\\\n",
    "x_1a_{31}+b_1 & x_2a_{32}+b_2 & x_3a_{33}+b_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "·\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & b_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `hpreact` with respect to `bngain` is `bnraw` and with respect to `bnraw` is `bngain`. Thus, we multiply these matrices times `dhpreact`. In addition, the row vector `bngain` is broadcasted so we have to sum vertically across the columns.\n",
    "\n",
    "The local derivative of `hpreact` with respect to `bnbias` is 1. Thus, we multiply 1 times `dhpreact`. In addition, the row vector `bnbias` is broadcasted so we have to sum vertically across the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbndiff (1) and dbnvar_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnraw` is calculated muliplying the matrix `bndiff` and the row vector `bnvar_inv`:\n",
    "\n",
    "*bnraw = bndiff * bnvar_inv*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{11}x_1 & a_{12}x_2 & a_{13}x_3 \\\\\n",
    "a_{21}x_1 & a_{22}x_2 & a_{23}x_3 \\\\\n",
    "a_{31}x_1 & a_{32}x_2 & a_{33}x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "·\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `bnraw` with respect to `bndiff` is `bnvar_inv` and with respect to `bnvar_inv` is `bndiff`. Thus, we multiply these matrices times `dbnraw`. In addition, the row vector `bnvar_inv` is broadcasted so we have to sum vertically across the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbnvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnvar_inv` is calculated dividing 1 by the square root of `bnvar` plus epsilon:\n",
    "\n",
    "*bnvar_inv = (bnvar + 1e-5)<sup>-0.5</sup>*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we multiply the local derivative of the above operation (d/dx x<sup>-0.5</sup> = -0.5x<sup>-1.5</sup>) times `dbnvar_inv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbndiff2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnvar` is calculated dividing the sum of the columns of the matrix `bndiff2` by n-1:\n",
    "\n",
    "*bnvar = 1/(n-1)bndiff2.sum(0, keepdim=True)*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{n-1}\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "+ & + & + \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "+ & + & + \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `bnvar` with respect to `bndiff2` is 1/n-1. Thus, we multiply a matrix of ones with the shape of `bndiff2` times 1/n-1. Then, we multiply that matrix times `dbnvar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbndiff2 = (1.0/(n-1)) * torch.ones_like(bndiff2) * dbnvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbndiff (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnvar` is calculated squaring element wise all the elements of `bndiff`:\n",
    "\n",
    "*bndiff2 = bndiff <sup>2</sup>*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, we multiply the local derivative of the above operation (d/dx x<sup>2</sup> = 2x) times `dbndiff2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbndiff += (2*bndiff) * dbndiff2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dhprebn (1) and dbnmeani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bndiff` is calculated subtracting the matrix `hprebn` with the row vector `bnmeani`.\n",
    "\n",
    "*bndiff = hprebn - bnmeani*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{11}-x_1 & a_{12}-x_2 & a_{13}-x_3 \\\\\n",
    "a_{21}-x_1 & a_{22}-x_2 & a_{23}-x_3 \\\\\n",
    "a_{31}-x_1 & a_{32}-x_2 & a_{33}-x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `bndiff` with respect to `hprebn` is 1 and with respect to `bnmeani` is -1. Thus, we multiply 1 and -1 times `dbndiff`. In addition, the row vector `bnmeani` is broadcasted so we have to sum vertically across the colunns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dhprebn (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bnmeani` is calculated dividing the sum of the columns of the matrix `hprebn` by n:\n",
    "\n",
    "*bnmeani = (1/n)hprebn.sum(0, keepdim=True)*\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\frac{1}{n}\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "+ & + & + \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "+ & + & + \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The local derivative of `bnmeani` with respect to `hprebn` is 1/n. Thus, we multiply a matrix of ones with the shape of `hprebn` times 1/n. Then, we multiply that matrix times `dbnmeani`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhprebn += 1.0/n * torch.ones_like(hprebn) * dbnmeani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dembcat, dW1, and db1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hprebn` is calculated matrix multypling the matrix `embcat` and `W1` and adding the broadcasted vector `b1`:\n",
    "\n",
    "*hprebn = embcat @ W1 + b1*\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "d_{11} & d_{12} \\\\\n",
    "d_{21} & d_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "c_{1} & c_{2} \\\\\n",
    "c_{1} & c_{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "From before:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial a} &= \\frac{\\partial L}{\\partial d} \\times b^{T} \\\\\n",
    "\\frac{\\partial L}{\\partial b} &= a^{T} \\times \\frac{\\partial L}{\\partial d} \\\\\n",
    "\\frac{\\partial L}{\\partial c} &= \\frac{\\partial L}{\\partial d} · sum(0)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`embcat` concatenates the last two vectors of `emb` in the forward pass (from [32, 3, 10] to [32,50]):\n",
    "\n",
    "*embcat = emb.view(emb.shape[0], -1)*\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, in the backward pass we just undo that concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "demb = dembcat.view(emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`emb` is calculated indexing the integers of the rows of the matrix `Xb` using the look-up table `C`:\n",
    "\n",
    "emb = C[Xb]\n",
    "\n",
    "Thus, in the backward pass we undo the indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dC = torch.zeros_like(C)\n",
    "\n",
    "for k in range(Xb.shape[0]):\n",
    "  for j in range(Xb.shape[1]): # iterate over all the elements of Xb\n",
    "    ix = Xb[k,j]\n",
    "    dC[ix] += demb[k,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cmp** is a utility function for comparing the gradients we just calculated manually with the gradients calculated by PyTorch autograd.\n",
    "\n",
    "- `dt` is the value calcualated by us manually.\n",
    "\n",
    "- `t` is the value calculated by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagate through cross_entropy but all in one go. To complete this challenge look at the mathematical expression of the loss, take the derivative, simplify the expression, and just write it out.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_fast: 3.3120360374450684\n"
     ]
    }
   ],
   "source": [
    "# BEFORE:\n",
    "\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes  # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1     # same as 1.0 / counts_sum\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "\n",
    "# NOW:\n",
    "\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print('loss_fast:', loss_fast.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.3120362758636475\n",
      "loss_fast: 3.3120360374450684\n",
      "diff (loss_fast - loss): -2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "print('loss:', loss.item())\n",
    "print('loss_fast:', loss_fast.item())\n",
    "print('diff (loss_fast - loss):', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax:**\n",
    "\n",
    "$$\n",
    "prob_i = \\frac{e^{logits_{i}}}{\\sum_{j}{e^{logits_{j}}}} \\\\\n",
    "$$\n",
    "\n",
    "For simplicity:\n",
    "\n",
    "$$\n",
    "prob_i = \\frac{e^{l_{i}}}{\\sum_{j}{e^{l_{j}}}} \\\\\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**Forward pass:**\n",
    "\n",
    "$$\n",
    "\\text{logits} \\xrightarrow{\\text{softmax}} \\text{probs} \\xrightarrow{\\text{- log}} \\text{- logprobs} \\xrightarrow{\\text{identity corrects}} \\text{- logprobs [ j ]}  \\xrightarrow{\\text{mean}} \\text{loss}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**For a single example**, loss is the negative log probability for y label:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{loss} &= -\\log prob_{y} \\\\\n",
    "&= -\\log \\frac{e^{l_{y}}}{\\sum_{j}{e^{l_{j}}}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**Backward pass**:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial loss}{\\partial l_{i}} &= \\frac{\\partial}{\\partial l_{i}}\\begin{bmatrix}-\\log \\frac{e^{l_{y}}}{\\sum_{j}{e^{l_{j}}}}\\end{bmatrix} \\\\\n",
    "&= - \\frac{\\sum_{j}{e^{l_{j}}}}{e^{l_{y}}} · \\frac{\\partial}{\\partial l_{i}}\\begin{bmatrix}\\frac{e^{l_{y}}}{\\sum_{j}{e^{l_{j}}}}\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If i ≠ y:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&= - \\frac{\\sum_{j}{e^{l_{j}}}}{e^{l_{y}}} · \\begin{bmatrix}0·\\frac{1}{\\sum_{j}{e^{l_{j}}}}-\\frac{e^{l_{y}}e^{l_{i}}}{(\\sum_{j}{e^{l_{j}}})^2}\\end{bmatrix} \\\\\n",
    "&= \\frac{(\\sum_{j}{e^{l_{j}}})e^{l_{y}}e^{l_{i}}}{e^{l_{y}}(\\sum_{j}{e^{l_{j}}})^2} \\\\\n",
    "&= \\frac{e^{l_{i}}}{\\sum_{j}{e^{l_{j}}}} \\\\\n",
    "&= prob_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "If i = y:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&= - \\frac{\\sum_{j}{e^{l_{j}}}}{e^{l_{y}}} · \\begin{bmatrix}\\frac{e^{l_{y}}}{\\sum_{j}{e^{l_{j}}}}-\\frac{e^{l_{y}}e^{l_{i}}}{(\\sum_{j}{e^{l_{j}}})^2}\\end{bmatrix} \\\\\n",
    "&= - \\frac{\\sum_{j}{e^{l_{j}}}}{e^{l_{y}}} · \\begin{bmatrix}\\frac{\\sum_{j}{e^{l_{j}}}e^{l_{y}}-e^{l_{y}}e^{l_{i}}}{(\\sum_{j}{e^{l_{j}}})^2}\\end{bmatrix} \\\\\n",
    "&= -\\frac{\\sum_{j}{e^{l_{j}}}-e^{l_{i}}}{\\sum_{j}{e^{l_{j}}}} \\\\\n",
    "&= \\frac{e^{l_{i}}}{\\sum_{j}{e^{l_{j}}}} - 1 \\\\\n",
    "&= prob_i - 1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**For a batch**, loss is the average of all the examples' losses. Hence, `dlogits` has to be scaled down by the number of examples `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE:\n",
    "\n",
    "# dlogprobs = torch.zeros_like(logprobs) # matrix of zeros with the shape of logprobs\n",
    "# dlogprobs[range(n), Yb] = -1.0/n\n",
    "# dprobs = (1.0 / probs) * dlogprobs\n",
    "# dcounts = counts_sum_inv * dprobs\n",
    "# dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "# dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "# dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "# dnorm_logits = counts * dcounts\n",
    "# dlogits = dnorm_logits.clone() # copy of dnorm_logits\n",
    "# dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "# dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "\n",
    "# NOW:\n",
    "\n",
    "dlogits = F.softmax(logits, 1) # calculate prob_i\n",
    "dlogits[range(n), Yb] -= 1     # subtract a 1 at the correct positions\n",
    "dlogits /= n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-09\n"
     ]
    }
   ],
   "source": [
    "cmp('logits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0621, 0.0927, 0.0185, 0.0512, 0.0204, 0.0865, 0.0248, 0.0333, 0.0168,\n",
       "        0.0311, 0.0365, 0.0339, 0.0377, 0.0286, 0.0342, 0.0140, 0.0085, 0.0172,\n",
       "        0.0166, 0.0536, 0.0549, 0.0211, 0.0240, 0.0750, 0.0599, 0.0265, 0.0203],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0621,  0.0927,  0.0185,  0.0512,  0.0204,  0.0865,  0.0248,  0.0333,\n",
       "        -0.9832,  0.0311,  0.0365,  0.0339,  0.0377,  0.0286,  0.0342,  0.0140,\n",
       "         0.0085,  0.0172,  0.0166,  0.0536,  0.0549,  0.0211,  0.0240,  0.0750,\n",
       "         0.0599,  0.0265,  0.0203], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x204338495d0>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop through batchnorm but all in one go to complete this challenge look at the mathematical expression of the output of batchnorm, take the derivative w.r.t. its input, simplify the expression, and just write it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([1, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7995\n",
      "  10000/ 200000: 2.1618\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 97\u001b[0m\n\u001b[0;32m     94\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m100000\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.01\u001b[39m \u001b[38;5;66;03m# step learning rate decay\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parameters, grads):\n\u001b[0;32m     96\u001b[0m   \u001b[38;5;66;03m#p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m   p\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlr \u001b[38;5;241m*\u001b[39m grad \u001b[38;5;66;03m# new way of swole doge TODO: enable\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# track stats\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# print every once in a while\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # 2nd layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    # tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # 1st layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # embedding\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "      for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "  #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "  #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#   cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.070523500442505\n",
      "val 2.109893560409546\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I achieved:\n",
    "# train 2.0718822479248047\n",
    "# val 2.1162495613098145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carmahzamille.\n",
      "khi.\n",
      "mreigeet.\n",
      "khalaysie.\n",
      "mahnen.\n",
      "delynn.\n",
      "jareen.\n",
      "nellara.\n",
      "chaiiv.\n",
      "kaleigh.\n",
      "ham.\n",
      "joce.\n",
      "quinn.\n",
      "shoison.\n",
      "jadiquintero.\n",
      "dearyxi.\n",
      "jace.\n",
      "pinsley.\n",
      "dae.\n",
      "iia.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
